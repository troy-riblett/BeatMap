{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import array\n",
    "import librosa\n",
    "import librosa.display\n",
    "import requests\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "from os import path\n",
    "from io import BytesIO\n",
    "import zipfile\n",
    "import shutil\n",
    "import glob\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Keras imports\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras.layers import Convolution2D, MaxPooling2D, Dense, Dropout, Activation, Flatten, merge, Input, BatchNormalization, LSTM, Reshape\n",
    "from keras.layers.core import *\n",
    "import keras_metrics\n",
    "\n",
    "# Sci-kit learn imports for pre-processing\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install keras_metrics\n",
    "#!pip install librosa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is used to download any songs from the BeatSaber Community Site. This is similar to the download done in the beatmap data analysis, but it grabs only the information needed and also grabs the filename for song files that differ from the expected DifficultyName.dat filename scheme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_song_dict():\n",
    "    song_dict = {}\n",
    "    song_dict[\"time\"] = []\n",
    "    song_dict[\"type\"] = []\n",
    "    song_dict[\"value\"] = []\n",
    "    song_dict[\"line_index\"] = []\n",
    "    song_dict[\"line_layer\"] = []\n",
    "    song_dict[\"cut_direction\"] = []\n",
    "    song_dict[\"duration\"] = []\n",
    "    song_dict[\"width\"] = []\n",
    "    song_dict[\"object_type\"] = []\n",
    "    \n",
    "    return song_dict\n",
    "\n",
    "def add_event_to_dict(song_dict, time, event_type, value):\n",
    "    \"\"\"Appends the values for an event to the song's dictionary\"\"\"\n",
    "    song_dict[\"time\"].append(time)\n",
    "    song_dict[\"type\"].append(event_type)\n",
    "    song_dict[\"value\"].append(value)\n",
    "    song_dict[\"line_index\"].append(np.nan) # NA on events\n",
    "    song_dict[\"line_layer\"].append(np.nan) # NA on events\n",
    "    song_dict[\"cut_direction\"].append(np.nan) # NA on events\n",
    "    song_dict[\"duration\"].append(np.nan) # NA on events\n",
    "    song_dict[\"width\"].append(np.nan) # NA on events\n",
    "    song_dict[\"object_type\"].append(\"Event\")\n",
    "\n",
    "    \n",
    "def add_note_to_dict(song_dict, time, line_index, line_layer, note_type, cut_direction):\n",
    "    \"\"\"Appends the values for a note (which includes bombs) to the song's dictionary\"\"\"\n",
    "    song_dict[\"time\"].append(time)\n",
    "    song_dict[\"type\"].append(note_type)\n",
    "    song_dict[\"value\"].append(np.nan) # NA on notes/bombs\n",
    "    song_dict[\"line_index\"].append(line_index)\n",
    "    song_dict[\"line_layer\"].append(line_layer)\n",
    "    song_dict[\"cut_direction\"].append(cut_direction)\n",
    "    song_dict[\"duration\"].append(np.nan) # NA on notes/bombs\n",
    "    song_dict[\"width\"].append(np.nan) # NA on notes/bombs\n",
    "    \n",
    "    if (note_type == 1) or (note_type == 0):\n",
    "        song_dict[\"object_type\"].append(\"Note\")\n",
    "    else:\n",
    "        song_dict[\"object_type\"].append(\"Bomb\")\n",
    "\n",
    "        \n",
    "def add_obstacle_to_dict(song_dict, time, line_index, obstacle_type, duration, width):\n",
    "    \"\"\"Appends the values for an obstacle to the song's dictionary\"\"\"\n",
    "    song_dict[\"time\"].append(time)\n",
    "    song_dict[\"type\"].append(obstacle_type)\n",
    "    song_dict[\"value\"].append(np.nan)\n",
    "    song_dict[\"line_index\"].append(line_index) # NA on obstacles\n",
    "    song_dict[\"line_layer\"].append(np.nan) # NA on obstacles\n",
    "    song_dict[\"cut_direction\"].append(np.nan) # NA on obstacles\n",
    "    song_dict[\"duration\"].append(duration) # NA on obstacles\n",
    "    song_dict[\"width\"].append(width) # NA on obstacles\n",
    "    song_dict[\"object_type\"].append(\"Obstacle\")\n",
    "\n",
    "    \n",
    "def get_pandas_song_map(row):\n",
    "    \"\"\"Get the pandas dataframe for a map file, or convert the song map file into a pandas representation\"\"\"\n",
    "    \n",
    "    orig_filename = \"data/{}/{}\".format(row[\"key\"], row[\"map_filename\"])\n",
    "    new_filename = orig_filename.replace(\".dat\", \"Pandas.json\")\n",
    "    \n",
    "    if path.exists(new_filename):\n",
    "        df = pd.read_json(new_filename)\n",
    "    else:\n",
    "        with open(orig_filename) as temp_json:\n",
    "            temp_data = json.load(temp_json)\n",
    "            temp_dict = create_song_dict()\n",
    "\n",
    "            # Get all notes\n",
    "            for note in temp_data[\"_notes\"]:\n",
    "                add_note_to_dict(temp_dict, note[\"_time\"] * 60 / row[\"bpm\"], note[\"_lineIndex\"], note[\"_lineLayer\"], \n",
    "                                 note[\"_type\"], note[\"_cutDirection\"])\n",
    "\n",
    "            for event in temp_data[\"_events\"]:\n",
    "                add_event_to_dict(temp_dict, event[\"_time\"] * 60 / row[\"bpm\"], event[\"_type\"], event[\"_value\"])\n",
    "\n",
    "            for obstacle in temp_data[\"_obstacles\"]:\n",
    "                add_obstacle_to_dict(temp_dict, obstacle[\"_time\"] * 60 / row[\"bpm\"], obstacle[\"_lineIndex\"], obstacle[\"_type\"],\n",
    "                                    obstacle[\"_duration\"], obstacle[\"_width\"])\n",
    "\n",
    "        # OUT OF SCOPE FOR THIS PROJECT FOR NOW\n",
    "        #for bpm_change in temp_data[\"_BPMChanges\"]\n",
    "        df = pd.DataFrame(temp_dict)\n",
    "        df[\"object_type\"] = df[\"object_type\"].astype('category')\n",
    "        df.to_json(new_filename)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_song_with_key(key):\n",
    "    try:\n",
    "        new_song_df = get_single_song_metadata_df(key) # info from beatsaver\n",
    "        download_song(key) # actual files from beatsaver\n",
    "        new_song_df = get_song_info_from_local_files(new_song_df) # metadata that can only be found from local files\n",
    "        new_song_df.apply(get_pandas_song_map, axis=1)\n",
    "        \n",
    "        return new_song_df\n",
    "    except Exception as e:\n",
    "        logger.warning(\"Couldn't download song for key: \" + key + \", error is: \" + str(e))\n",
    "        empty_dict = {}\n",
    "        empty_dict[\"key\"] = [key]\n",
    "        return pd.DataFrame(empty_dict)\n",
    "    \n",
    "\n",
    "def get_song_info_from_local_files(song_df):\n",
    "    # Open the info.dat file which contains local metadata\n",
    "    with open('./Data/{}/info.dat'.format(song_df.iloc[0][\"key\"]), errors=\"ignore\", encoding=\"utf-8\") as json_file:\n",
    "        # Convert to JSON and grab the filename of the song\n",
    "        data = json.load(json_file)\n",
    "        song_df[\"filename\"] = data[\"_songFilename\"]\n",
    "        \n",
    "        # Get song length via librosa\n",
    "        song_df[\"song_length\"] = librosa.get_duration(filename=\"./Data/{}/{}\".format(song_df.iloc[0][\"key\"], \n",
    "                                                                                         song_df.iloc[0][\"filename\"]))\n",
    "        first_difficulty = True\n",
    "        copy_df = song_df.copy()\n",
    "        \n",
    "        # Iterate through all types of maps\n",
    "        for i in range(len(data[\"_difficultyBeatmapSets\"])):\n",
    "            beat_map_set = data[\"_difficultyBeatmapSets\"][i]\n",
    "            \n",
    "            # We can only use standard, so make sure we have the standard map\n",
    "            if beat_map_set[\"_beatmapCharacteristicName\"] == \"Standard\":\n",
    "                # Iterate through all different difficulties listed and grab their associated filename\n",
    "                for j in range(len(beat_map_set[\"_difficultyBeatmaps\"])):\n",
    "                    dif_beat_map = beat_map_set[\"_difficultyBeatmaps\"][j]\n",
    "                    \n",
    "                    if first_difficulty == True:\n",
    "                        first_difficulty = False\n",
    "                        song_df[\"difficulty\"] = dif_beat_map[\"_difficulty\"]\n",
    "                        song_df[\"map_filename\"] = dif_beat_map[\"_beatmapFilename\"]\n",
    "                    else:\n",
    "                        copy_df[\"difficulty\"] = dif_beat_map[\"_difficulty\"]\n",
    "                        copy_df[\"map_filename\"] = dif_beat_map[\"_beatmapFilename\"]\n",
    "                        song_df = pd.concat([song_df, copy_df])\n",
    "        \n",
    "    return song_df\n",
    "        \n",
    "                                    \n",
    "def get_single_song_metadata_df(data_key):\n",
    "    \"\"\"Retrieves the metadata info for a single song\"\"\"\n",
    "    ratings_data = {}\n",
    "    # Basic counts for upvotes, downvotes, and downloads and the download URL\n",
    "    data = requests.get(\"https://beatsaver.com/api/maps/detail/{}\".format(data_key)).json()\n",
    "    \n",
    "    logging.debug(data)\n",
    "    \n",
    "    ratings_data[\"key\"] = [data_key]\n",
    "    ratings_data[\"level_author_name\"] = [data[\"metadata\"][\"levelAuthorName\"]]\n",
    "    ratings_data[\"song_name\"] = [data[\"metadata\"][\"songName\"]]\n",
    "    ratings_data[\"song_sub_name\"] = [data[\"metadata\"][\"songSubName\"]]\n",
    "    ratings_data[\"upvotes\"] = [data[\"stats\"][\"upVotes\"]]\n",
    "    ratings_data[\"downvotes\"] = [data[\"stats\"][\"downVotes\"]]\n",
    "    ratings_data[\"plays\"] = [data[\"stats\"][\"plays\"]]\n",
    "    ratings_data[\"upload_date\"] = [data[\"uploaded\"]]\n",
    "    ratings_data[\"downloads\"] = [data[\"stats\"][\"downloads\"]]\n",
    "    ratings_data[\"bpm\"] = [data[\"metadata\"][\"bpm\"]]\n",
    "    \n",
    "    logging.info(ratings_data)\n",
    "    song_df = pd.DataFrame.from_dict(ratings_data)\n",
    "    song_df[\"net_votes\"] = song_df[\"upvotes\"] - song_df[\"downvotes\"]\n",
    "    song_df[\"percent_upvotes\"] = 0\n",
    "    \n",
    "    if song_df.iloc[0][\"upvotes\"] != 0:\n",
    "        song_df[\"percent_upvotes\"] = song_df[\"upvotes\"] / (song_df[\"upvotes\"] + song_df[\"downvotes\"])\n",
    "    \n",
    "    return song_df\n",
    "    \n",
    "\n",
    "def create_dir(key):\n",
    "    \"\"\"Creates a folder for the passed in key if one does not already exist. Returns the directory name\"\"\"\n",
    "    directory = \"./Data/{}\".format(key)\n",
    "    if not os.path.exists(directory):\n",
    "        logging.debug(\"Path for \" + key + \" didn't already exist. Creating\")\n",
    "        os.makedirs(directory)\n",
    "    return directory\n",
    "\n",
    "\n",
    "def download_song(key):\n",
    "    \"\"\"Perform the actual song download\"\"\"\n",
    "    directory = create_dir(key)\n",
    "\n",
    "    if already_downloaded(key):\n",
    "        logging.info(\"Songs already downloaded for key \" + key)\n",
    "    else:\n",
    "        # Get the metadata and the song download link from the metadata\n",
    "        song_zip = requests.get(\"https://beatsaver.com/api/download/key/{}\".format(key), stream=True)\n",
    "        os.chdir(directory)\n",
    "\n",
    "        if song_zip.ok:\n",
    "            # Extract to a zip file\n",
    "            z = zipfile.ZipFile(BytesIO(song_zip.content))\n",
    "            z.extractall()\n",
    "\n",
    "            file_list = os.listdir(\".\")\n",
    "            for file in file_list:\n",
    "                # Clean up non-data files like cover, and random lightmap.exe\n",
    "                if not file.endswith(\".dat\") and not file.endswith(\".egg\") and not file.endswith(\".ogg\"):\n",
    "                    # There are also occasionally folders that are created. We don't need those either\n",
    "                    if os.path.isfile(file):\n",
    "                        os.remove(file)\n",
    "                    else:\n",
    "                        shutil.rmtree(file)\n",
    "        else:\n",
    "            logging.warning(\"Could not get files for key {}. Response code {}\".format(key, song_zip.status_code))\n",
    "            # Create a file in the folder showing that this key is invalid\n",
    "            open(\"invalid.txt\", \"a\").close()\n",
    "\n",
    "        os.chdir(\"../..\")\n",
    "\n",
    "\n",
    "def already_downloaded(key):\n",
    "    \"\"\"Check if the song file has already been downloaded. Return true if it has\"\"\"\n",
    "    lookup1 = \"./Data/{}/*.egg\".format(key)\n",
    "    lookup2 = \"./Data/{}/*.ogg\".format(key)\n",
    "    lookup3 = \"./Data/{}/invalid.txt\".format(key)\n",
    "    return len(glob.glob(lookup1)) > 0 or len(glob.glob(lookup2)) > 0 or len(glob.glob(lookup3)) > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%time\n",
    "# COMMENTED OUT SINCE THE FOLLOWING TOOK 4h 9min 51s to run for 25657 keys (with a lot of songs already downloaded)\n",
    "# IF YOU RUN THE FOLLOWING IT WILL DOWNLOAD *ALL* BEATSABER SONGS ON THE COMMUNITY SITE\n",
    "#logger = logging.getLogger()\n",
    "#logger.setLevel(logging.CRITICAL)\n",
    "#song_df = pd.DataFrame()\n",
    "\n",
    "#max_key = int(requests.get(\"https://beatsaver.com/api/maps/latest/0\").json()[\"docs\"][0][\"key\"], 16)\n",
    "\n",
    "# Get all hex keys\n",
    "#for i in range(max_key):\n",
    "#    new_song_df = download_song_with_key(hex(i)[2:])\n",
    "#    song_df = pd.concat([song_df, new_song_df], sort=False)\n",
    "    \n",
    "#    if i % 100 == 0:\n",
    "#        print(\"Downloaded {} Keys\".format(i))\n",
    "\n",
    "#song_df.to_csv(\"data/MetadataNew.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have as many songs as we want, it is time to decide on a subset of songs to use for this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>key</th>\n",
       "      <th>level_author_name</th>\n",
       "      <th>song_name</th>\n",
       "      <th>song_sub_name</th>\n",
       "      <th>upvotes</th>\n",
       "      <th>downvotes</th>\n",
       "      <th>plays</th>\n",
       "      <th>upload_date</th>\n",
       "      <th>downloads</th>\n",
       "      <th>bpm</th>\n",
       "      <th>net_votes</th>\n",
       "      <th>percent_upvotes</th>\n",
       "      <th>filename</th>\n",
       "      <th>song_length</th>\n",
       "      <th>difficulty</th>\n",
       "      <th>map_filename</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1996</th>\n",
       "      <td>0</td>\n",
       "      <td>570</td>\n",
       "      <td>greatyazer</td>\n",
       "      <td>Mr. Blue Sky</td>\n",
       "      <td>Electric Light Orchestra</td>\n",
       "      <td>6092.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>39426.0</td>\n",
       "      <td>2018-06-16T16:53:34.000Z</td>\n",
       "      <td>342795.0</td>\n",
       "      <td>174.0</td>\n",
       "      <td>5972.0</td>\n",
       "      <td>0.980683</td>\n",
       "      <td>song.egg</td>\n",
       "      <td>222.197551</td>\n",
       "      <td>Expert</td>\n",
       "      <td>Expert.dat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1994</th>\n",
       "      <td>0</td>\n",
       "      <td>570</td>\n",
       "      <td>greatyazer</td>\n",
       "      <td>Mr. Blue Sky</td>\n",
       "      <td>Electric Light Orchestra</td>\n",
       "      <td>6092.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>39426.0</td>\n",
       "      <td>2018-06-16T16:53:34.000Z</td>\n",
       "      <td>342795.0</td>\n",
       "      <td>174.0</td>\n",
       "      <td>5972.0</td>\n",
       "      <td>0.980683</td>\n",
       "      <td>song.egg</td>\n",
       "      <td>222.197551</td>\n",
       "      <td>Normal</td>\n",
       "      <td>Normal.dat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1995</th>\n",
       "      <td>0</td>\n",
       "      <td>570</td>\n",
       "      <td>greatyazer</td>\n",
       "      <td>Mr. Blue Sky</td>\n",
       "      <td>Electric Light Orchestra</td>\n",
       "      <td>6092.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>39426.0</td>\n",
       "      <td>2018-06-16T16:53:34.000Z</td>\n",
       "      <td>342795.0</td>\n",
       "      <td>174.0</td>\n",
       "      <td>5972.0</td>\n",
       "      <td>0.980683</td>\n",
       "      <td>song.egg</td>\n",
       "      <td>222.197551</td>\n",
       "      <td>Hard</td>\n",
       "      <td>Hard.dat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>417</th>\n",
       "      <td>0</td>\n",
       "      <td>141</td>\n",
       "      <td>greatyazer</td>\n",
       "      <td>Gangnam Style</td>\n",
       "      <td>PSY</td>\n",
       "      <td>6164.0</td>\n",
       "      <td>225.0</td>\n",
       "      <td>82700.0</td>\n",
       "      <td>2018-05-20T09:59:02.000Z</td>\n",
       "      <td>520342.0</td>\n",
       "      <td>132.0</td>\n",
       "      <td>5939.0</td>\n",
       "      <td>0.964783</td>\n",
       "      <td>song.egg</td>\n",
       "      <td>218.824187</td>\n",
       "      <td>Normal</td>\n",
       "      <td>Normal.dat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>418</th>\n",
       "      <td>0</td>\n",
       "      <td>141</td>\n",
       "      <td>greatyazer</td>\n",
       "      <td>Gangnam Style</td>\n",
       "      <td>PSY</td>\n",
       "      <td>6164.0</td>\n",
       "      <td>225.0</td>\n",
       "      <td>82700.0</td>\n",
       "      <td>2018-05-20T09:59:02.000Z</td>\n",
       "      <td>520342.0</td>\n",
       "      <td>132.0</td>\n",
       "      <td>5939.0</td>\n",
       "      <td>0.964783</td>\n",
       "      <td>song.egg</td>\n",
       "      <td>218.824187</td>\n",
       "      <td>Hard</td>\n",
       "      <td>Hard.dat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>419</th>\n",
       "      <td>0</td>\n",
       "      <td>141</td>\n",
       "      <td>greatyazer</td>\n",
       "      <td>Gangnam Style</td>\n",
       "      <td>PSY</td>\n",
       "      <td>6164.0</td>\n",
       "      <td>225.0</td>\n",
       "      <td>82700.0</td>\n",
       "      <td>2018-05-20T09:59:02.000Z</td>\n",
       "      <td>520342.0</td>\n",
       "      <td>132.0</td>\n",
       "      <td>5939.0</td>\n",
       "      <td>0.964783</td>\n",
       "      <td>song.egg</td>\n",
       "      <td>218.824187</td>\n",
       "      <td>Expert</td>\n",
       "      <td>Expert.dat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>377</th>\n",
       "      <td>0</td>\n",
       "      <td>124</td>\n",
       "      <td>jobas</td>\n",
       "      <td>Rasputin (Funk Overload)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6253.0</td>\n",
       "      <td>318.0</td>\n",
       "      <td>29624.0</td>\n",
       "      <td>2018-05-19T16:36:34.000Z</td>\n",
       "      <td>406245.0</td>\n",
       "      <td>149.0</td>\n",
       "      <td>5935.0</td>\n",
       "      <td>0.951606</td>\n",
       "      <td>song.egg</td>\n",
       "      <td>238.524082</td>\n",
       "      <td>Hard</td>\n",
       "      <td>Hard.dat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>596</th>\n",
       "      <td>0</td>\n",
       "      <td>1bf</td>\n",
       "      <td>calijor</td>\n",
       "      <td>Lone Digger</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5569.0</td>\n",
       "      <td>287.0</td>\n",
       "      <td>57999.0</td>\n",
       "      <td>2018-05-23T00:15:19.000Z</td>\n",
       "      <td>356627.0</td>\n",
       "      <td>124.0</td>\n",
       "      <td>5282.0</td>\n",
       "      <td>0.950990</td>\n",
       "      <td>Caravan Palace - Lone Digger.egg</td>\n",
       "      <td>169.926531</td>\n",
       "      <td>Expert</td>\n",
       "      <td>Expert.dat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>595</th>\n",
       "      <td>0</td>\n",
       "      <td>1bf</td>\n",
       "      <td>calijor</td>\n",
       "      <td>Lone Digger</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5569.0</td>\n",
       "      <td>287.0</td>\n",
       "      <td>57999.0</td>\n",
       "      <td>2018-05-23T00:15:19.000Z</td>\n",
       "      <td>356627.0</td>\n",
       "      <td>124.0</td>\n",
       "      <td>5282.0</td>\n",
       "      <td>0.950990</td>\n",
       "      <td>Caravan Palace - Lone Digger.egg</td>\n",
       "      <td>169.926531</td>\n",
       "      <td>Hard</td>\n",
       "      <td>Hard.dat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>594</th>\n",
       "      <td>0</td>\n",
       "      <td>1bf</td>\n",
       "      <td>calijor</td>\n",
       "      <td>Lone Digger</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5569.0</td>\n",
       "      <td>287.0</td>\n",
       "      <td>57999.0</td>\n",
       "      <td>2018-05-23T00:15:19.000Z</td>\n",
       "      <td>356627.0</td>\n",
       "      <td>124.0</td>\n",
       "      <td>5282.0</td>\n",
       "      <td>0.950990</td>\n",
       "      <td>Caravan Palace - Lone Digger.egg</td>\n",
       "      <td>169.926531</td>\n",
       "      <td>Normal</td>\n",
       "      <td>Normal.dat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>593</th>\n",
       "      <td>0</td>\n",
       "      <td>1bf</td>\n",
       "      <td>calijor</td>\n",
       "      <td>Lone Digger</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5569.0</td>\n",
       "      <td>287.0</td>\n",
       "      <td>57999.0</td>\n",
       "      <td>2018-05-23T00:15:19.000Z</td>\n",
       "      <td>356627.0</td>\n",
       "      <td>124.0</td>\n",
       "      <td>5282.0</td>\n",
       "      <td>0.950990</td>\n",
       "      <td>Caravan Palace - Lone Digger.egg</td>\n",
       "      <td>169.926531</td>\n",
       "      <td>Easy</td>\n",
       "      <td>Easy.dat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1148</th>\n",
       "      <td>0</td>\n",
       "      <td>32e</td>\n",
       "      <td>runrockgame</td>\n",
       "      <td>Harder Better Faster Stronger</td>\n",
       "      <td>Daft Punk</td>\n",
       "      <td>4574.0</td>\n",
       "      <td>310.0</td>\n",
       "      <td>74223.0</td>\n",
       "      <td>2018-06-01T18:01:45.000Z</td>\n",
       "      <td>528911.0</td>\n",
       "      <td>123.0</td>\n",
       "      <td>4264.0</td>\n",
       "      <td>0.936527</td>\n",
       "      <td>Daft Punk - Harder Better Faster Stronger.egg</td>\n",
       "      <td>224.088000</td>\n",
       "      <td>Expert</td>\n",
       "      <td>Expert.dat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1147</th>\n",
       "      <td>0</td>\n",
       "      <td>32e</td>\n",
       "      <td>runrockgame</td>\n",
       "      <td>Harder Better Faster Stronger</td>\n",
       "      <td>Daft Punk</td>\n",
       "      <td>4574.0</td>\n",
       "      <td>310.0</td>\n",
       "      <td>74223.0</td>\n",
       "      <td>2018-06-01T18:01:45.000Z</td>\n",
       "      <td>528911.0</td>\n",
       "      <td>123.0</td>\n",
       "      <td>4264.0</td>\n",
       "      <td>0.936527</td>\n",
       "      <td>Daft Punk - Harder Better Faster Stronger.egg</td>\n",
       "      <td>224.088000</td>\n",
       "      <td>Hard</td>\n",
       "      <td>Hard.dat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>736</th>\n",
       "      <td>0</td>\n",
       "      <td>217</td>\n",
       "      <td>freeek</td>\n",
       "      <td>Beat it</td>\n",
       "      <td>Michael Jackson</td>\n",
       "      <td>4239.0</td>\n",
       "      <td>282.0</td>\n",
       "      <td>117624.0</td>\n",
       "      <td>2018-05-25T14:20:19.000Z</td>\n",
       "      <td>499438.0</td>\n",
       "      <td>139.0</td>\n",
       "      <td>3957.0</td>\n",
       "      <td>0.937624</td>\n",
       "      <td>Beat it.egg</td>\n",
       "      <td>143.118662</td>\n",
       "      <td>Hard</td>\n",
       "      <td>Hard.dat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>735</th>\n",
       "      <td>0</td>\n",
       "      <td>217</td>\n",
       "      <td>freeek</td>\n",
       "      <td>Beat it</td>\n",
       "      <td>Michael Jackson</td>\n",
       "      <td>4239.0</td>\n",
       "      <td>282.0</td>\n",
       "      <td>117624.0</td>\n",
       "      <td>2018-05-25T14:20:19.000Z</td>\n",
       "      <td>499438.0</td>\n",
       "      <td>139.0</td>\n",
       "      <td>3957.0</td>\n",
       "      <td>0.937624</td>\n",
       "      <td>Beat it.egg</td>\n",
       "      <td>143.118662</td>\n",
       "      <td>Normal</td>\n",
       "      <td>Normal.dat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>734</th>\n",
       "      <td>0</td>\n",
       "      <td>217</td>\n",
       "      <td>freeek</td>\n",
       "      <td>Beat it</td>\n",
       "      <td>Michael Jackson</td>\n",
       "      <td>4239.0</td>\n",
       "      <td>282.0</td>\n",
       "      <td>117624.0</td>\n",
       "      <td>2018-05-25T14:20:19.000Z</td>\n",
       "      <td>499438.0</td>\n",
       "      <td>139.0</td>\n",
       "      <td>3957.0</td>\n",
       "      <td>0.937624</td>\n",
       "      <td>Beat it.egg</td>\n",
       "      <td>143.118662</td>\n",
       "      <td>Easy</td>\n",
       "      <td>Easy.dat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>737</th>\n",
       "      <td>0</td>\n",
       "      <td>217</td>\n",
       "      <td>freeek</td>\n",
       "      <td>Beat it</td>\n",
       "      <td>Michael Jackson</td>\n",
       "      <td>4239.0</td>\n",
       "      <td>282.0</td>\n",
       "      <td>117624.0</td>\n",
       "      <td>2018-05-25T14:20:19.000Z</td>\n",
       "      <td>499438.0</td>\n",
       "      <td>139.0</td>\n",
       "      <td>3957.0</td>\n",
       "      <td>0.937624</td>\n",
       "      <td>Beat it.egg</td>\n",
       "      <td>143.118662</td>\n",
       "      <td>Expert</td>\n",
       "      <td>Expert.dat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>294</th>\n",
       "      <td>0</td>\n",
       "      <td>e4</td>\n",
       "      <td>purphoros</td>\n",
       "      <td>Every Time We Touch</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3683.0</td>\n",
       "      <td>135.0</td>\n",
       "      <td>42754.0</td>\n",
       "      <td>2018-05-18T03:51:03.000Z</td>\n",
       "      <td>319691.0</td>\n",
       "      <td>142.0</td>\n",
       "      <td>3548.0</td>\n",
       "      <td>0.964641</td>\n",
       "      <td>Cascada - Every Time We Touch.egg</td>\n",
       "      <td>199.172041</td>\n",
       "      <td>Expert</td>\n",
       "      <td>Expert.dat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1423</th>\n",
       "      <td>0</td>\n",
       "      <td>3fc</td>\n",
       "      <td>blueasis</td>\n",
       "      <td>Seven Nation Army</td>\n",
       "      <td>The White Stripes</td>\n",
       "      <td>3619.0</td>\n",
       "      <td>155.0</td>\n",
       "      <td>56470.0</td>\n",
       "      <td>2018-06-06T18:51:03.000Z</td>\n",
       "      <td>378166.0</td>\n",
       "      <td>124.0</td>\n",
       "      <td>3464.0</td>\n",
       "      <td>0.958930</td>\n",
       "      <td>song.egg</td>\n",
       "      <td>232.153107</td>\n",
       "      <td>Hard</td>\n",
       "      <td>Hard.dat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1422</th>\n",
       "      <td>0</td>\n",
       "      <td>3fc</td>\n",
       "      <td>blueasis</td>\n",
       "      <td>Seven Nation Army</td>\n",
       "      <td>The White Stripes</td>\n",
       "      <td>3619.0</td>\n",
       "      <td>155.0</td>\n",
       "      <td>56470.0</td>\n",
       "      <td>2018-06-06T18:51:03.000Z</td>\n",
       "      <td>378166.0</td>\n",
       "      <td>124.0</td>\n",
       "      <td>3464.0</td>\n",
       "      <td>0.958930</td>\n",
       "      <td>song.egg</td>\n",
       "      <td>232.153107</td>\n",
       "      <td>Normal</td>\n",
       "      <td>Normal.dat</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0  key level_author_name                      song_name  \\\n",
       "1996           0  570        greatyazer                   Mr. Blue Sky   \n",
       "1994           0  570        greatyazer                   Mr. Blue Sky   \n",
       "1995           0  570        greatyazer                   Mr. Blue Sky   \n",
       "417            0  141        greatyazer                  Gangnam Style   \n",
       "418            0  141        greatyazer                  Gangnam Style   \n",
       "419            0  141        greatyazer                  Gangnam Style   \n",
       "377            0  124             jobas       Rasputin (Funk Overload)   \n",
       "596            0  1bf           calijor                    Lone Digger   \n",
       "595            0  1bf           calijor                    Lone Digger   \n",
       "594            0  1bf           calijor                    Lone Digger   \n",
       "593            0  1bf           calijor                    Lone Digger   \n",
       "1148           0  32e       runrockgame  Harder Better Faster Stronger   \n",
       "1147           0  32e       runrockgame  Harder Better Faster Stronger   \n",
       "736            0  217            freeek                        Beat it   \n",
       "735            0  217            freeek                        Beat it   \n",
       "734            0  217            freeek                        Beat it   \n",
       "737            0  217            freeek                        Beat it   \n",
       "294            0   e4         purphoros            Every Time We Touch   \n",
       "1423           0  3fc          blueasis              Seven Nation Army   \n",
       "1422           0  3fc          blueasis              Seven Nation Army   \n",
       "\n",
       "                 song_sub_name  upvotes  downvotes     plays  \\\n",
       "1996  Electric Light Orchestra   6092.0      120.0   39426.0   \n",
       "1994  Electric Light Orchestra   6092.0      120.0   39426.0   \n",
       "1995  Electric Light Orchestra   6092.0      120.0   39426.0   \n",
       "417                        PSY   6164.0      225.0   82700.0   \n",
       "418                        PSY   6164.0      225.0   82700.0   \n",
       "419                        PSY   6164.0      225.0   82700.0   \n",
       "377                        NaN   6253.0      318.0   29624.0   \n",
       "596                        NaN   5569.0      287.0   57999.0   \n",
       "595                        NaN   5569.0      287.0   57999.0   \n",
       "594                        NaN   5569.0      287.0   57999.0   \n",
       "593                        NaN   5569.0      287.0   57999.0   \n",
       "1148                 Daft Punk   4574.0      310.0   74223.0   \n",
       "1147                 Daft Punk   4574.0      310.0   74223.0   \n",
       "736            Michael Jackson   4239.0      282.0  117624.0   \n",
       "735            Michael Jackson   4239.0      282.0  117624.0   \n",
       "734            Michael Jackson   4239.0      282.0  117624.0   \n",
       "737            Michael Jackson   4239.0      282.0  117624.0   \n",
       "294                        NaN   3683.0      135.0   42754.0   \n",
       "1423         The White Stripes   3619.0      155.0   56470.0   \n",
       "1422         The White Stripes   3619.0      155.0   56470.0   \n",
       "\n",
       "                   upload_date  downloads    bpm  net_votes  percent_upvotes  \\\n",
       "1996  2018-06-16T16:53:34.000Z   342795.0  174.0     5972.0         0.980683   \n",
       "1994  2018-06-16T16:53:34.000Z   342795.0  174.0     5972.0         0.980683   \n",
       "1995  2018-06-16T16:53:34.000Z   342795.0  174.0     5972.0         0.980683   \n",
       "417   2018-05-20T09:59:02.000Z   520342.0  132.0     5939.0         0.964783   \n",
       "418   2018-05-20T09:59:02.000Z   520342.0  132.0     5939.0         0.964783   \n",
       "419   2018-05-20T09:59:02.000Z   520342.0  132.0     5939.0         0.964783   \n",
       "377   2018-05-19T16:36:34.000Z   406245.0  149.0     5935.0         0.951606   \n",
       "596   2018-05-23T00:15:19.000Z   356627.0  124.0     5282.0         0.950990   \n",
       "595   2018-05-23T00:15:19.000Z   356627.0  124.0     5282.0         0.950990   \n",
       "594   2018-05-23T00:15:19.000Z   356627.0  124.0     5282.0         0.950990   \n",
       "593   2018-05-23T00:15:19.000Z   356627.0  124.0     5282.0         0.950990   \n",
       "1148  2018-06-01T18:01:45.000Z   528911.0  123.0     4264.0         0.936527   \n",
       "1147  2018-06-01T18:01:45.000Z   528911.0  123.0     4264.0         0.936527   \n",
       "736   2018-05-25T14:20:19.000Z   499438.0  139.0     3957.0         0.937624   \n",
       "735   2018-05-25T14:20:19.000Z   499438.0  139.0     3957.0         0.937624   \n",
       "734   2018-05-25T14:20:19.000Z   499438.0  139.0     3957.0         0.937624   \n",
       "737   2018-05-25T14:20:19.000Z   499438.0  139.0     3957.0         0.937624   \n",
       "294   2018-05-18T03:51:03.000Z   319691.0  142.0     3548.0         0.964641   \n",
       "1423  2018-06-06T18:51:03.000Z   378166.0  124.0     3464.0         0.958930   \n",
       "1422  2018-06-06T18:51:03.000Z   378166.0  124.0     3464.0         0.958930   \n",
       "\n",
       "                                           filename  song_length difficulty  \\\n",
       "1996                                       song.egg   222.197551     Expert   \n",
       "1994                                       song.egg   222.197551     Normal   \n",
       "1995                                       song.egg   222.197551       Hard   \n",
       "417                                        song.egg   218.824187     Normal   \n",
       "418                                        song.egg   218.824187       Hard   \n",
       "419                                        song.egg   218.824187     Expert   \n",
       "377                                        song.egg   238.524082       Hard   \n",
       "596                Caravan Palace - Lone Digger.egg   169.926531     Expert   \n",
       "595                Caravan Palace - Lone Digger.egg   169.926531       Hard   \n",
       "594                Caravan Palace - Lone Digger.egg   169.926531     Normal   \n",
       "593                Caravan Palace - Lone Digger.egg   169.926531       Easy   \n",
       "1148  Daft Punk - Harder Better Faster Stronger.egg   224.088000     Expert   \n",
       "1147  Daft Punk - Harder Better Faster Stronger.egg   224.088000       Hard   \n",
       "736                                     Beat it.egg   143.118662       Hard   \n",
       "735                                     Beat it.egg   143.118662     Normal   \n",
       "734                                     Beat it.egg   143.118662       Easy   \n",
       "737                                     Beat it.egg   143.118662     Expert   \n",
       "294               Cascada - Every Time We Touch.egg   199.172041     Expert   \n",
       "1423                                       song.egg   232.153107       Hard   \n",
       "1422                                       song.egg   232.153107     Normal   \n",
       "\n",
       "     map_filename  \n",
       "1996   Expert.dat  \n",
       "1994   Normal.dat  \n",
       "1995     Hard.dat  \n",
       "417    Normal.dat  \n",
       "418      Hard.dat  \n",
       "419    Expert.dat  \n",
       "377      Hard.dat  \n",
       "596    Expert.dat  \n",
       "595      Hard.dat  \n",
       "594    Normal.dat  \n",
       "593      Easy.dat  \n",
       "1148   Expert.dat  \n",
       "1147     Hard.dat  \n",
       "736      Hard.dat  \n",
       "735    Normal.dat  \n",
       "734      Easy.dat  \n",
       "737    Expert.dat  \n",
       "294    Expert.dat  \n",
       "1423     Hard.dat  \n",
       "1422   Normal.dat  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "song_df = pd.read_csv(\"./Data/MetadataNew.csv\")\n",
    "song_df = song_df.sort_values(\"net_votes\", ascending=False)\n",
    "song_df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1996    570-Expert\n",
       "1994    570-Normal\n",
       "1995      570-Hard\n",
       "417     141-Normal\n",
       "418       141-Hard\n",
       "419     141-Expert\n",
       "377       124-Hard\n",
       "596     1bf-Expert\n",
       "595       1bf-Hard\n",
       "594     1bf-Normal\n",
       "593       1bf-Easy\n",
       "1148    32e-Expert\n",
       "1147      32e-Hard\n",
       "736       217-Hard\n",
       "735     217-Normal\n",
       "734       217-Easy\n",
       "737     217-Expert\n",
       "294      e4-Expert\n",
       "1423      3fc-Hard\n",
       "1422    3fc-Normal\n",
       "Name: Key-Dif, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "song_df[\"Key-Dif\"] = song_df[\"key\"] + \"-\" + song_df[\"difficulty\"]\n",
    "song_df.head(20)[\"Key-Dif\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we now have the keys that we want to use, but we need some actual data to train on. \n",
    "\n",
    "For training we are going to have 2 steps. The first step (which is started below) is to train a neural network on how to determine where to place notes, bombs, obstacles, and events in the song (without classifying the position or any classification problems about the objects themselves).\n",
    "\n",
    "To do that the approach is as follows:\n",
    "1. Create spectrograms of the song in a set size of chunks (e.g. 1 second chunks) so that we can have a constant input length (with the end of the song padded with 0s)\n",
    "2. Get the count of each object type at each timestep (e.g. count how many notes occur at each 10 milisecond spot)\n",
    "3. Use the spectrograms as the training data and the object counts as the \"labels\" for the neural net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_spectrogram_from_file(filename, num_mels, win_len, pitch_shift=0):\n",
    "    y, sr = librosa.load(filename)\n",
    "    \n",
    "    if pitch_shift != 0:\n",
    "        y = librosa.effects.pitch_shift(y, sr, pitch_shift)\n",
    "        \n",
    "    hop_length = 128\n",
    "    \n",
    "    if win_len < 10:\n",
    "        hop_length = 64\n",
    "        \n",
    "    # Create a spectrogram with the window lengths that we want\n",
    "    spec = librosa.feature.melspectrogram(y=y, sr=sr, \n",
    "                                          hop_length=hop_length, \n",
    "                                          n_mels=num_mels, fmin=20, fmax=8000)\n",
    "    # Convert the power of the spectrogram into decibels instead and then normalize it around 1\n",
    "    spec = librosa.power_to_db(spec, np.max, top_db=100) / 100 + 1\n",
    "    \n",
    "    file_len = librosa.core.get_duration(y, sr)\n",
    "    \n",
    "    time_indices = librosa.time_to_frames(np.arange(0, file_len, 0.001 * win_len), sr=sr, hop_length=hop_length)\n",
    "    \n",
    "    return spec, time_indices\n",
    "\n",
    "def get_spectrogram(song_df, key, num_mels, win_len, pitch_shift=0):\n",
    "    \"\"\"Get the spectrograms of the song with the specified window length and number of mels\"\"\"\n",
    "    \n",
    "    # See if we already have the melspectrogram created since that can take a bit\n",
    "    if pitch_shift == 0:\n",
    "        spec_filename = \"data/{}/{}-{}melspec.npy\".format(key, num_mels, win_len)\n",
    "    else:\n",
    "        spec_filename = \"data/{}/{}-{}-{}melspec.npy\".format(key, num_mels, win_len, pitch_shift)\n",
    "    \n",
    "    index_filename = \"data/{}/time_indices.npy\".format(key)\n",
    "    \n",
    "    if path.exists(spec_filename):\n",
    "        spec = np.load(spec_filename)\n",
    "        time_indices = np.load(index_filename)\n",
    "    else:\n",
    "        # Get the name of file that will have the actual song data (e.g. song.ogg)\n",
    "        filename = \"data/{}/{}\".format(key, song_df[song_df.key == key].iloc[0][\"filename\"])\n",
    "\n",
    "        # Create the spectrogram\n",
    "        spec, time_indices = get_spectrogram_from_file(filename, num_mels, win_len, pitch_shift)\n",
    "        \n",
    "        # Save the newly created file\n",
    "        np.save(spec_filename, spec)\n",
    "        \n",
    "        if not path.exists(index_filename):\n",
    "            np.save(index_filename, time_indices)\n",
    "        \n",
    "    return spec, time_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_onset_count_arr_from_df(df, object_type, arr, array_len):\n",
    "    \"\"\"Reads the data from the dataframe and adds it to the passed in numpy arr\"\"\"\n",
    "    \n",
    "    # Gets the current window that we are curious about\n",
    "    temp_df = df[(df.object_type == object_type)]\n",
    "    \n",
    "    # Iterates\n",
    "    for index, row in temp_df.iterrows():\n",
    "        cur_index = int(row[\"input_num\"])\n",
    "        \n",
    "        if cur_index < array_len:\n",
    "            arr[cur_index] = row[\"time\"]\n",
    "        \n",
    "\n",
    "def get_object_counts(song_df, key, difficulty, win_len, array_len):\n",
    "    #import pdb; pdb.set_trace()\n",
    "    \n",
    "    object_filename = \"data/{}/{}-{}objects.npy\".format(key, difficulty, win_len)\n",
    "    \n",
    "    if path.exists(object_filename):\n",
    "        object_arr = np.load(object_filename)\n",
    "    else:\n",
    "    \n",
    "        # Get the filename and read it into a pandas dataframe\n",
    "        obj_df = get_pandas_song_map(song_df[(song_df.key == key) & (song_df.difficulty == difficulty)].iloc[0])\n",
    "        logging.debug(\"obj_df before: {}\".format(obj_df.sort_values(\"time\")))\n",
    "\n",
    "        # Get the number of objects that occur at each timeframe\n",
    "        obj_df[\"input_num\"] = round(obj_df[\"time\"] * 1000 / win_len)\n",
    "        obj_df = obj_df.sort_values(\"input_num\").groupby([\"input_num\", \"object_type\"])[\"time\"].count().to_frame().reset_index()\n",
    "        logging.debug(\"obj_df after: {}\".format(obj_df))\n",
    "\n",
    "        notes_arr = np.zeros(array_len)\n",
    "        bombs_arr = np.zeros(array_len)\n",
    "        obstacles_arr = np.zeros(array_len)\n",
    "        events_arr = np.zeros(array_len)\n",
    "\n",
    "        set_onset_count_arr_from_df(obj_df, \"Note\", notes_arr, array_len)\n",
    "        set_onset_count_arr_from_df(obj_df, \"Bomb\", bombs_arr, array_len)\n",
    "        set_onset_count_arr_from_df(obj_df, \"Obstacle\", obstacles_arr, array_len)\n",
    "        set_onset_count_arr_from_df(obj_df, \"Event\", events_arr, array_len)\n",
    "        \n",
    "        object_arr = np.vstack([notes_arr, bombs_arr, obstacles_arr, events_arr])\n",
    "        np.save(object_filename, object_arr)\n",
    "    \n",
    "    return object_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dif_val_from_string(dif):\n",
    "    # Get the difficulty as an integer. This is a category but we are NOT using one-hot encoding as\n",
    "    # \"Expert\" is more like \"ExpertPlus\" than \"Easy\" so a numerical category is fine here\n",
    "    # Normalized on 1 to match other neural net inputs\n",
    "    if dif == \"Easy\":\n",
    "        return .2\n",
    "    elif dif == \"Normal\":\n",
    "        return .4\n",
    "    elif dif == \"Hard\":\n",
    "        return .6\n",
    "    elif dif == \"Expert\":\n",
    "        return .8\n",
    "    elif dif == \"ExpertPlus\":\n",
    "        return 1\n",
    "    else:\n",
    "        raise Exception(\"Invalid difficulty passed to get_song_count_train_data\")\n",
    "\n",
    "def get_meta_arr(input_len, dif_val, win_len):\n",
    "    dif_ray = np.ones((input_len)) * dif_val\n",
    "        \n",
    "    # Gets the end index of the first 10 seconds\n",
    "    # End result should be an array that counts up to 1 over num_mil_start miliseconds\n",
    "    num_mil_start = 10000\n",
    "    start_indices = np.arange(0, input_len) * win_len / num_mil_start\n",
    "    end_of_start = int(num_mil_start / win_len) \n",
    "    start_indices[end_of_start:] = start_indices[end_of_start]\n",
    "\n",
    "    # Repeat for end 10 seconds\n",
    "    num_mil_end = 10000\n",
    "    num_end_indices = int(num_mil_end / win_len)\n",
    "    start_of_end = input_len - num_end_indices\n",
    "    end_indices = np.zeros(input_len)\n",
    "\n",
    "    for i in range(num_end_indices):\n",
    "        end_indices[start_of_end + i] = i * win_len / num_mil_end\n",
    "\n",
    "    # Stack all of the meta data type arrays\n",
    "    meta_arr = np.hstack([dif_ray.reshape(input_len, 1), \n",
    "                          start_indices.reshape(input_len, 1), \n",
    "                          end_indices.reshape(input_len, 1)])\n",
    "\n",
    "    return meta_arr\n",
    "        \n",
    "def get_song_count_train_data(song_df, key, num_mels, win_len, difficulty, pitch_shift=0, obj_type=\"Note\"):\n",
    "    \"\"\"Get lists of numpy arrays for a song with the training data and the counts (aka labels)\"\"\"\n",
    "    \n",
    "    # Get all spectrogram values for the song\n",
    "    spec, time_indices = get_spectrogram(song_df, key, num_mels, win_len, pitch_shift)\n",
    "    input_len = len(time_indices)\n",
    "    \n",
    "    obj_list = []\n",
    "    \n",
    "    # Get the difficulty value\n",
    "    dif_val = dif_val_from_string(difficulty)\n",
    "    \n",
    "    if dif_val != 0:\n",
    "        #import pdb; pdb.set_trace()\n",
    "        meta_arr = get_meta_arr(input_len, dif_val, win_len)\n",
    "\n",
    "        # Get the counts at each point for all objects\n",
    "        objs = get_object_counts(song_df, key, difficulty, win_len, input_len)\n",
    "\n",
    "        if obj_type == \"Note\":\n",
    "            obj_arr = objs[0]\n",
    "        elif obj_type == \"Bomb\":\n",
    "            obj_arr = objs[1]\n",
    "        elif obj_type == \"Obst\":\n",
    "            obj_arr = objs[2]\n",
    "        elif obj_type == \"Event\":\n",
    "            obj_arr = objs[3]\n",
    "        else:\n",
    "            raise Exception(\"Invalid obj_type passed to get_song_count_train_data: {}\".format(obj_type))\n",
    "    \n",
    "    \n",
    "    return meta_arr, spec, time_indices, obj_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_spectrogram(arr, time_indices, start_index, num_samples, chunk_radius):\n",
    "    \"\"\"Converts a 2d array into a 3D array with different chunks. At a chunk radius of 5 each array index will include 5\n",
    "    indices from both sides of every index of the array\"\"\"\n",
    "    cur_index = int(start_index)\n",
    "    \n",
    "    #TODO - adjust to work with the new time index system\n",
    "    \n",
    "    chunk_arr_list = []\n",
    "    \n",
    "    while cur_index < min(len(time_indices), start_index + num_samples):\n",
    "        if not isinstance(cur_index, int):\n",
    "            logging.error(\"cur_index is not an integer. cur_index {}. Type {}\".format(cur_index, type(cur_index)))\n",
    "            \n",
    "        true_index = time_indices[cur_index]\n",
    "        \n",
    "        min_index = int(max(0, true_index - chunk_radius))\n",
    "        max_index = int(min(arr.shape[1], true_index + chunk_radius + 1))\n",
    "        logging.debug(\"min_index is {} and max_index is {}\".format(min_index, max_index))\n",
    "        \n",
    "        if min_index > max_index:\n",
    "            logging.error(\"min_index is {} and max_index is {}\".format(min_index, max_index))\n",
    "            logging.error(\"true_index is {} and chunk_radius is {}, vs arr.shape[1] at {}\".format(true_index, chunk_radius, arr.shape[1]))\n",
    "        \n",
    "        pad_left = int(abs(max(-(true_index - chunk_radius), 0)))\n",
    "        pad_right = int(abs(min(0, arr.shape[1] - (true_index + chunk_radius + 1))))\n",
    "        logging.debug(\"Pad_left is {} and pad_right is {}\".format(pad_left, pad_right))\n",
    "        #import pdb; pdb.set_trace()\n",
    "        \n",
    "        if pad_left > 0 or pad_right > 0:\n",
    "            logging.debug(\"Padding from min index {} to max index {} with pad left {} and pad right {}\".format(min_index, max_index, pad_left, pad_right))\n",
    "            new_ray = np.pad(arr[:, min_index:max_index], [(0, 0), (pad_left, pad_right)], 'constant', constant_values=0)\n",
    "        else:\n",
    "            new_ray = arr[:, min_index:max_index]\n",
    "            \n",
    "        chunk_arr_list.append(new_ray)\n",
    "        \n",
    "        cur_index += 1\n",
    "    \n",
    "    if len(chunk_arr_list) == 0:\n",
    "        logging.error(\"For array of shape {} at start index {} for num_samples {} got no values\".format(arr.shape, start_index, num_samples))\n",
    "    \n",
    "    stack_arr = np.stack(chunk_arr_list)\n",
    "    logging.debug(\"stack_arr shape: {}\".format(stack_arr.shape))\n",
    "    return stack_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape_spec_arr(spec):\n",
    "    # Reshapes an array of spectrogram\n",
    "    N, ydim, xdim = spec.shape\n",
    "    return spec.reshape(N, ydim, xdim, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_if_in_range(arr, index, val):\n",
    "    if index >= 0 and index < len(arr) and arr[index] < val:\n",
    "        arr[index] = val\n",
    "\n",
    "# Class for generating data to feed for note classification\n",
    "class PlacementDataGenerator(keras.utils.Sequence):\n",
    "    'Generates data for Keras'\n",
    "    def __init__(self, key_difs, song_df, num_mels, win_len, chunk_radius, object_type=\"Note\", batch_size=200):\n",
    "        'Initialization'\n",
    "        self.key_difs = key_difs\n",
    "        self.song_df = song_df\n",
    "        self.num_mels = num_mels\n",
    "        self.win_len = win_len\n",
    "        self.chunk_radius = chunk_radius\n",
    "        self.object_type = object_type\n",
    "        self.batch_size = batch_size\n",
    "        self.num_samples = 0\n",
    "        \n",
    "        self.pitch_shift = 0\n",
    "        self.pitch_index = 0\n",
    "        self.pitch_shifts = [0, 4, -4, 8, -8, 12, -12, 16, -16]\n",
    "        \n",
    "        self.indices = []\n",
    "        for key_dif in self.key_difs:\n",
    "            key, dif = key_dif.split(\"-\")\n",
    "            spec, time_indices = get_spectrogram(song_df, key, num_mels, win_len)\n",
    "            input_len = len(time_indices)\n",
    "            \n",
    "            self.num_samples += self.batch_size * math.ceil(input_len / self.batch_size)\n",
    "            self.indices.append(self.num_samples)\n",
    "        \n",
    "        logging.info(\"Indices are: {}\".format(self.indices))\n",
    "        # self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the number of batches per epoch'\n",
    "        return math.ceil(self.num_samples / self.batch_size)\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        'Update pitches after each epoc'\n",
    "        \n",
    "        self.pitch_index += 1\n",
    "        \n",
    "        if self.pitch_index >= len(self.pitch_shifts):\n",
    "            self.pitch_index = 0\n",
    "        \n",
    "        self.pitch_shift = self.pitch_shifts[self.pitch_index]\n",
    "        \n",
    "        for key_dif in self.key_difs:\n",
    "            key, dif = key_dif.split(\"-\")\n",
    "            spec, time_indices = get_spectrogram(song_df, key, num_mels, win_len, self.pitch_shift)\n",
    "            \n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        'Generate one batch of data'\n",
    "        #import pdb; pdb.set_trace()\n",
    "        # Get key and difficulty for current       \n",
    "        key_index = 0\n",
    "        \n",
    "        logging.debug(\"Getting batch: {}\".format(index))\n",
    "        \n",
    "        while index * self.batch_size >= self.indices[key_index]:\n",
    "            key_index += 1\n",
    "        \n",
    "        key, dif = self.key_difs[key_index].split(\"-\")\n",
    "        \n",
    "        if key_index > 0:\n",
    "            index -= int(self.indices[key_index - 1] / self.batch_size)\n",
    "        \n",
    "        # Get data for the current key and difficulty\n",
    "        meta_arr, spec_arr, time_indices, obj_arr = get_song_count_train_data(self.song_df, key, self.num_mels, self.win_len, dif, \n",
    "                                                                self.pitch_shift, self.object_type)\n",
    "        logging.debug(\"Index is: {}\".format(index))\n",
    "        spec_arr_new = chunk_spectrogram(spec_arr, time_indices, index * self.batch_size, self.batch_size, self.chunk_radius)\n",
    "        \n",
    "        # We are only looking at whether a note is at a timestep or not, so reduce to either 1 or 0\n",
    "        obj_arr = np.ceil(obj_arr / 100)\n",
    "        \n",
    "        logging.debug(\"{} array {}\".format(self.object_type, obj_arr))\n",
    "        end_index = min(meta_arr.shape[0], index * self.batch_size + self.batch_size)\n",
    "        \n",
    "        # We care about song index because it has to learn to not play notes at the very beginning and end of the song\n",
    "        meta_arr = meta_arr[index * self.batch_size:end_index]\n",
    "        \n",
    "        obj_arr = obj_arr[index * self.batch_size:end_index]\n",
    "        \n",
    "        set_indices = []\n",
    "        \n",
    "        for i in range(0, len(obj_arr)):\n",
    "            if obj_arr[i] == 1:\n",
    "                set_indices.append(i - 1)\n",
    "                set_indices.append(i + 1)\n",
    "        \n",
    "        for index in set_indices:\n",
    "            set_if_in_range(obj_arr, index, 1)\n",
    "        \n",
    "        logging.debug(\"{} array after updates: {}\".format(self.object_type, obj_arr))\n",
    "        \n",
    "        # Reshape the spectrogram to fit the expected 4 dimensions\n",
    "        spec_arr_new = reshape_spec_arr(spec_arr_new)\n",
    "        \n",
    "        logging.debug(\"Shapes. Spec: {}, Dif: {}, Note: {}\".format(spec_arr_new.shape, meta_arr.shape, obj_arr.shape))\n",
    "        \n",
    "        return [spec_arr_new, meta_arr], obj_arr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For cleaning up misc numpy files if needed\n",
    "#for filename in glob.glob(\"Data/*/*.npy\"):\n",
    "#    os.remove(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_experiment_number(model_folder):\n",
    "    if not path.exists(\"Experiments/{}/experiment_count.txt\".format(model_folder)):\n",
    "        return 1\n",
    "    else:\n",
    "        with open(\"Experiments/{}/experiment_count.txt\".format(model_folder), \"r\") as f:\n",
    "            num = f.read()\n",
    "        return int(num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_experiment_number(model_folder, num):\n",
    "    with open(\"Experiments/{}/experiment_count.txt\".format(model_folder), \"w\") as f:\n",
    "        f.write(str(num))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def increment_experiment_number(model_folder):\n",
    "    num = get_experiment_number(model_folder)\n",
    "    write_experiment_number(model_folder, num + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_experiment_folder(model_folder):\n",
    "    exp_num = get_experiment_number(model_folder)\n",
    "    \n",
    "    file_path = \"Experiments/{}/Experiment{}\".format(model_folder, exp_num)\n",
    "    \n",
    "    if not path.exists(file_path):\n",
    "        os.mkdir(file_path)\n",
    "    \n",
    "    return file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_experiment_dfs(model_folder):\n",
    "    summary_filename = \"Experiments/{}/model_summaries.pd\".format(model_folder)\n",
    "    if not path.exists(summary_filename):\n",
    "        \n",
    "        model_dict = {\"experiment_num\": [], \"valid\": [], \"num_mels\": [], \"win_len\": [], \"chunk_radius\": [], \n",
    "            \"batch_size\": [], \"learning_rate\": [], \"note_class_weight\": [],\n",
    "            \"model_version\": [], \"spec_conv_layers\": [], \"spec_dense_layers\": [], \"combined_layers\": []}\n",
    "        \n",
    "        model_df = pd.DataFrame(model_dict)\n",
    "        model_df.to_csv(summary_filename, index=False)\n",
    "    else:\n",
    "        model_df = pd.read_csv(summary_filename)\n",
    "        \n",
    "    metric_filename = \"Experiments/{}/training_metrics.pd\".format(model_folder)\n",
    "    if not path.exists(metric_filename):\n",
    "        \n",
    "        metric_dict = {\"val_loss\": [], \"val_acc\":[], \"val_precision\":[], \"val_recall\": [],\n",
    "            \"loss\": [], \"acc\":[], \"precision\":[], \"recall\": [],\n",
    "            \"epoch_num\": [], \"experiment_num\": []}\n",
    "        \n",
    "        metric_df = pd.DataFrame(metric_dict)\n",
    "        metric_df.to_csv(metric_filename, index=False)\n",
    "    else:\n",
    "        metric_df = pd.read_csv(metric_filename)\n",
    "    \n",
    "    return model_df, metric_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_dense_layers(input_layer, units, activation=\"relu\", num_layers=1):\n",
    "    dense = Dense(units, activation ='relu')(input_layer)\n",
    "    \n",
    "    for i in range(num_layers - 1):\n",
    "        dense = Dense(units, activation ='relu')(dense)\n",
    "        \n",
    "    return dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_conv2D_layers(input_layer, filters, kernel_size, num_1x1_layers = 0, max_pool=False, activation=\"relu\"):\n",
    "    conv = Convolution2D(filters, kernel_size, activation=activation)(input_layer)\n",
    "    \n",
    "    for i in range(num_1x1_layers):\n",
    "        conv = Convolution2D(filters, (1,1), activation=activation)(conv)\n",
    "    \n",
    "    if max_pool == True:\n",
    "        conv = MaxPooling2D(pool_size=(2,2))(conv)\n",
    "        \n",
    "    return conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_conv2D_obj(filters, kernel_size, num_1x1_layers = 0, max_pool=False, activation=\"relu\"):\n",
    "    return {\"filters\":filters, \"kernel_size\": kernel_size, \n",
    "            \"layers\": num_1x1_layers, \"max_pool\": max_pool, \"activation\": activation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dense_obj(units, activation=\"relu\", num_layers=1):\n",
    "    return {\"units\":units, \"activation\":activation, \"num_layers\":num_layers}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_note_place_experiment(song_df, num_mels, win_len, chunk_radius, \n",
    "                              batch_size, num_epochs, learning_rate, \n",
    "                              note_class_weight, train_keys, validation_keys,\n",
    "                              model_folder, model_version, \n",
    "                              spec_conv_layers, spec_dense_layers, combined_layers):\n",
    "    \n",
    "    exp_num = get_experiment_number(model_folder)\n",
    "    summary_df, metric_df = get_experiment_dfs(model_folder)\n",
    "    \n",
    "    model_dict = {\"experiment_num\": [exp_num], \"valid\": [True],  \"num_mels\": [num_mels], \"win_len\": [win_len], \"chunk_radius\": [chunk_radius], \n",
    "            \"batch_size\": [batch_size], \"learning_rate\": [learning_rate], \"note_class_weight\": [note_class_weight],\n",
    "            \"model_version\": [model_version], \"spec_conv_layers\": [spec_conv_layers],\n",
    "            \"spec_dense_layers\": [spec_dense_layers], \"combined_layers\": [combined_layers]}\n",
    "    \n",
    "    model_df = pd.DataFrame(model_dict)\n",
    "    \n",
    "    model_dup_df = model_df.drop(\"experiment_num\", axis=1)\n",
    "    summary_dup_df = summary_df.drop(\"experiment_num\", axis=1)\n",
    "    \n",
    "    count_row = summary_df.shape[0]\n",
    "    model_string = model_dup_df.iloc[0].to_string()\n",
    "    \n",
    "    is_duplicate = False\n",
    "\n",
    "    for i in range(count_row):\n",
    "        if summary_dup_df.iloc[i].to_string() == model_string:\n",
    "            is_duplicate = True\n",
    "            break\n",
    "    \n",
    "    \n",
    "    # Check for duplicates\n",
    "    if is_duplicate == False:\n",
    "        summary_df = pd.concat([summary_df, model_df])\n",
    "        \n",
    "        valid_model = True\n",
    "        try:\n",
    "            image_input = Input(shape=(num_mels, chunk_radius * 2 + 1, 1))\n",
    "\n",
    "            first = True\n",
    "\n",
    "            for conv_layer in spec_conv_layers:\n",
    "                if first:\n",
    "                    conv = add_conv2D_layers(image_input, conv_layer[\"filters\"], conv_layer[\"kernel_size\"],\n",
    "                                               conv_layer[\"layers\"], conv_layer[\"max_pool\"], conv_layer[\"activation\"])\n",
    "                    first = False\n",
    "                else:\n",
    "                    conv = add_conv2D_layers(conv, conv_layer[\"filters\"], conv_layer[\"kernel_size\"],\n",
    "                                               conv_layer[\"layers\"], conv_layer[\"max_pool\"], conv_layer[\"activation\"])\n",
    "\n",
    "            # Flatten for output to a dense layer\n",
    "            if first:\n",
    "                first_dense = Flatten()(image_input)\n",
    "            else:\n",
    "                first_dense = Flatten()(conv)\n",
    "\n",
    "\n",
    "            # Add any dense layers that are defined\n",
    "            for dense_layer in spec_dense_layers:\n",
    "                first_dense = add_dense_layers(first_dense, dense_layer[\"units\"], dense_layer[\"activation\"], dense_layer[\"num_layers\"])\n",
    "\n",
    "\n",
    "            # Timing and difficulty information\n",
    "            other_data_input = Input(shape=(3, ))\n",
    "\n",
    "            merged_model = keras.layers.concatenate([first_dense, other_data_input])\n",
    "\n",
    "            for dense_layer in combined_layers:\n",
    "                merged_model = add_dense_layers(merged_model, dense_layer[\"units\"], dense_layer[\"activation\"], dense_layer[\"num_layers\"])\n",
    "\n",
    "            predictions = Dense(1, activation ='sigmoid')(merged_model)\n",
    "\n",
    "            # Create the model\n",
    "            note_place_class_model = Model(inputs=[image_input, other_data_input], outputs=predictions)\n",
    "\n",
    "            # Set optimizer and compile\n",
    "            optimizer = keras.optimizers.Adam(lr=learning_rate)\n",
    "            note_place_class_model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=[\"accuracy\", keras_metrics.precision(), keras_metrics.recall()])\n",
    "\n",
    "            # Create generators\n",
    "            train_generator = PlacementDataGenerator(train_keys, song_df, num_mels, win_len, chunk_radius, \"Note\", batch_size)\n",
    "            validation_generator = PlacementDataGenerator(validation_keys, song_df, num_mels, win_len, chunk_radius, \"Note\", batch_size)\n",
    "\n",
    "\n",
    "            history = keras.callbacks.History()\n",
    "            note_place_class_model.fit_generator(generator=train_generator, \n",
    "                                                 epochs=num_epochs, \n",
    "                                                 validation_data=validation_generator, \n",
    "                                                 class_weight=note_class_weight,\n",
    "                                                 callbacks=[history])\n",
    "        except ValueError as e:\n",
    "            summary_df.loc[summary_df.experiment_num == exp_num, \"valid\"] = False\n",
    "            valid_model = False\n",
    "\n",
    "            logging.error(\"Experiment skipped due to error. Model: {}\\nError: {}\".format(model_dict, e))\n",
    "\n",
    "\n",
    "        exp_folder = get_experiment_folder(model_folder)\n",
    "\n",
    "        if valid_model:\n",
    "            hist_df = pd.DataFrame(history.history)\n",
    "            hist_df['epoch_num'] = hist_df.index + 1\n",
    "            hist_df['experiment_num'] = exp_num\n",
    "\n",
    "            hist_df.to_csv(\"{}/hist.pd\".format(exp_folder), index=False)\n",
    "\n",
    "            metric_df = pd.concat([metric_df, hist_df])\n",
    "\n",
    "            metric_filename = \"Experiments/{}/training_metrics.pd\".format(model_folder)\n",
    "            metric_df.to_csv(metric_filename, index=False)\n",
    "\n",
    "        summary_filename = \"Experiments/{}/model_summaries.pd\".format(model_folder)\n",
    "        summary_df.to_csv(summary_filename, index=False)\n",
    "\n",
    "        increment_experiment_number(model_folder)\n",
    "    else:\n",
    "        logging.info(\"Experiment skipped due to duplicates. Model: {}\".format(model_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_keys_xxl = song_df.head(100)[\"Key-Dif\"].to_numpy()[0:90]\n",
    "validation_keys_xxl = song_df.head(100)[\"Key-Dif\"].to_numpy()[90:100]\n",
    "train_keys_xl = song_df.head(50)[\"Key-Dif\"].to_numpy()[0:41]\n",
    "validation_keys_xl = song_df.head(50)[\"Key-Dif\"].to_numpy()[41:50]\n",
    "train_keys_l = song_df.head(20)[\"Key-Dif\"].to_numpy()[0:13]\n",
    "validation_keys_l = song_df.head(20)[\"Key-Dif\"].to_numpy()[13:20]\n",
    "train_keys_m = song_df.head(20)[\"Key-Dif\"].to_numpy()[0:3]\n",
    "validation_keys_m = song_df.head(20)[\"Key-Dif\"].to_numpy()[4:6]\n",
    "train_keys_s = [song_df.head(20)[\"Key-Dif\"].to_numpy()[0]]\n",
    "validation_keys_s = [song_df.head(20)[\"Key-Dif\"].to_numpy()[5]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_mels = 80\n",
    "win_len = 10\n",
    "batch_size = 400\n",
    "num_epochs = 50\n",
    "\n",
    "#chunk_radius = 5\n",
    "#learning_rate = 0.0001\n",
    "\n",
    "#class_weight = {0: 1, .25: 5, .5: 5, .75: 5, .9: 5, 1: 10} # Weigh onsets more heavily than non-onsets due to the imbalance\n",
    "#note_class_weight = {0: 1, 1: 15} # Weigh onsets more heavily than non-onsets due to the imbalance\n",
    "#event_class_weight = {0: 1, 1: 15}\n",
    "\n",
    "train_keys = train_keys_l\n",
    "validation_keys = validation_keys_l\n",
    "model_folder = \"NotePlacement\"\n",
    "model_version = 1 # Since this is the first version since we have started logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Experiment skipped due to duplicates. Model: {'experiment_num': [240], 'valid': [True], 'num_mels': [80], 'win_len': [10], 'chunk_radius': [3], 'batch_size': [400], 'learning_rate': [1e-05], 'note_class_weight': [{0: 1, 1: 10}], 'model_version': [1], 'spec_conv_layers': [[{'filters': 64, 'kernel_size': (1, 7), 'layers': 4, 'max_pool': False, 'activation': 'relu'}]], 'spec_dense_layers': [[{'units': 61, 'activation': 'relu', 'num_layers': 3}]], 'combined_layers': [[{'units': 7, 'activation': 'relu', 'num_layers': 3}]]}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\TJ\\.conda\\envs\\beatmap\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\TJ\\.conda\\envs\\beatmap\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\TJ\\.conda\\envs\\beatmap\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\TJ\\.conda\\envs\\beatmap\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\TJ\\.conda\\envs\\beatmap\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\TJ\\.conda\\envs\\beatmap\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\TJ\\.conda\\envs\\beatmap\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\TJ\\.conda\\envs\\beatmap\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "ERROR:root:Experiment skipped due to error. Model: {'experiment_num': [240], 'valid': [True], 'num_mels': [80], 'win_len': [10], 'chunk_radius': [3], 'batch_size': [400], 'learning_rate': [1e-05], 'note_class_weight': [{0: 1, 1: 10}], 'model_version': [1], 'spec_conv_layers': [[{'filters': 64, 'kernel_size': (3, 3), 'layers': 2, 'max_pool': True, 'activation': 'relu'}, {'filters': 128, 'kernel_size': (3, 3), 'layers': 2, 'max_pool': True, 'activation': 'relu'}]], 'spec_dense_layers': [[{'units': 61, 'activation': 'relu', 'num_layers': 2}]], 'combined_layers': [[{'units': 7, 'activation': 'relu', 'num_layers': 3}]]}\n",
      "Error: Negative dimension size caused by subtracting 3 from 2 for 'conv2d_4/convolution' (op: 'Conv2D') with input shapes: [?,39,2,64], [3,3,64,128].\n",
      "ERROR:root:Experiment skipped due to error. Model: {'experiment_num': [241], 'valid': [True], 'num_mels': [80], 'win_len': [10], 'chunk_radius': [3], 'batch_size': [400], 'learning_rate': [1e-05], 'note_class_weight': [{0: 1, 1: 10}], 'model_version': [1], 'spec_conv_layers': [[{'filters': 64, 'kernel_size': (3, 3), 'layers': 2, 'max_pool': True, 'activation': 'relu'}, {'filters': 128, 'kernel_size': (3, 3), 'layers': 2, 'max_pool': True, 'activation': 'relu'}, {'filters': 256, 'kernel_size': (3, 3), 'layers': 2, 'max_pool': True, 'activation': 'relu'}, {'filters': 512, 'kernel_size': (3, 3), 'layers': 2, 'max_pool': True, 'activation': 'relu'}]], 'spec_dense_layers': [[]], 'combined_layers': [[{'units': 7, 'activation': 'relu', 'num_layers': 3}]]}\n",
      "Error: Negative dimension size caused by subtracting 3 from 2 for 'conv2d_8/convolution' (op: 'Conv2D') with input shapes: [?,39,2,64], [3,3,64,128].\n",
      "INFO:root:Experiment skipped due to duplicates. Model: {'experiment_num': [242], 'valid': [True], 'num_mels': [80], 'win_len': [10], 'chunk_radius': [3], 'batch_size': [400], 'learning_rate': [1e-05], 'note_class_weight': [{0: 1, 1: 15}], 'model_version': [1], 'spec_conv_layers': [[{'filters': 64, 'kernel_size': (1, 7), 'layers': 4, 'max_pool': False, 'activation': 'relu'}]], 'spec_dense_layers': [[{'units': 61, 'activation': 'relu', 'num_layers': 3}]], 'combined_layers': [[{'units': 7, 'activation': 'relu', 'num_layers': 3}]]}\n",
      "ERROR:root:Experiment skipped due to error. Model: {'experiment_num': [242], 'valid': [True], 'num_mels': [80], 'win_len': [10], 'chunk_radius': [3], 'batch_size': [400], 'learning_rate': [1e-05], 'note_class_weight': [{0: 1, 1: 15}], 'model_version': [1], 'spec_conv_layers': [[{'filters': 64, 'kernel_size': (3, 3), 'layers': 2, 'max_pool': True, 'activation': 'relu'}, {'filters': 128, 'kernel_size': (3, 3), 'layers': 2, 'max_pool': True, 'activation': 'relu'}]], 'spec_dense_layers': [[{'units': 61, 'activation': 'relu', 'num_layers': 2}]], 'combined_layers': [[{'units': 7, 'activation': 'relu', 'num_layers': 3}]]}\n",
      "Error: Negative dimension size caused by subtracting 3 from 2 for 'conv2d_12/convolution' (op: 'Conv2D') with input shapes: [?,39,2,64], [3,3,64,128].\n",
      "ERROR:root:Experiment skipped due to error. Model: {'experiment_num': [243], 'valid': [True], 'num_mels': [80], 'win_len': [10], 'chunk_radius': [3], 'batch_size': [400], 'learning_rate': [1e-05], 'note_class_weight': [{0: 1, 1: 15}], 'model_version': [1], 'spec_conv_layers': [[{'filters': 64, 'kernel_size': (3, 3), 'layers': 2, 'max_pool': True, 'activation': 'relu'}, {'filters': 128, 'kernel_size': (3, 3), 'layers': 2, 'max_pool': True, 'activation': 'relu'}, {'filters': 256, 'kernel_size': (3, 3), 'layers': 2, 'max_pool': True, 'activation': 'relu'}, {'filters': 512, 'kernel_size': (3, 3), 'layers': 2, 'max_pool': True, 'activation': 'relu'}]], 'spec_dense_layers': [[]], 'combined_layers': [[{'units': 7, 'activation': 'relu', 'num_layers': 3}]]}\n",
      "Error: Negative dimension size caused by subtracting 3 from 2 for 'conv2d_16/convolution' (op: 'Conv2D') with input shapes: [?,39,2,64], [3,3,64,128].\n",
      "INFO:root:Experiment skipped due to duplicates. Model: {'experiment_num': [244], 'valid': [True], 'num_mels': [80], 'win_len': [10], 'chunk_radius': [3], 'batch_size': [400], 'learning_rate': [1e-05], 'note_class_weight': [{0: 1, 1: 20}], 'model_version': [1], 'spec_conv_layers': [[{'filters': 64, 'kernel_size': (1, 7), 'layers': 4, 'max_pool': False, 'activation': 'relu'}]], 'spec_dense_layers': [[{'units': 61, 'activation': 'relu', 'num_layers': 3}]], 'combined_layers': [[{'units': 7, 'activation': 'relu', 'num_layers': 3}]]}\n",
      "ERROR:root:Experiment skipped due to error. Model: {'experiment_num': [244], 'valid': [True], 'num_mels': [80], 'win_len': [10], 'chunk_radius': [3], 'batch_size': [400], 'learning_rate': [1e-05], 'note_class_weight': [{0: 1, 1: 20}], 'model_version': [1], 'spec_conv_layers': [[{'filters': 64, 'kernel_size': (3, 3), 'layers': 2, 'max_pool': True, 'activation': 'relu'}, {'filters': 128, 'kernel_size': (3, 3), 'layers': 2, 'max_pool': True, 'activation': 'relu'}]], 'spec_dense_layers': [[{'units': 61, 'activation': 'relu', 'num_layers': 2}]], 'combined_layers': [[{'units': 7, 'activation': 'relu', 'num_layers': 3}]]}\n",
      "Error: Negative dimension size caused by subtracting 3 from 2 for 'conv2d_20/convolution' (op: 'Conv2D') with input shapes: [?,39,2,64], [3,3,64,128].\n",
      "ERROR:root:Experiment skipped due to error. Model: {'experiment_num': [245], 'valid': [True], 'num_mels': [80], 'win_len': [10], 'chunk_radius': [3], 'batch_size': [400], 'learning_rate': [1e-05], 'note_class_weight': [{0: 1, 1: 20}], 'model_version': [1], 'spec_conv_layers': [[{'filters': 64, 'kernel_size': (3, 3), 'layers': 2, 'max_pool': True, 'activation': 'relu'}, {'filters': 128, 'kernel_size': (3, 3), 'layers': 2, 'max_pool': True, 'activation': 'relu'}, {'filters': 256, 'kernel_size': (3, 3), 'layers': 2, 'max_pool': True, 'activation': 'relu'}, {'filters': 512, 'kernel_size': (3, 3), 'layers': 2, 'max_pool': True, 'activation': 'relu'}]], 'spec_dense_layers': [[]], 'combined_layers': [[{'units': 7, 'activation': 'relu', 'num_layers': 3}]]}\n",
      "Error: Negative dimension size caused by subtracting 3 from 2 for 'conv2d_24/convolution' (op: 'Conv2D') with input shapes: [?,39,2,64], [3,3,64,128].\n",
      "INFO:root:Experiment skipped due to duplicates. Model: {'experiment_num': [246], 'valid': [True], 'num_mels': [80], 'win_len': [10], 'chunk_radius': [3], 'batch_size': [400], 'learning_rate': [0.0001], 'note_class_weight': [{0: 1, 1: 10}], 'model_version': [1], 'spec_conv_layers': [[{'filters': 64, 'kernel_size': (1, 7), 'layers': 4, 'max_pool': False, 'activation': 'relu'}]], 'spec_dense_layers': [[{'units': 61, 'activation': 'relu', 'num_layers': 3}]], 'combined_layers': [[{'units': 7, 'activation': 'relu', 'num_layers': 3}]]}\n",
      "ERROR:root:Experiment skipped due to error. Model: {'experiment_num': [246], 'valid': [True], 'num_mels': [80], 'win_len': [10], 'chunk_radius': [3], 'batch_size': [400], 'learning_rate': [0.0001], 'note_class_weight': [{0: 1, 1: 10}], 'model_version': [1], 'spec_conv_layers': [[{'filters': 64, 'kernel_size': (3, 3), 'layers': 2, 'max_pool': True, 'activation': 'relu'}, {'filters': 128, 'kernel_size': (3, 3), 'layers': 2, 'max_pool': True, 'activation': 'relu'}]], 'spec_dense_layers': [[{'units': 61, 'activation': 'relu', 'num_layers': 2}]], 'combined_layers': [[{'units': 7, 'activation': 'relu', 'num_layers': 3}]]}\n",
      "Error: Negative dimension size caused by subtracting 3 from 2 for 'conv2d_28/convolution' (op: 'Conv2D') with input shapes: [?,39,2,64], [3,3,64,128].\n",
      "ERROR:root:Experiment skipped due to error. Model: {'experiment_num': [247], 'valid': [True], 'num_mels': [80], 'win_len': [10], 'chunk_radius': [3], 'batch_size': [400], 'learning_rate': [0.0001], 'note_class_weight': [{0: 1, 1: 10}], 'model_version': [1], 'spec_conv_layers': [[{'filters': 64, 'kernel_size': (3, 3), 'layers': 2, 'max_pool': True, 'activation': 'relu'}, {'filters': 128, 'kernel_size': (3, 3), 'layers': 2, 'max_pool': True, 'activation': 'relu'}, {'filters': 256, 'kernel_size': (3, 3), 'layers': 2, 'max_pool': True, 'activation': 'relu'}, {'filters': 512, 'kernel_size': (3, 3), 'layers': 2, 'max_pool': True, 'activation': 'relu'}]], 'spec_dense_layers': [[]], 'combined_layers': [[{'units': 7, 'activation': 'relu', 'num_layers': 3}]]}\n",
      "Error: Negative dimension size caused by subtracting 3 from 2 for 'conv2d_32/convolution' (op: 'Conv2D') with input shapes: [?,39,2,64], [3,3,64,128].\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Experiment skipped due to duplicates. Model: {'experiment_num': [248], 'valid': [True], 'num_mels': [80], 'win_len': [10], 'chunk_radius': [3], 'batch_size': [400], 'learning_rate': [0.0001], 'note_class_weight': [{0: 1, 1: 15}], 'model_version': [1], 'spec_conv_layers': [[{'filters': 64, 'kernel_size': (1, 7), 'layers': 4, 'max_pool': False, 'activation': 'relu'}]], 'spec_dense_layers': [[{'units': 61, 'activation': 'relu', 'num_layers': 3}]], 'combined_layers': [[{'units': 7, 'activation': 'relu', 'num_layers': 3}]]}\n",
      "ERROR:root:Experiment skipped due to error. Model: {'experiment_num': [248], 'valid': [True], 'num_mels': [80], 'win_len': [10], 'chunk_radius': [3], 'batch_size': [400], 'learning_rate': [0.0001], 'note_class_weight': [{0: 1, 1: 15}], 'model_version': [1], 'spec_conv_layers': [[{'filters': 64, 'kernel_size': (3, 3), 'layers': 2, 'max_pool': True, 'activation': 'relu'}, {'filters': 128, 'kernel_size': (3, 3), 'layers': 2, 'max_pool': True, 'activation': 'relu'}]], 'spec_dense_layers': [[{'units': 61, 'activation': 'relu', 'num_layers': 2}]], 'combined_layers': [[{'units': 7, 'activation': 'relu', 'num_layers': 3}]]}\n",
      "Error: Negative dimension size caused by subtracting 3 from 2 for 'conv2d_36/convolution' (op: 'Conv2D') with input shapes: [?,39,2,64], [3,3,64,128].\n",
      "ERROR:root:Experiment skipped due to error. Model: {'experiment_num': [249], 'valid': [True], 'num_mels': [80], 'win_len': [10], 'chunk_radius': [3], 'batch_size': [400], 'learning_rate': [0.0001], 'note_class_weight': [{0: 1, 1: 15}], 'model_version': [1], 'spec_conv_layers': [[{'filters': 64, 'kernel_size': (3, 3), 'layers': 2, 'max_pool': True, 'activation': 'relu'}, {'filters': 128, 'kernel_size': (3, 3), 'layers': 2, 'max_pool': True, 'activation': 'relu'}, {'filters': 256, 'kernel_size': (3, 3), 'layers': 2, 'max_pool': True, 'activation': 'relu'}, {'filters': 512, 'kernel_size': (3, 3), 'layers': 2, 'max_pool': True, 'activation': 'relu'}]], 'spec_dense_layers': [[]], 'combined_layers': [[{'units': 7, 'activation': 'relu', 'num_layers': 3}]]}\n",
      "Error: Negative dimension size caused by subtracting 3 from 2 for 'conv2d_40/convolution' (op: 'Conv2D') with input shapes: [?,39,2,64], [3,3,64,128].\n",
      "INFO:root:Experiment skipped due to duplicates. Model: {'experiment_num': [250], 'valid': [True], 'num_mels': [80], 'win_len': [10], 'chunk_radius': [3], 'batch_size': [400], 'learning_rate': [0.0001], 'note_class_weight': [{0: 1, 1: 20}], 'model_version': [1], 'spec_conv_layers': [[{'filters': 64, 'kernel_size': (1, 7), 'layers': 4, 'max_pool': False, 'activation': 'relu'}]], 'spec_dense_layers': [[{'units': 61, 'activation': 'relu', 'num_layers': 3}]], 'combined_layers': [[{'units': 7, 'activation': 'relu', 'num_layers': 3}]]}\n",
      "ERROR:root:Experiment skipped due to error. Model: {'experiment_num': [250], 'valid': [True], 'num_mels': [80], 'win_len': [10], 'chunk_radius': [3], 'batch_size': [400], 'learning_rate': [0.0001], 'note_class_weight': [{0: 1, 1: 20}], 'model_version': [1], 'spec_conv_layers': [[{'filters': 64, 'kernel_size': (3, 3), 'layers': 2, 'max_pool': True, 'activation': 'relu'}, {'filters': 128, 'kernel_size': (3, 3), 'layers': 2, 'max_pool': True, 'activation': 'relu'}]], 'spec_dense_layers': [[{'units': 61, 'activation': 'relu', 'num_layers': 2}]], 'combined_layers': [[{'units': 7, 'activation': 'relu', 'num_layers': 3}]]}\n",
      "Error: Negative dimension size caused by subtracting 3 from 2 for 'conv2d_44/convolution' (op: 'Conv2D') with input shapes: [?,39,2,64], [3,3,64,128].\n",
      "ERROR:root:Experiment skipped due to error. Model: {'experiment_num': [251], 'valid': [True], 'num_mels': [80], 'win_len': [10], 'chunk_radius': [3], 'batch_size': [400], 'learning_rate': [0.0001], 'note_class_weight': [{0: 1, 1: 20}], 'model_version': [1], 'spec_conv_layers': [[{'filters': 64, 'kernel_size': (3, 3), 'layers': 2, 'max_pool': True, 'activation': 'relu'}, {'filters': 128, 'kernel_size': (3, 3), 'layers': 2, 'max_pool': True, 'activation': 'relu'}, {'filters': 256, 'kernel_size': (3, 3), 'layers': 2, 'max_pool': True, 'activation': 'relu'}, {'filters': 512, 'kernel_size': (3, 3), 'layers': 2, 'max_pool': True, 'activation': 'relu'}]], 'spec_dense_layers': [[]], 'combined_layers': [[{'units': 7, 'activation': 'relu', 'num_layers': 3}]]}\n",
      "Error: Negative dimension size caused by subtracting 3 from 2 for 'conv2d_48/convolution' (op: 'Conv2D') with input shapes: [?,39,2,64], [3,3,64,128].\n",
      "INFO:root:Experiment skipped due to duplicates. Model: {'experiment_num': [252], 'valid': [True], 'num_mels': [80], 'win_len': [10], 'chunk_radius': [3], 'batch_size': [400], 'learning_rate': [0.001], 'note_class_weight': [{0: 1, 1: 10}], 'model_version': [1], 'spec_conv_layers': [[{'filters': 64, 'kernel_size': (1, 7), 'layers': 4, 'max_pool': False, 'activation': 'relu'}]], 'spec_dense_layers': [[{'units': 61, 'activation': 'relu', 'num_layers': 3}]], 'combined_layers': [[{'units': 7, 'activation': 'relu', 'num_layers': 3}]]}\n",
      "ERROR:root:Experiment skipped due to error. Model: {'experiment_num': [252], 'valid': [True], 'num_mels': [80], 'win_len': [10], 'chunk_radius': [3], 'batch_size': [400], 'learning_rate': [0.001], 'note_class_weight': [{0: 1, 1: 10}], 'model_version': [1], 'spec_conv_layers': [[{'filters': 64, 'kernel_size': (3, 3), 'layers': 2, 'max_pool': True, 'activation': 'relu'}, {'filters': 128, 'kernel_size': (3, 3), 'layers': 2, 'max_pool': True, 'activation': 'relu'}]], 'spec_dense_layers': [[{'units': 61, 'activation': 'relu', 'num_layers': 2}]], 'combined_layers': [[{'units': 7, 'activation': 'relu', 'num_layers': 3}]]}\n",
      "Error: Negative dimension size caused by subtracting 3 from 2 for 'conv2d_52/convolution' (op: 'Conv2D') with input shapes: [?,39,2,64], [3,3,64,128].\n",
      "ERROR:root:Experiment skipped due to error. Model: {'experiment_num': [253], 'valid': [True], 'num_mels': [80], 'win_len': [10], 'chunk_radius': [3], 'batch_size': [400], 'learning_rate': [0.001], 'note_class_weight': [{0: 1, 1: 10}], 'model_version': [1], 'spec_conv_layers': [[{'filters': 64, 'kernel_size': (3, 3), 'layers': 2, 'max_pool': True, 'activation': 'relu'}, {'filters': 128, 'kernel_size': (3, 3), 'layers': 2, 'max_pool': True, 'activation': 'relu'}, {'filters': 256, 'kernel_size': (3, 3), 'layers': 2, 'max_pool': True, 'activation': 'relu'}, {'filters': 512, 'kernel_size': (3, 3), 'layers': 2, 'max_pool': True, 'activation': 'relu'}]], 'spec_dense_layers': [[]], 'combined_layers': [[{'units': 7, 'activation': 'relu', 'num_layers': 3}]]}\n",
      "Error: Negative dimension size caused by subtracting 3 from 2 for 'conv2d_56/convolution' (op: 'Conv2D') with input shapes: [?,39,2,64], [3,3,64,128].\n",
      "INFO:root:Experiment skipped due to duplicates. Model: {'experiment_num': [254], 'valid': [True], 'num_mels': [80], 'win_len': [10], 'chunk_radius': [3], 'batch_size': [400], 'learning_rate': [0.001], 'note_class_weight': [{0: 1, 1: 15}], 'model_version': [1], 'spec_conv_layers': [[{'filters': 64, 'kernel_size': (1, 7), 'layers': 4, 'max_pool': False, 'activation': 'relu'}]], 'spec_dense_layers': [[{'units': 61, 'activation': 'relu', 'num_layers': 3}]], 'combined_layers': [[{'units': 7, 'activation': 'relu', 'num_layers': 3}]]}\n",
      "ERROR:root:Experiment skipped due to error. Model: {'experiment_num': [254], 'valid': [True], 'num_mels': [80], 'win_len': [10], 'chunk_radius': [3], 'batch_size': [400], 'learning_rate': [0.001], 'note_class_weight': [{0: 1, 1: 15}], 'model_version': [1], 'spec_conv_layers': [[{'filters': 64, 'kernel_size': (3, 3), 'layers': 2, 'max_pool': True, 'activation': 'relu'}, {'filters': 128, 'kernel_size': (3, 3), 'layers': 2, 'max_pool': True, 'activation': 'relu'}]], 'spec_dense_layers': [[{'units': 61, 'activation': 'relu', 'num_layers': 2}]], 'combined_layers': [[{'units': 7, 'activation': 'relu', 'num_layers': 3}]]}\n",
      "Error: Negative dimension size caused by subtracting 3 from 2 for 'conv2d_60/convolution' (op: 'Conv2D') with input shapes: [?,39,2,64], [3,3,64,128].\n",
      "ERROR:root:Experiment skipped due to error. Model: {'experiment_num': [255], 'valid': [True], 'num_mels': [80], 'win_len': [10], 'chunk_radius': [3], 'batch_size': [400], 'learning_rate': [0.001], 'note_class_weight': [{0: 1, 1: 15}], 'model_version': [1], 'spec_conv_layers': [[{'filters': 64, 'kernel_size': (3, 3), 'layers': 2, 'max_pool': True, 'activation': 'relu'}, {'filters': 128, 'kernel_size': (3, 3), 'layers': 2, 'max_pool': True, 'activation': 'relu'}, {'filters': 256, 'kernel_size': (3, 3), 'layers': 2, 'max_pool': True, 'activation': 'relu'}, {'filters': 512, 'kernel_size': (3, 3), 'layers': 2, 'max_pool': True, 'activation': 'relu'}]], 'spec_dense_layers': [[]], 'combined_layers': [[{'units': 7, 'activation': 'relu', 'num_layers': 3}]]}\n",
      "Error: Negative dimension size caused by subtracting 3 from 2 for 'conv2d_64/convolution' (op: 'Conv2D') with input shapes: [?,39,2,64], [3,3,64,128].\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Experiment skipped due to duplicates. Model: {'experiment_num': [256], 'valid': [True], 'num_mels': [80], 'win_len': [10], 'chunk_radius': [3], 'batch_size': [400], 'learning_rate': [0.001], 'note_class_weight': [{0: 1, 1: 20}], 'model_version': [1], 'spec_conv_layers': [[{'filters': 64, 'kernel_size': (1, 7), 'layers': 4, 'max_pool': False, 'activation': 'relu'}]], 'spec_dense_layers': [[{'units': 61, 'activation': 'relu', 'num_layers': 3}]], 'combined_layers': [[{'units': 7, 'activation': 'relu', 'num_layers': 3}]]}\n",
      "ERROR:root:Experiment skipped due to error. Model: {'experiment_num': [256], 'valid': [True], 'num_mels': [80], 'win_len': [10], 'chunk_radius': [3], 'batch_size': [400], 'learning_rate': [0.001], 'note_class_weight': [{0: 1, 1: 20}], 'model_version': [1], 'spec_conv_layers': [[{'filters': 64, 'kernel_size': (3, 3), 'layers': 2, 'max_pool': True, 'activation': 'relu'}, {'filters': 128, 'kernel_size': (3, 3), 'layers': 2, 'max_pool': True, 'activation': 'relu'}]], 'spec_dense_layers': [[{'units': 61, 'activation': 'relu', 'num_layers': 2}]], 'combined_layers': [[{'units': 7, 'activation': 'relu', 'num_layers': 3}]]}\n",
      "Error: Negative dimension size caused by subtracting 3 from 2 for 'conv2d_68/convolution' (op: 'Conv2D') with input shapes: [?,39,2,64], [3,3,64,128].\n",
      "ERROR:root:Experiment skipped due to error. Model: {'experiment_num': [257], 'valid': [True], 'num_mels': [80], 'win_len': [10], 'chunk_radius': [3], 'batch_size': [400], 'learning_rate': [0.001], 'note_class_weight': [{0: 1, 1: 20}], 'model_version': [1], 'spec_conv_layers': [[{'filters': 64, 'kernel_size': (3, 3), 'layers': 2, 'max_pool': True, 'activation': 'relu'}, {'filters': 128, 'kernel_size': (3, 3), 'layers': 2, 'max_pool': True, 'activation': 'relu'}, {'filters': 256, 'kernel_size': (3, 3), 'layers': 2, 'max_pool': True, 'activation': 'relu'}, {'filters': 512, 'kernel_size': (3, 3), 'layers': 2, 'max_pool': True, 'activation': 'relu'}]], 'spec_dense_layers': [[]], 'combined_layers': [[{'units': 7, 'activation': 'relu', 'num_layers': 3}]]}\n",
      "Error: Negative dimension size caused by subtracting 3 from 2 for 'conv2d_72/convolution' (op: 'Conv2D') with input shapes: [?,39,2,64], [3,3,64,128].\n",
      "INFO:root:Experiment skipped due to duplicates. Model: {'experiment_num': [258], 'valid': [True], 'num_mels': [80], 'win_len': [10], 'chunk_radius': [5], 'batch_size': [400], 'learning_rate': [1e-05], 'note_class_weight': [{0: 1, 1: 10}], 'model_version': [1], 'spec_conv_layers': [[{'filters': 64, 'kernel_size': (1, 11), 'layers': 4, 'max_pool': False, 'activation': 'relu'}]], 'spec_dense_layers': [[{'units': 101, 'activation': 'relu', 'num_layers': 3}]], 'combined_layers': [[{'units': 11, 'activation': 'relu', 'num_layers': 3}]]}\n",
      "INFO:root:Experiment skipped due to duplicates. Model: {'experiment_num': [258], 'valid': [True], 'num_mels': [80], 'win_len': [10], 'chunk_radius': [5], 'batch_size': [400], 'learning_rate': [1e-05], 'note_class_weight': [{0: 1, 1: 10}], 'model_version': [1], 'spec_conv_layers': [[{'filters': 64, 'kernel_size': (3, 3), 'layers': 2, 'max_pool': True, 'activation': 'relu'}, {'filters': 128, 'kernel_size': (3, 3), 'layers': 2, 'max_pool': True, 'activation': 'relu'}]], 'spec_dense_layers': [[{'units': 101, 'activation': 'relu', 'num_layers': 2}]], 'combined_layers': [[{'units': 11, 'activation': 'relu', 'num_layers': 3}]]}\n",
      "ERROR:root:Experiment skipped due to error. Model: {'experiment_num': [258], 'valid': [True], 'num_mels': [80], 'win_len': [10], 'chunk_radius': [5], 'batch_size': [400], 'learning_rate': [1e-05], 'note_class_weight': [{0: 1, 1: 10}], 'model_version': [1], 'spec_conv_layers': [[{'filters': 64, 'kernel_size': (3, 3), 'layers': 2, 'max_pool': True, 'activation': 'relu'}, {'filters': 128, 'kernel_size': (3, 3), 'layers': 2, 'max_pool': True, 'activation': 'relu'}, {'filters': 256, 'kernel_size': (3, 3), 'layers': 2, 'max_pool': True, 'activation': 'relu'}, {'filters': 512, 'kernel_size': (3, 3), 'layers': 2, 'max_pool': True, 'activation': 'relu'}]], 'spec_dense_layers': [[]], 'combined_layers': [[{'units': 11, 'activation': 'relu', 'num_layers': 3}]]}\n",
      "Error: Negative dimension size caused by subtracting 3 from 1 for 'conv2d_79/convolution' (op: 'Conv2D') with input shapes: [?,18,1,128], [3,3,128,256].\n",
      "INFO:root:Experiment skipped due to duplicates. Model: {'experiment_num': [259], 'valid': [True], 'num_mels': [80], 'win_len': [10], 'chunk_radius': [5], 'batch_size': [400], 'learning_rate': [1e-05], 'note_class_weight': [{0: 1, 1: 15}], 'model_version': [1], 'spec_conv_layers': [[{'filters': 64, 'kernel_size': (1, 11), 'layers': 4, 'max_pool': False, 'activation': 'relu'}]], 'spec_dense_layers': [[{'units': 101, 'activation': 'relu', 'num_layers': 3}]], 'combined_layers': [[{'units': 11, 'activation': 'relu', 'num_layers': 3}]]}\n",
      "INFO:root:Experiment skipped due to duplicates. Model: {'experiment_num': [259], 'valid': [True], 'num_mels': [80], 'win_len': [10], 'chunk_radius': [5], 'batch_size': [400], 'learning_rate': [1e-05], 'note_class_weight': [{0: 1, 1: 15}], 'model_version': [1], 'spec_conv_layers': [[{'filters': 64, 'kernel_size': (3, 3), 'layers': 2, 'max_pool': True, 'activation': 'relu'}, {'filters': 128, 'kernel_size': (3, 3), 'layers': 2, 'max_pool': True, 'activation': 'relu'}]], 'spec_dense_layers': [[{'units': 101, 'activation': 'relu', 'num_layers': 2}]], 'combined_layers': [[{'units': 11, 'activation': 'relu', 'num_layers': 3}]]}\n",
      "ERROR:root:Experiment skipped due to error. Model: {'experiment_num': [259], 'valid': [True], 'num_mels': [80], 'win_len': [10], 'chunk_radius': [5], 'batch_size': [400], 'learning_rate': [1e-05], 'note_class_weight': [{0: 1, 1: 15}], 'model_version': [1], 'spec_conv_layers': [[{'filters': 64, 'kernel_size': (3, 3), 'layers': 2, 'max_pool': True, 'activation': 'relu'}, {'filters': 128, 'kernel_size': (3, 3), 'layers': 2, 'max_pool': True, 'activation': 'relu'}, {'filters': 256, 'kernel_size': (3, 3), 'layers': 2, 'max_pool': True, 'activation': 'relu'}, {'filters': 512, 'kernel_size': (3, 3), 'layers': 2, 'max_pool': True, 'activation': 'relu'}]], 'spec_dense_layers': [[]], 'combined_layers': [[{'units': 11, 'activation': 'relu', 'num_layers': 3}]]}\n",
      "Error: Negative dimension size caused by subtracting 3 from 1 for 'conv2d_86/convolution' (op: 'Conv2D') with input shapes: [?,18,1,128], [3,3,128,256].\n",
      "INFO:root:Experiment skipped due to duplicates. Model: {'experiment_num': [260], 'valid': [True], 'num_mels': [80], 'win_len': [10], 'chunk_radius': [5], 'batch_size': [400], 'learning_rate': [1e-05], 'note_class_weight': [{0: 1, 1: 20}], 'model_version': [1], 'spec_conv_layers': [[{'filters': 64, 'kernel_size': (1, 11), 'layers': 4, 'max_pool': False, 'activation': 'relu'}]], 'spec_dense_layers': [[{'units': 101, 'activation': 'relu', 'num_layers': 3}]], 'combined_layers': [[{'units': 11, 'activation': 'relu', 'num_layers': 3}]]}\n",
      "INFO:root:Experiment skipped due to duplicates. Model: {'experiment_num': [260], 'valid': [True], 'num_mels': [80], 'win_len': [10], 'chunk_radius': [5], 'batch_size': [400], 'learning_rate': [1e-05], 'note_class_weight': [{0: 1, 1: 20}], 'model_version': [1], 'spec_conv_layers': [[{'filters': 64, 'kernel_size': (3, 3), 'layers': 2, 'max_pool': True, 'activation': 'relu'}, {'filters': 128, 'kernel_size': (3, 3), 'layers': 2, 'max_pool': True, 'activation': 'relu'}]], 'spec_dense_layers': [[{'units': 101, 'activation': 'relu', 'num_layers': 2}]], 'combined_layers': [[{'units': 11, 'activation': 'relu', 'num_layers': 3}]]}\n",
      "ERROR:root:Experiment skipped due to error. Model: {'experiment_num': [260], 'valid': [True], 'num_mels': [80], 'win_len': [10], 'chunk_radius': [5], 'batch_size': [400], 'learning_rate': [1e-05], 'note_class_weight': [{0: 1, 1: 20}], 'model_version': [1], 'spec_conv_layers': [[{'filters': 64, 'kernel_size': (3, 3), 'layers': 2, 'max_pool': True, 'activation': 'relu'}, {'filters': 128, 'kernel_size': (3, 3), 'layers': 2, 'max_pool': True, 'activation': 'relu'}, {'filters': 256, 'kernel_size': (3, 3), 'layers': 2, 'max_pool': True, 'activation': 'relu'}, {'filters': 512, 'kernel_size': (3, 3), 'layers': 2, 'max_pool': True, 'activation': 'relu'}]], 'spec_dense_layers': [[]], 'combined_layers': [[{'units': 11, 'activation': 'relu', 'num_layers': 3}]]}\n",
      "Error: Negative dimension size caused by subtracting 3 from 1 for 'conv2d_93/convolution' (op: 'Conv2D') with input shapes: [?,18,1,128], [3,3,128,256].\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Experiment skipped due to duplicates. Model: {'experiment_num': [261], 'valid': [True], 'num_mels': [80], 'win_len': [10], 'chunk_radius': [5], 'batch_size': [400], 'learning_rate': [0.0001], 'note_class_weight': [{0: 1, 1: 10}], 'model_version': [1], 'spec_conv_layers': [[{'filters': 64, 'kernel_size': (1, 11), 'layers': 4, 'max_pool': False, 'activation': 'relu'}]], 'spec_dense_layers': [[{'units': 101, 'activation': 'relu', 'num_layers': 3}]], 'combined_layers': [[{'units': 11, 'activation': 'relu', 'num_layers': 3}]]}\n",
      "INFO:root:Experiment skipped due to duplicates. Model: {'experiment_num': [261], 'valid': [True], 'num_mels': [80], 'win_len': [10], 'chunk_radius': [5], 'batch_size': [400], 'learning_rate': [0.0001], 'note_class_weight': [{0: 1, 1: 10}], 'model_version': [1], 'spec_conv_layers': [[{'filters': 64, 'kernel_size': (3, 3), 'layers': 2, 'max_pool': True, 'activation': 'relu'}, {'filters': 128, 'kernel_size': (3, 3), 'layers': 2, 'max_pool': True, 'activation': 'relu'}]], 'spec_dense_layers': [[{'units': 101, 'activation': 'relu', 'num_layers': 2}]], 'combined_layers': [[{'units': 11, 'activation': 'relu', 'num_layers': 3}]]}\n",
      "ERROR:root:Experiment skipped due to error. Model: {'experiment_num': [261], 'valid': [True], 'num_mels': [80], 'win_len': [10], 'chunk_radius': [5], 'batch_size': [400], 'learning_rate': [0.0001], 'note_class_weight': [{0: 1, 1: 10}], 'model_version': [1], 'spec_conv_layers': [[{'filters': 64, 'kernel_size': (3, 3), 'layers': 2, 'max_pool': True, 'activation': 'relu'}, {'filters': 128, 'kernel_size': (3, 3), 'layers': 2, 'max_pool': True, 'activation': 'relu'}, {'filters': 256, 'kernel_size': (3, 3), 'layers': 2, 'max_pool': True, 'activation': 'relu'}, {'filters': 512, 'kernel_size': (3, 3), 'layers': 2, 'max_pool': True, 'activation': 'relu'}]], 'spec_dense_layers': [[]], 'combined_layers': [[{'units': 11, 'activation': 'relu', 'num_layers': 3}]]}\n",
      "Error: Negative dimension size caused by subtracting 3 from 1 for 'conv2d_100/convolution' (op: 'Conv2D') with input shapes: [?,18,1,128], [3,3,128,256].\n",
      "INFO:root:Experiment skipped due to duplicates. Model: {'experiment_num': [262], 'valid': [True], 'num_mels': [80], 'win_len': [10], 'chunk_radius': [5], 'batch_size': [400], 'learning_rate': [0.0001], 'note_class_weight': [{0: 1, 1: 15}], 'model_version': [1], 'spec_conv_layers': [[{'filters': 64, 'kernel_size': (1, 11), 'layers': 4, 'max_pool': False, 'activation': 'relu'}]], 'spec_dense_layers': [[{'units': 101, 'activation': 'relu', 'num_layers': 3}]], 'combined_layers': [[{'units': 11, 'activation': 'relu', 'num_layers': 3}]]}\n",
      "INFO:root:Experiment skipped due to duplicates. Model: {'experiment_num': [262], 'valid': [True], 'num_mels': [80], 'win_len': [10], 'chunk_radius': [5], 'batch_size': [400], 'learning_rate': [0.0001], 'note_class_weight': [{0: 1, 1: 15}], 'model_version': [1], 'spec_conv_layers': [[{'filters': 64, 'kernel_size': (3, 3), 'layers': 2, 'max_pool': True, 'activation': 'relu'}, {'filters': 128, 'kernel_size': (3, 3), 'layers': 2, 'max_pool': True, 'activation': 'relu'}]], 'spec_dense_layers': [[{'units': 101, 'activation': 'relu', 'num_layers': 2}]], 'combined_layers': [[{'units': 11, 'activation': 'relu', 'num_layers': 3}]]}\n",
      "ERROR:root:Experiment skipped due to error. Model: {'experiment_num': [262], 'valid': [True], 'num_mels': [80], 'win_len': [10], 'chunk_radius': [5], 'batch_size': [400], 'learning_rate': [0.0001], 'note_class_weight': [{0: 1, 1: 15}], 'model_version': [1], 'spec_conv_layers': [[{'filters': 64, 'kernel_size': (3, 3), 'layers': 2, 'max_pool': True, 'activation': 'relu'}, {'filters': 128, 'kernel_size': (3, 3), 'layers': 2, 'max_pool': True, 'activation': 'relu'}, {'filters': 256, 'kernel_size': (3, 3), 'layers': 2, 'max_pool': True, 'activation': 'relu'}, {'filters': 512, 'kernel_size': (3, 3), 'layers': 2, 'max_pool': True, 'activation': 'relu'}]], 'spec_dense_layers': [[]], 'combined_layers': [[{'units': 11, 'activation': 'relu', 'num_layers': 3}]]}\n",
      "Error: Negative dimension size caused by subtracting 3 from 1 for 'conv2d_107/convolution' (op: 'Conv2D') with input shapes: [?,18,1,128], [3,3,128,256].\n",
      "INFO:root:Experiment skipped due to duplicates. Model: {'experiment_num': [263], 'valid': [True], 'num_mels': [80], 'win_len': [10], 'chunk_radius': [5], 'batch_size': [400], 'learning_rate': [0.0001], 'note_class_weight': [{0: 1, 1: 20}], 'model_version': [1], 'spec_conv_layers': [[{'filters': 64, 'kernel_size': (1, 11), 'layers': 4, 'max_pool': False, 'activation': 'relu'}]], 'spec_dense_layers': [[{'units': 101, 'activation': 'relu', 'num_layers': 3}]], 'combined_layers': [[{'units': 11, 'activation': 'relu', 'num_layers': 3}]]}\n",
      "INFO:root:Experiment skipped due to duplicates. Model: {'experiment_num': [263], 'valid': [True], 'num_mels': [80], 'win_len': [10], 'chunk_radius': [5], 'batch_size': [400], 'learning_rate': [0.0001], 'note_class_weight': [{0: 1, 1: 20}], 'model_version': [1], 'spec_conv_layers': [[{'filters': 64, 'kernel_size': (3, 3), 'layers': 2, 'max_pool': True, 'activation': 'relu'}, {'filters': 128, 'kernel_size': (3, 3), 'layers': 2, 'max_pool': True, 'activation': 'relu'}]], 'spec_dense_layers': [[{'units': 101, 'activation': 'relu', 'num_layers': 2}]], 'combined_layers': [[{'units': 11, 'activation': 'relu', 'num_layers': 3}]]}\n",
      "ERROR:root:Experiment skipped due to error. Model: {'experiment_num': [263], 'valid': [True], 'num_mels': [80], 'win_len': [10], 'chunk_radius': [5], 'batch_size': [400], 'learning_rate': [0.0001], 'note_class_weight': [{0: 1, 1: 20}], 'model_version': [1], 'spec_conv_layers': [[{'filters': 64, 'kernel_size': (3, 3), 'layers': 2, 'max_pool': True, 'activation': 'relu'}, {'filters': 128, 'kernel_size': (3, 3), 'layers': 2, 'max_pool': True, 'activation': 'relu'}, {'filters': 256, 'kernel_size': (3, 3), 'layers': 2, 'max_pool': True, 'activation': 'relu'}, {'filters': 512, 'kernel_size': (3, 3), 'layers': 2, 'max_pool': True, 'activation': 'relu'}]], 'spec_dense_layers': [[]], 'combined_layers': [[{'units': 11, 'activation': 'relu', 'num_layers': 3}]]}\n",
      "Error: Negative dimension size caused by subtracting 3 from 1 for 'conv2d_114/convolution' (op: 'Conv2D') with input shapes: [?,18,1,128], [3,3,128,256].\n",
      "INFO:root:Experiment skipped due to duplicates. Model: {'experiment_num': [264], 'valid': [True], 'num_mels': [80], 'win_len': [10], 'chunk_radius': [5], 'batch_size': [400], 'learning_rate': [0.001], 'note_class_weight': [{0: 1, 1: 10}], 'model_version': [1], 'spec_conv_layers': [[{'filters': 64, 'kernel_size': (1, 11), 'layers': 4, 'max_pool': False, 'activation': 'relu'}]], 'spec_dense_layers': [[{'units': 101, 'activation': 'relu', 'num_layers': 3}]], 'combined_layers': [[{'units': 11, 'activation': 'relu', 'num_layers': 3}]]}\n",
      "INFO:root:Experiment skipped due to duplicates. Model: {'experiment_num': [264], 'valid': [True], 'num_mels': [80], 'win_len': [10], 'chunk_radius': [5], 'batch_size': [400], 'learning_rate': [0.001], 'note_class_weight': [{0: 1, 1: 10}], 'model_version': [1], 'spec_conv_layers': [[{'filters': 64, 'kernel_size': (3, 3), 'layers': 2, 'max_pool': True, 'activation': 'relu'}, {'filters': 128, 'kernel_size': (3, 3), 'layers': 2, 'max_pool': True, 'activation': 'relu'}]], 'spec_dense_layers': [[{'units': 101, 'activation': 'relu', 'num_layers': 2}]], 'combined_layers': [[{'units': 11, 'activation': 'relu', 'num_layers': 3}]]}\n",
      "ERROR:root:Experiment skipped due to error. Model: {'experiment_num': [264], 'valid': [True], 'num_mels': [80], 'win_len': [10], 'chunk_radius': [5], 'batch_size': [400], 'learning_rate': [0.001], 'note_class_weight': [{0: 1, 1: 10}], 'model_version': [1], 'spec_conv_layers': [[{'filters': 64, 'kernel_size': (3, 3), 'layers': 2, 'max_pool': True, 'activation': 'relu'}, {'filters': 128, 'kernel_size': (3, 3), 'layers': 2, 'max_pool': True, 'activation': 'relu'}, {'filters': 256, 'kernel_size': (3, 3), 'layers': 2, 'max_pool': True, 'activation': 'relu'}, {'filters': 512, 'kernel_size': (3, 3), 'layers': 2, 'max_pool': True, 'activation': 'relu'}]], 'spec_dense_layers': [[]], 'combined_layers': [[{'units': 11, 'activation': 'relu', 'num_layers': 3}]]}\n",
      "Error: Negative dimension size caused by subtracting 3 from 1 for 'conv2d_121/convolution' (op: 'Conv2D') with input shapes: [?,18,1,128], [3,3,128,256].\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Experiment skipped due to duplicates. Model: {'experiment_num': [265], 'valid': [True], 'num_mels': [80], 'win_len': [10], 'chunk_radius': [5], 'batch_size': [400], 'learning_rate': [0.001], 'note_class_weight': [{0: 1, 1: 15}], 'model_version': [1], 'spec_conv_layers': [[{'filters': 64, 'kernel_size': (1, 11), 'layers': 4, 'max_pool': False, 'activation': 'relu'}]], 'spec_dense_layers': [[{'units': 101, 'activation': 'relu', 'num_layers': 3}]], 'combined_layers': [[{'units': 11, 'activation': 'relu', 'num_layers': 3}]]}\n",
      "INFO:root:Experiment skipped due to duplicates. Model: {'experiment_num': [265], 'valid': [True], 'num_mels': [80], 'win_len': [10], 'chunk_radius': [5], 'batch_size': [400], 'learning_rate': [0.001], 'note_class_weight': [{0: 1, 1: 15}], 'model_version': [1], 'spec_conv_layers': [[{'filters': 64, 'kernel_size': (3, 3), 'layers': 2, 'max_pool': True, 'activation': 'relu'}, {'filters': 128, 'kernel_size': (3, 3), 'layers': 2, 'max_pool': True, 'activation': 'relu'}]], 'spec_dense_layers': [[{'units': 101, 'activation': 'relu', 'num_layers': 2}]], 'combined_layers': [[{'units': 11, 'activation': 'relu', 'num_layers': 3}]]}\n",
      "ERROR:root:Experiment skipped due to error. Model: {'experiment_num': [265], 'valid': [True], 'num_mels': [80], 'win_len': [10], 'chunk_radius': [5], 'batch_size': [400], 'learning_rate': [0.001], 'note_class_weight': [{0: 1, 1: 15}], 'model_version': [1], 'spec_conv_layers': [[{'filters': 64, 'kernel_size': (3, 3), 'layers': 2, 'max_pool': True, 'activation': 'relu'}, {'filters': 128, 'kernel_size': (3, 3), 'layers': 2, 'max_pool': True, 'activation': 'relu'}, {'filters': 256, 'kernel_size': (3, 3), 'layers': 2, 'max_pool': True, 'activation': 'relu'}, {'filters': 512, 'kernel_size': (3, 3), 'layers': 2, 'max_pool': True, 'activation': 'relu'}]], 'spec_dense_layers': [[]], 'combined_layers': [[{'units': 11, 'activation': 'relu', 'num_layers': 3}]]}\n",
      "Error: Negative dimension size caused by subtracting 3 from 1 for 'conv2d_128/convolution' (op: 'Conv2D') with input shapes: [?,18,1,128], [3,3,128,256].\n",
      "INFO:root:Experiment skipped due to duplicates. Model: {'experiment_num': [266], 'valid': [True], 'num_mels': [80], 'win_len': [10], 'chunk_radius': [5], 'batch_size': [400], 'learning_rate': [0.001], 'note_class_weight': [{0: 1, 1: 20}], 'model_version': [1], 'spec_conv_layers': [[{'filters': 64, 'kernel_size': (1, 11), 'layers': 4, 'max_pool': False, 'activation': 'relu'}]], 'spec_dense_layers': [[{'units': 101, 'activation': 'relu', 'num_layers': 3}]], 'combined_layers': [[{'units': 11, 'activation': 'relu', 'num_layers': 3}]]}\n",
      "INFO:root:Experiment skipped due to duplicates. Model: {'experiment_num': [266], 'valid': [True], 'num_mels': [80], 'win_len': [10], 'chunk_radius': [5], 'batch_size': [400], 'learning_rate': [0.001], 'note_class_weight': [{0: 1, 1: 20}], 'model_version': [1], 'spec_conv_layers': [[{'filters': 64, 'kernel_size': (3, 3), 'layers': 2, 'max_pool': True, 'activation': 'relu'}, {'filters': 128, 'kernel_size': (3, 3), 'layers': 2, 'max_pool': True, 'activation': 'relu'}]], 'spec_dense_layers': [[{'units': 101, 'activation': 'relu', 'num_layers': 2}]], 'combined_layers': [[{'units': 11, 'activation': 'relu', 'num_layers': 3}]]}\n",
      "ERROR:root:Experiment skipped due to error. Model: {'experiment_num': [266], 'valid': [True], 'num_mels': [80], 'win_len': [10], 'chunk_radius': [5], 'batch_size': [400], 'learning_rate': [0.001], 'note_class_weight': [{0: 1, 1: 20}], 'model_version': [1], 'spec_conv_layers': [[{'filters': 64, 'kernel_size': (3, 3), 'layers': 2, 'max_pool': True, 'activation': 'relu'}, {'filters': 128, 'kernel_size': (3, 3), 'layers': 2, 'max_pool': True, 'activation': 'relu'}, {'filters': 256, 'kernel_size': (3, 3), 'layers': 2, 'max_pool': True, 'activation': 'relu'}, {'filters': 512, 'kernel_size': (3, 3), 'layers': 2, 'max_pool': True, 'activation': 'relu'}]], 'spec_dense_layers': [[]], 'combined_layers': [[{'units': 11, 'activation': 'relu', 'num_layers': 3}]]}\n",
      "Error: Negative dimension size caused by subtracting 3 from 1 for 'conv2d_135/convolution' (op: 'Conv2D') with input shapes: [?,18,1,128], [3,3,128,256].\n",
      "INFO:root:Experiment skipped due to duplicates. Model: {'experiment_num': [267], 'valid': [True], 'num_mels': [80], 'win_len': [10], 'chunk_radius': [7], 'batch_size': [400], 'learning_rate': [1e-05], 'note_class_weight': [{0: 1, 1: 10}], 'model_version': [1], 'spec_conv_layers': [[{'filters': 64, 'kernel_size': (1, 15), 'layers': 4, 'max_pool': False, 'activation': 'relu'}]], 'spec_dense_layers': [[{'units': 141, 'activation': 'relu', 'num_layers': 3}]], 'combined_layers': [[{'units': 15, 'activation': 'relu', 'num_layers': 3}]]}\n",
      "INFO:root:Experiment skipped due to duplicates. Model: {'experiment_num': [267], 'valid': [True], 'num_mels': [80], 'win_len': [10], 'chunk_radius': [7], 'batch_size': [400], 'learning_rate': [1e-05], 'note_class_weight': [{0: 1, 1: 10}], 'model_version': [1], 'spec_conv_layers': [[{'filters': 64, 'kernel_size': (3, 3), 'layers': 2, 'max_pool': True, 'activation': 'relu'}, {'filters': 128, 'kernel_size': (3, 3), 'layers': 2, 'max_pool': True, 'activation': 'relu'}]], 'spec_dense_layers': [[{'units': 141, 'activation': 'relu', 'num_layers': 2}]], 'combined_layers': [[{'units': 15, 'activation': 'relu', 'num_layers': 3}]]}\n",
      "ERROR:root:Experiment skipped due to error. Model: {'experiment_num': [267], 'valid': [True], 'num_mels': [80], 'win_len': [10], 'chunk_radius': [7], 'batch_size': [400], 'learning_rate': [1e-05], 'note_class_weight': [{0: 1, 1: 10}], 'model_version': [1], 'spec_conv_layers': [[{'filters': 64, 'kernel_size': (3, 3), 'layers': 2, 'max_pool': True, 'activation': 'relu'}, {'filters': 128, 'kernel_size': (3, 3), 'layers': 2, 'max_pool': True, 'activation': 'relu'}, {'filters': 256, 'kernel_size': (3, 3), 'layers': 2, 'max_pool': True, 'activation': 'relu'}, {'filters': 512, 'kernel_size': (3, 3), 'layers': 2, 'max_pool': True, 'activation': 'relu'}]], 'spec_dense_layers': [[]], 'combined_layers': [[{'units': 15, 'activation': 'relu', 'num_layers': 3}]]}\n",
      "Error: Negative dimension size caused by subtracting 3 from 2 for 'conv2d_142/convolution' (op: 'Conv2D') with input shapes: [?,18,2,128], [3,3,128,256].\n",
      "INFO:root:Experiment skipped due to duplicates. Model: {'experiment_num': [268], 'valid': [True], 'num_mels': [80], 'win_len': [10], 'chunk_radius': [7], 'batch_size': [400], 'learning_rate': [1e-05], 'note_class_weight': [{0: 1, 1: 15}], 'model_version': [1], 'spec_conv_layers': [[{'filters': 64, 'kernel_size': (1, 15), 'layers': 4, 'max_pool': False, 'activation': 'relu'}]], 'spec_dense_layers': [[{'units': 141, 'activation': 'relu', 'num_layers': 3}]], 'combined_layers': [[{'units': 15, 'activation': 'relu', 'num_layers': 3}]]}\n",
      "INFO:root:Experiment skipped due to duplicates. Model: {'experiment_num': [268], 'valid': [True], 'num_mels': [80], 'win_len': [10], 'chunk_radius': [7], 'batch_size': [400], 'learning_rate': [1e-05], 'note_class_weight': [{0: 1, 1: 15}], 'model_version': [1], 'spec_conv_layers': [[{'filters': 64, 'kernel_size': (3, 3), 'layers': 2, 'max_pool': True, 'activation': 'relu'}, {'filters': 128, 'kernel_size': (3, 3), 'layers': 2, 'max_pool': True, 'activation': 'relu'}]], 'spec_dense_layers': [[{'units': 141, 'activation': 'relu', 'num_layers': 2}]], 'combined_layers': [[{'units': 15, 'activation': 'relu', 'num_layers': 3}]]}\n",
      "ERROR:root:Experiment skipped due to error. Model: {'experiment_num': [268], 'valid': [True], 'num_mels': [80], 'win_len': [10], 'chunk_radius': [7], 'batch_size': [400], 'learning_rate': [1e-05], 'note_class_weight': [{0: 1, 1: 15}], 'model_version': [1], 'spec_conv_layers': [[{'filters': 64, 'kernel_size': (3, 3), 'layers': 2, 'max_pool': True, 'activation': 'relu'}, {'filters': 128, 'kernel_size': (3, 3), 'layers': 2, 'max_pool': True, 'activation': 'relu'}, {'filters': 256, 'kernel_size': (3, 3), 'layers': 2, 'max_pool': True, 'activation': 'relu'}, {'filters': 512, 'kernel_size': (3, 3), 'layers': 2, 'max_pool': True, 'activation': 'relu'}]], 'spec_dense_layers': [[]], 'combined_layers': [[{'units': 15, 'activation': 'relu', 'num_layers': 3}]]}\n",
      "Error: Negative dimension size caused by subtracting 3 from 2 for 'conv2d_149/convolution' (op: 'Conv2D') with input shapes: [?,18,2,128], [3,3,128,256].\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Experiment skipped due to duplicates. Model: {'experiment_num': [269], 'valid': [True], 'num_mels': [80], 'win_len': [10], 'chunk_radius': [7], 'batch_size': [400], 'learning_rate': [1e-05], 'note_class_weight': [{0: 1, 1: 20}], 'model_version': [1], 'spec_conv_layers': [[{'filters': 64, 'kernel_size': (1, 15), 'layers': 4, 'max_pool': False, 'activation': 'relu'}]], 'spec_dense_layers': [[{'units': 141, 'activation': 'relu', 'num_layers': 3}]], 'combined_layers': [[{'units': 15, 'activation': 'relu', 'num_layers': 3}]]}\n",
      "INFO:root:Experiment skipped due to duplicates. Model: {'experiment_num': [269], 'valid': [True], 'num_mels': [80], 'win_len': [10], 'chunk_radius': [7], 'batch_size': [400], 'learning_rate': [1e-05], 'note_class_weight': [{0: 1, 1: 20}], 'model_version': [1], 'spec_conv_layers': [[{'filters': 64, 'kernel_size': (3, 3), 'layers': 2, 'max_pool': True, 'activation': 'relu'}, {'filters': 128, 'kernel_size': (3, 3), 'layers': 2, 'max_pool': True, 'activation': 'relu'}]], 'spec_dense_layers': [[{'units': 141, 'activation': 'relu', 'num_layers': 2}]], 'combined_layers': [[{'units': 15, 'activation': 'relu', 'num_layers': 3}]]}\n",
      "ERROR:root:Experiment skipped due to error. Model: {'experiment_num': [269], 'valid': [True], 'num_mels': [80], 'win_len': [10], 'chunk_radius': [7], 'batch_size': [400], 'learning_rate': [1e-05], 'note_class_weight': [{0: 1, 1: 20}], 'model_version': [1], 'spec_conv_layers': [[{'filters': 64, 'kernel_size': (3, 3), 'layers': 2, 'max_pool': True, 'activation': 'relu'}, {'filters': 128, 'kernel_size': (3, 3), 'layers': 2, 'max_pool': True, 'activation': 'relu'}, {'filters': 256, 'kernel_size': (3, 3), 'layers': 2, 'max_pool': True, 'activation': 'relu'}, {'filters': 512, 'kernel_size': (3, 3), 'layers': 2, 'max_pool': True, 'activation': 'relu'}]], 'spec_dense_layers': [[]], 'combined_layers': [[{'units': 15, 'activation': 'relu', 'num_layers': 3}]]}\n",
      "Error: Negative dimension size caused by subtracting 3 from 2 for 'conv2d_156/convolution' (op: 'Conv2D') with input shapes: [?,18,2,128], [3,3,128,256].\n",
      "INFO:root:Experiment skipped due to duplicates. Model: {'experiment_num': [270], 'valid': [True], 'num_mels': [80], 'win_len': [10], 'chunk_radius': [7], 'batch_size': [400], 'learning_rate': [0.0001], 'note_class_weight': [{0: 1, 1: 10}], 'model_version': [1], 'spec_conv_layers': [[{'filters': 64, 'kernel_size': (1, 15), 'layers': 4, 'max_pool': False, 'activation': 'relu'}]], 'spec_dense_layers': [[{'units': 141, 'activation': 'relu', 'num_layers': 3}]], 'combined_layers': [[{'units': 15, 'activation': 'relu', 'num_layers': 3}]]}\n",
      "INFO:root:Experiment skipped due to duplicates. Model: {'experiment_num': [270], 'valid': [True], 'num_mels': [80], 'win_len': [10], 'chunk_radius': [7], 'batch_size': [400], 'learning_rate': [0.0001], 'note_class_weight': [{0: 1, 1: 10}], 'model_version': [1], 'spec_conv_layers': [[{'filters': 64, 'kernel_size': (3, 3), 'layers': 2, 'max_pool': True, 'activation': 'relu'}, {'filters': 128, 'kernel_size': (3, 3), 'layers': 2, 'max_pool': True, 'activation': 'relu'}]], 'spec_dense_layers': [[{'units': 141, 'activation': 'relu', 'num_layers': 2}]], 'combined_layers': [[{'units': 15, 'activation': 'relu', 'num_layers': 3}]]}\n",
      "ERROR:root:Experiment skipped due to error. Model: {'experiment_num': [270], 'valid': [True], 'num_mels': [80], 'win_len': [10], 'chunk_radius': [7], 'batch_size': [400], 'learning_rate': [0.0001], 'note_class_weight': [{0: 1, 1: 10}], 'model_version': [1], 'spec_conv_layers': [[{'filters': 64, 'kernel_size': (3, 3), 'layers': 2, 'max_pool': True, 'activation': 'relu'}, {'filters': 128, 'kernel_size': (3, 3), 'layers': 2, 'max_pool': True, 'activation': 'relu'}, {'filters': 256, 'kernel_size': (3, 3), 'layers': 2, 'max_pool': True, 'activation': 'relu'}, {'filters': 512, 'kernel_size': (3, 3), 'layers': 2, 'max_pool': True, 'activation': 'relu'}]], 'spec_dense_layers': [[]], 'combined_layers': [[{'units': 15, 'activation': 'relu', 'num_layers': 3}]]}\n",
      "Error: Negative dimension size caused by subtracting 3 from 2 for 'conv2d_163/convolution' (op: 'Conv2D') with input shapes: [?,18,2,128], [3,3,128,256].\n",
      "INFO:root:Experiment skipped due to duplicates. Model: {'experiment_num': [271], 'valid': [True], 'num_mels': [80], 'win_len': [10], 'chunk_radius': [7], 'batch_size': [400], 'learning_rate': [0.0001], 'note_class_weight': [{0: 1, 1: 15}], 'model_version': [1], 'spec_conv_layers': [[{'filters': 64, 'kernel_size': (1, 15), 'layers': 4, 'max_pool': False, 'activation': 'relu'}]], 'spec_dense_layers': [[{'units': 141, 'activation': 'relu', 'num_layers': 3}]], 'combined_layers': [[{'units': 15, 'activation': 'relu', 'num_layers': 3}]]}\n",
      "INFO:root:Experiment skipped due to duplicates. Model: {'experiment_num': [271], 'valid': [True], 'num_mels': [80], 'win_len': [10], 'chunk_radius': [7], 'batch_size': [400], 'learning_rate': [0.0001], 'note_class_weight': [{0: 1, 1: 15}], 'model_version': [1], 'spec_conv_layers': [[{'filters': 64, 'kernel_size': (3, 3), 'layers': 2, 'max_pool': True, 'activation': 'relu'}, {'filters': 128, 'kernel_size': (3, 3), 'layers': 2, 'max_pool': True, 'activation': 'relu'}]], 'spec_dense_layers': [[{'units': 141, 'activation': 'relu', 'num_layers': 2}]], 'combined_layers': [[{'units': 15, 'activation': 'relu', 'num_layers': 3}]]}\n",
      "ERROR:root:Experiment skipped due to error. Model: {'experiment_num': [271], 'valid': [True], 'num_mels': [80], 'win_len': [10], 'chunk_radius': [7], 'batch_size': [400], 'learning_rate': [0.0001], 'note_class_weight': [{0: 1, 1: 15}], 'model_version': [1], 'spec_conv_layers': [[{'filters': 64, 'kernel_size': (3, 3), 'layers': 2, 'max_pool': True, 'activation': 'relu'}, {'filters': 128, 'kernel_size': (3, 3), 'layers': 2, 'max_pool': True, 'activation': 'relu'}, {'filters': 256, 'kernel_size': (3, 3), 'layers': 2, 'max_pool': True, 'activation': 'relu'}, {'filters': 512, 'kernel_size': (3, 3), 'layers': 2, 'max_pool': True, 'activation': 'relu'}]], 'spec_dense_layers': [[]], 'combined_layers': [[{'units': 15, 'activation': 'relu', 'num_layers': 3}]]}\n",
      "Error: Negative dimension size caused by subtracting 3 from 2 for 'conv2d_170/convolution' (op: 'Conv2D') with input shapes: [?,18,2,128], [3,3,128,256].\n",
      "INFO:root:Experiment skipped due to duplicates. Model: {'experiment_num': [272], 'valid': [True], 'num_mels': [80], 'win_len': [10], 'chunk_radius': [7], 'batch_size': [400], 'learning_rate': [0.0001], 'note_class_weight': [{0: 1, 1: 20}], 'model_version': [1], 'spec_conv_layers': [[{'filters': 64, 'kernel_size': (1, 15), 'layers': 4, 'max_pool': False, 'activation': 'relu'}]], 'spec_dense_layers': [[{'units': 141, 'activation': 'relu', 'num_layers': 3}]], 'combined_layers': [[{'units': 15, 'activation': 'relu', 'num_layers': 3}]]}\n",
      "INFO:root:Experiment skipped due to duplicates. Model: {'experiment_num': [272], 'valid': [True], 'num_mels': [80], 'win_len': [10], 'chunk_radius': [7], 'batch_size': [400], 'learning_rate': [0.0001], 'note_class_weight': [{0: 1, 1: 20}], 'model_version': [1], 'spec_conv_layers': [[{'filters': 64, 'kernel_size': (3, 3), 'layers': 2, 'max_pool': True, 'activation': 'relu'}, {'filters': 128, 'kernel_size': (3, 3), 'layers': 2, 'max_pool': True, 'activation': 'relu'}]], 'spec_dense_layers': [[{'units': 141, 'activation': 'relu', 'num_layers': 2}]], 'combined_layers': [[{'units': 15, 'activation': 'relu', 'num_layers': 3}]]}\n",
      "ERROR:root:Experiment skipped due to error. Model: {'experiment_num': [272], 'valid': [True], 'num_mels': [80], 'win_len': [10], 'chunk_radius': [7], 'batch_size': [400], 'learning_rate': [0.0001], 'note_class_weight': [{0: 1, 1: 20}], 'model_version': [1], 'spec_conv_layers': [[{'filters': 64, 'kernel_size': (3, 3), 'layers': 2, 'max_pool': True, 'activation': 'relu'}, {'filters': 128, 'kernel_size': (3, 3), 'layers': 2, 'max_pool': True, 'activation': 'relu'}, {'filters': 256, 'kernel_size': (3, 3), 'layers': 2, 'max_pool': True, 'activation': 'relu'}, {'filters': 512, 'kernel_size': (3, 3), 'layers': 2, 'max_pool': True, 'activation': 'relu'}]], 'spec_dense_layers': [[]], 'combined_layers': [[{'units': 15, 'activation': 'relu', 'num_layers': 3}]]}\n",
      "Error: Negative dimension size caused by subtracting 3 from 2 for 'conv2d_177/convolution' (op: 'Conv2D') with input shapes: [?,18,2,128], [3,3,128,256].\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Experiment skipped due to duplicates. Model: {'experiment_num': [273], 'valid': [True], 'num_mels': [80], 'win_len': [10], 'chunk_radius': [7], 'batch_size': [400], 'learning_rate': [0.001], 'note_class_weight': [{0: 1, 1: 10}], 'model_version': [1], 'spec_conv_layers': [[{'filters': 64, 'kernel_size': (1, 15), 'layers': 4, 'max_pool': False, 'activation': 'relu'}]], 'spec_dense_layers': [[{'units': 141, 'activation': 'relu', 'num_layers': 3}]], 'combined_layers': [[{'units': 15, 'activation': 'relu', 'num_layers': 3}]]}\n",
      "INFO:root:Experiment skipped due to duplicates. Model: {'experiment_num': [273], 'valid': [True], 'num_mels': [80], 'win_len': [10], 'chunk_radius': [7], 'batch_size': [400], 'learning_rate': [0.001], 'note_class_weight': [{0: 1, 1: 10}], 'model_version': [1], 'spec_conv_layers': [[{'filters': 64, 'kernel_size': (3, 3), 'layers': 2, 'max_pool': True, 'activation': 'relu'}, {'filters': 128, 'kernel_size': (3, 3), 'layers': 2, 'max_pool': True, 'activation': 'relu'}]], 'spec_dense_layers': [[{'units': 141, 'activation': 'relu', 'num_layers': 2}]], 'combined_layers': [[{'units': 15, 'activation': 'relu', 'num_layers': 3}]]}\n",
      "ERROR:root:Experiment skipped due to error. Model: {'experiment_num': [273], 'valid': [True], 'num_mels': [80], 'win_len': [10], 'chunk_radius': [7], 'batch_size': [400], 'learning_rate': [0.001], 'note_class_weight': [{0: 1, 1: 10}], 'model_version': [1], 'spec_conv_layers': [[{'filters': 64, 'kernel_size': (3, 3), 'layers': 2, 'max_pool': True, 'activation': 'relu'}, {'filters': 128, 'kernel_size': (3, 3), 'layers': 2, 'max_pool': True, 'activation': 'relu'}, {'filters': 256, 'kernel_size': (3, 3), 'layers': 2, 'max_pool': True, 'activation': 'relu'}, {'filters': 512, 'kernel_size': (3, 3), 'layers': 2, 'max_pool': True, 'activation': 'relu'}]], 'spec_dense_layers': [[]], 'combined_layers': [[{'units': 15, 'activation': 'relu', 'num_layers': 3}]]}\n",
      "Error: Negative dimension size caused by subtracting 3 from 2 for 'conv2d_184/convolution' (op: 'Conv2D') with input shapes: [?,18,2,128], [3,3,128,256].\n",
      "INFO:root:Experiment skipped due to duplicates. Model: {'experiment_num': [274], 'valid': [True], 'num_mels': [80], 'win_len': [10], 'chunk_radius': [7], 'batch_size': [400], 'learning_rate': [0.001], 'note_class_weight': [{0: 1, 1: 15}], 'model_version': [1], 'spec_conv_layers': [[{'filters': 64, 'kernel_size': (1, 15), 'layers': 4, 'max_pool': False, 'activation': 'relu'}]], 'spec_dense_layers': [[{'units': 141, 'activation': 'relu', 'num_layers': 3}]], 'combined_layers': [[{'units': 15, 'activation': 'relu', 'num_layers': 3}]]}\n",
      "INFO:root:Experiment skipped due to duplicates. Model: {'experiment_num': [274], 'valid': [True], 'num_mels': [80], 'win_len': [10], 'chunk_radius': [7], 'batch_size': [400], 'learning_rate': [0.001], 'note_class_weight': [{0: 1, 1: 15}], 'model_version': [1], 'spec_conv_layers': [[{'filters': 64, 'kernel_size': (3, 3), 'layers': 2, 'max_pool': True, 'activation': 'relu'}, {'filters': 128, 'kernel_size': (3, 3), 'layers': 2, 'max_pool': True, 'activation': 'relu'}]], 'spec_dense_layers': [[{'units': 141, 'activation': 'relu', 'num_layers': 2}]], 'combined_layers': [[{'units': 15, 'activation': 'relu', 'num_layers': 3}]]}\n",
      "ERROR:root:Experiment skipped due to error. Model: {'experiment_num': [274], 'valid': [True], 'num_mels': [80], 'win_len': [10], 'chunk_radius': [7], 'batch_size': [400], 'learning_rate': [0.001], 'note_class_weight': [{0: 1, 1: 15}], 'model_version': [1], 'spec_conv_layers': [[{'filters': 64, 'kernel_size': (3, 3), 'layers': 2, 'max_pool': True, 'activation': 'relu'}, {'filters': 128, 'kernel_size': (3, 3), 'layers': 2, 'max_pool': True, 'activation': 'relu'}, {'filters': 256, 'kernel_size': (3, 3), 'layers': 2, 'max_pool': True, 'activation': 'relu'}, {'filters': 512, 'kernel_size': (3, 3), 'layers': 2, 'max_pool': True, 'activation': 'relu'}]], 'spec_dense_layers': [[]], 'combined_layers': [[{'units': 15, 'activation': 'relu', 'num_layers': 3}]]}\n",
      "Error: Negative dimension size caused by subtracting 3 from 2 for 'conv2d_191/convolution' (op: 'Conv2D') with input shapes: [?,18,2,128], [3,3,128,256].\n",
      "INFO:root:Experiment skipped due to duplicates. Model: {'experiment_num': [275], 'valid': [True], 'num_mels': [80], 'win_len': [10], 'chunk_radius': [7], 'batch_size': [400], 'learning_rate': [0.001], 'note_class_weight': [{0: 1, 1: 20}], 'model_version': [1], 'spec_conv_layers': [[{'filters': 64, 'kernel_size': (1, 15), 'layers': 4, 'max_pool': False, 'activation': 'relu'}]], 'spec_dense_layers': [[{'units': 141, 'activation': 'relu', 'num_layers': 3}]], 'combined_layers': [[{'units': 15, 'activation': 'relu', 'num_layers': 3}]]}\n",
      "INFO:root:Experiment skipped due to duplicates. Model: {'experiment_num': [275], 'valid': [True], 'num_mels': [80], 'win_len': [10], 'chunk_radius': [7], 'batch_size': [400], 'learning_rate': [0.001], 'note_class_weight': [{0: 1, 1: 20}], 'model_version': [1], 'spec_conv_layers': [[{'filters': 64, 'kernel_size': (3, 3), 'layers': 2, 'max_pool': True, 'activation': 'relu'}, {'filters': 128, 'kernel_size': (3, 3), 'layers': 2, 'max_pool': True, 'activation': 'relu'}]], 'spec_dense_layers': [[{'units': 141, 'activation': 'relu', 'num_layers': 2}]], 'combined_layers': [[{'units': 15, 'activation': 'relu', 'num_layers': 3}]]}\n",
      "ERROR:root:Experiment skipped due to error. Model: {'experiment_num': [275], 'valid': [True], 'num_mels': [80], 'win_len': [10], 'chunk_radius': [7], 'batch_size': [400], 'learning_rate': [0.001], 'note_class_weight': [{0: 1, 1: 20}], 'model_version': [1], 'spec_conv_layers': [[{'filters': 64, 'kernel_size': (3, 3), 'layers': 2, 'max_pool': True, 'activation': 'relu'}, {'filters': 128, 'kernel_size': (3, 3), 'layers': 2, 'max_pool': True, 'activation': 'relu'}, {'filters': 256, 'kernel_size': (3, 3), 'layers': 2, 'max_pool': True, 'activation': 'relu'}, {'filters': 512, 'kernel_size': (3, 3), 'layers': 2, 'max_pool': True, 'activation': 'relu'}]], 'spec_dense_layers': [[]], 'combined_layers': [[{'units': 15, 'activation': 'relu', 'num_layers': 3}]]}\n",
      "Error: Negative dimension size caused by subtracting 3 from 2 for 'conv2d_198/convolution' (op: 'Conv2D') with input shapes: [?,18,2,128], [3,3,128,256].\n",
      "INFO:root:Experiment skipped due to duplicates. Model: {'experiment_num': [276], 'valid': [True], 'num_mels': [80], 'win_len': [10], 'chunk_radius': [9], 'batch_size': [400], 'learning_rate': [1e-05], 'note_class_weight': [{0: 1, 1: 10}], 'model_version': [1], 'spec_conv_layers': [[{'filters': 64, 'kernel_size': (1, 19), 'layers': 4, 'max_pool': False, 'activation': 'relu'}]], 'spec_dense_layers': [[{'units': 181, 'activation': 'relu', 'num_layers': 3}]], 'combined_layers': [[{'units': 19, 'activation': 'relu', 'num_layers': 3}]]}\n",
      "INFO:root:Experiment skipped due to duplicates. Model: {'experiment_num': [276], 'valid': [True], 'num_mels': [80], 'win_len': [10], 'chunk_radius': [9], 'batch_size': [400], 'learning_rate': [1e-05], 'note_class_weight': [{0: 1, 1: 10}], 'model_version': [1], 'spec_conv_layers': [[{'filters': 64, 'kernel_size': (3, 3), 'layers': 2, 'max_pool': True, 'activation': 'relu'}, {'filters': 128, 'kernel_size': (3, 3), 'layers': 2, 'max_pool': True, 'activation': 'relu'}]], 'spec_dense_layers': [[{'units': 181, 'activation': 'relu', 'num_layers': 2}]], 'combined_layers': [[{'units': 19, 'activation': 'relu', 'num_layers': 3}]]}\n",
      "ERROR:root:Experiment skipped due to error. Model: {'experiment_num': [276], 'valid': [True], 'num_mels': [80], 'win_len': [10], 'chunk_radius': [9], 'batch_size': [400], 'learning_rate': [1e-05], 'note_class_weight': [{0: 1, 1: 10}], 'model_version': [1], 'spec_conv_layers': [[{'filters': 64, 'kernel_size': (3, 3), 'layers': 2, 'max_pool': True, 'activation': 'relu'}, {'filters': 128, 'kernel_size': (3, 3), 'layers': 2, 'max_pool': True, 'activation': 'relu'}, {'filters': 256, 'kernel_size': (3, 3), 'layers': 2, 'max_pool': True, 'activation': 'relu'}, {'filters': 512, 'kernel_size': (3, 3), 'layers': 2, 'max_pool': True, 'activation': 'relu'}]], 'spec_dense_layers': [[]], 'combined_layers': [[{'units': 19, 'activation': 'relu', 'num_layers': 3}]]}\n",
      "Error: Negative dimension size caused by subtracting 2 from 1 for 'max_pooling2d_57/MaxPool' (op: 'MaxPool') with input shapes: [?,16,1,256].\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Experiment skipped due to duplicates. Model: {'experiment_num': [277], 'valid': [True], 'num_mels': [80], 'win_len': [10], 'chunk_radius': [9], 'batch_size': [400], 'learning_rate': [1e-05], 'note_class_weight': [{0: 1, 1: 15}], 'model_version': [1], 'spec_conv_layers': [[{'filters': 64, 'kernel_size': (1, 19), 'layers': 4, 'max_pool': False, 'activation': 'relu'}]], 'spec_dense_layers': [[{'units': 181, 'activation': 'relu', 'num_layers': 3}]], 'combined_layers': [[{'units': 19, 'activation': 'relu', 'num_layers': 3}]]}\n",
      "INFO:root:Experiment skipped due to duplicates. Model: {'experiment_num': [277], 'valid': [True], 'num_mels': [80], 'win_len': [10], 'chunk_radius': [9], 'batch_size': [400], 'learning_rate': [1e-05], 'note_class_weight': [{0: 1, 1: 15}], 'model_version': [1], 'spec_conv_layers': [[{'filters': 64, 'kernel_size': (3, 3), 'layers': 2, 'max_pool': True, 'activation': 'relu'}, {'filters': 128, 'kernel_size': (3, 3), 'layers': 2, 'max_pool': True, 'activation': 'relu'}]], 'spec_dense_layers': [[{'units': 181, 'activation': 'relu', 'num_layers': 2}]], 'combined_layers': [[{'units': 19, 'activation': 'relu', 'num_layers': 3}]]}\n",
      "ERROR:root:Experiment skipped due to error. Model: {'experiment_num': [277], 'valid': [True], 'num_mels': [80], 'win_len': [10], 'chunk_radius': [9], 'batch_size': [400], 'learning_rate': [1e-05], 'note_class_weight': [{0: 1, 1: 15}], 'model_version': [1], 'spec_conv_layers': [[{'filters': 64, 'kernel_size': (3, 3), 'layers': 2, 'max_pool': True, 'activation': 'relu'}, {'filters': 128, 'kernel_size': (3, 3), 'layers': 2, 'max_pool': True, 'activation': 'relu'}, {'filters': 256, 'kernel_size': (3, 3), 'layers': 2, 'max_pool': True, 'activation': 'relu'}, {'filters': 512, 'kernel_size': (3, 3), 'layers': 2, 'max_pool': True, 'activation': 'relu'}]], 'spec_dense_layers': [[]], 'combined_layers': [[{'units': 19, 'activation': 'relu', 'num_layers': 3}]]}\n",
      "Error: Negative dimension size caused by subtracting 2 from 1 for 'max_pooling2d_60/MaxPool' (op: 'MaxPool') with input shapes: [?,16,1,256].\n",
      "INFO:root:Experiment skipped due to duplicates. Model: {'experiment_num': [278], 'valid': [True], 'num_mels': [80], 'win_len': [10], 'chunk_radius': [9], 'batch_size': [400], 'learning_rate': [1e-05], 'note_class_weight': [{0: 1, 1: 20}], 'model_version': [1], 'spec_conv_layers': [[{'filters': 64, 'kernel_size': (1, 19), 'layers': 4, 'max_pool': False, 'activation': 'relu'}]], 'spec_dense_layers': [[{'units': 181, 'activation': 'relu', 'num_layers': 3}]], 'combined_layers': [[{'units': 19, 'activation': 'relu', 'num_layers': 3}]]}\n",
      "INFO:root:Experiment skipped due to duplicates. Model: {'experiment_num': [278], 'valid': [True], 'num_mels': [80], 'win_len': [10], 'chunk_radius': [9], 'batch_size': [400], 'learning_rate': [1e-05], 'note_class_weight': [{0: 1, 1: 20}], 'model_version': [1], 'spec_conv_layers': [[{'filters': 64, 'kernel_size': (3, 3), 'layers': 2, 'max_pool': True, 'activation': 'relu'}, {'filters': 128, 'kernel_size': (3, 3), 'layers': 2, 'max_pool': True, 'activation': 'relu'}]], 'spec_dense_layers': [[{'units': 181, 'activation': 'relu', 'num_layers': 2}]], 'combined_layers': [[{'units': 19, 'activation': 'relu', 'num_layers': 3}]]}\n",
      "ERROR:root:Experiment skipped due to error. Model: {'experiment_num': [278], 'valid': [True], 'num_mels': [80], 'win_len': [10], 'chunk_radius': [9], 'batch_size': [400], 'learning_rate': [1e-05], 'note_class_weight': [{0: 1, 1: 20}], 'model_version': [1], 'spec_conv_layers': [[{'filters': 64, 'kernel_size': (3, 3), 'layers': 2, 'max_pool': True, 'activation': 'relu'}, {'filters': 128, 'kernel_size': (3, 3), 'layers': 2, 'max_pool': True, 'activation': 'relu'}, {'filters': 256, 'kernel_size': (3, 3), 'layers': 2, 'max_pool': True, 'activation': 'relu'}, {'filters': 512, 'kernel_size': (3, 3), 'layers': 2, 'max_pool': True, 'activation': 'relu'}]], 'spec_dense_layers': [[]], 'combined_layers': [[{'units': 19, 'activation': 'relu', 'num_layers': 3}]]}\n",
      "Error: Negative dimension size caused by subtracting 2 from 1 for 'max_pooling2d_63/MaxPool' (op: 'MaxPool') with input shapes: [?,16,1,256].\n",
      "INFO:root:Experiment skipped due to duplicates. Model: {'experiment_num': [279], 'valid': [True], 'num_mels': [80], 'win_len': [10], 'chunk_radius': [9], 'batch_size': [400], 'learning_rate': [0.0001], 'note_class_weight': [{0: 1, 1: 10}], 'model_version': [1], 'spec_conv_layers': [[{'filters': 64, 'kernel_size': (1, 19), 'layers': 4, 'max_pool': False, 'activation': 'relu'}]], 'spec_dense_layers': [[{'units': 181, 'activation': 'relu', 'num_layers': 3}]], 'combined_layers': [[{'units': 19, 'activation': 'relu', 'num_layers': 3}]]}\n",
      "INFO:root:Experiment skipped due to duplicates. Model: {'experiment_num': [279], 'valid': [True], 'num_mels': [80], 'win_len': [10], 'chunk_radius': [9], 'batch_size': [400], 'learning_rate': [0.0001], 'note_class_weight': [{0: 1, 1: 10}], 'model_version': [1], 'spec_conv_layers': [[{'filters': 64, 'kernel_size': (3, 3), 'layers': 2, 'max_pool': True, 'activation': 'relu'}, {'filters': 128, 'kernel_size': (3, 3), 'layers': 2, 'max_pool': True, 'activation': 'relu'}]], 'spec_dense_layers': [[{'units': 181, 'activation': 'relu', 'num_layers': 2}]], 'combined_layers': [[{'units': 19, 'activation': 'relu', 'num_layers': 3}]]}\n",
      "ERROR:root:Experiment skipped due to error. Model: {'experiment_num': [279], 'valid': [True], 'num_mels': [80], 'win_len': [10], 'chunk_radius': [9], 'batch_size': [400], 'learning_rate': [0.0001], 'note_class_weight': [{0: 1, 1: 10}], 'model_version': [1], 'spec_conv_layers': [[{'filters': 64, 'kernel_size': (3, 3), 'layers': 2, 'max_pool': True, 'activation': 'relu'}, {'filters': 128, 'kernel_size': (3, 3), 'layers': 2, 'max_pool': True, 'activation': 'relu'}, {'filters': 256, 'kernel_size': (3, 3), 'layers': 2, 'max_pool': True, 'activation': 'relu'}, {'filters': 512, 'kernel_size': (3, 3), 'layers': 2, 'max_pool': True, 'activation': 'relu'}]], 'spec_dense_layers': [[]], 'combined_layers': [[{'units': 19, 'activation': 'relu', 'num_layers': 3}]]}\n",
      "Error: Negative dimension size caused by subtracting 2 from 1 for 'max_pooling2d_66/MaxPool' (op: 'MaxPool') with input shapes: [?,16,1,256].\n",
      "INFO:root:Experiment skipped due to duplicates. Model: {'experiment_num': [280], 'valid': [True], 'num_mels': [80], 'win_len': [10], 'chunk_radius': [9], 'batch_size': [400], 'learning_rate': [0.0001], 'note_class_weight': [{0: 1, 1: 15}], 'model_version': [1], 'spec_conv_layers': [[{'filters': 64, 'kernel_size': (1, 19), 'layers': 4, 'max_pool': False, 'activation': 'relu'}]], 'spec_dense_layers': [[{'units': 181, 'activation': 'relu', 'num_layers': 3}]], 'combined_layers': [[{'units': 19, 'activation': 'relu', 'num_layers': 3}]]}\n",
      "INFO:root:Experiment skipped due to duplicates. Model: {'experiment_num': [280], 'valid': [True], 'num_mels': [80], 'win_len': [10], 'chunk_radius': [9], 'batch_size': [400], 'learning_rate': [0.0001], 'note_class_weight': [{0: 1, 1: 15}], 'model_version': [1], 'spec_conv_layers': [[{'filters': 64, 'kernel_size': (3, 3), 'layers': 2, 'max_pool': True, 'activation': 'relu'}, {'filters': 128, 'kernel_size': (3, 3), 'layers': 2, 'max_pool': True, 'activation': 'relu'}]], 'spec_dense_layers': [[{'units': 181, 'activation': 'relu', 'num_layers': 2}]], 'combined_layers': [[{'units': 19, 'activation': 'relu', 'num_layers': 3}]]}\n",
      "ERROR:root:Experiment skipped due to error. Model: {'experiment_num': [280], 'valid': [True], 'num_mels': [80], 'win_len': [10], 'chunk_radius': [9], 'batch_size': [400], 'learning_rate': [0.0001], 'note_class_weight': [{0: 1, 1: 15}], 'model_version': [1], 'spec_conv_layers': [[{'filters': 64, 'kernel_size': (3, 3), 'layers': 2, 'max_pool': True, 'activation': 'relu'}, {'filters': 128, 'kernel_size': (3, 3), 'layers': 2, 'max_pool': True, 'activation': 'relu'}, {'filters': 256, 'kernel_size': (3, 3), 'layers': 2, 'max_pool': True, 'activation': 'relu'}, {'filters': 512, 'kernel_size': (3, 3), 'layers': 2, 'max_pool': True, 'activation': 'relu'}]], 'spec_dense_layers': [[]], 'combined_layers': [[{'units': 19, 'activation': 'relu', 'num_layers': 3}]]}\n",
      "Error: Negative dimension size caused by subtracting 2 from 1 for 'max_pooling2d_69/MaxPool' (op: 'MaxPool') with input shapes: [?,16,1,256].\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Experiment skipped due to duplicates. Model: {'experiment_num': [281], 'valid': [True], 'num_mels': [80], 'win_len': [10], 'chunk_radius': [9], 'batch_size': [400], 'learning_rate': [0.0001], 'note_class_weight': [{0: 1, 1: 20}], 'model_version': [1], 'spec_conv_layers': [[{'filters': 64, 'kernel_size': (1, 19), 'layers': 4, 'max_pool': False, 'activation': 'relu'}]], 'spec_dense_layers': [[{'units': 181, 'activation': 'relu', 'num_layers': 3}]], 'combined_layers': [[{'units': 19, 'activation': 'relu', 'num_layers': 3}]]}\n",
      "INFO:root:Experiment skipped due to duplicates. Model: {'experiment_num': [281], 'valid': [True], 'num_mels': [80], 'win_len': [10], 'chunk_radius': [9], 'batch_size': [400], 'learning_rate': [0.0001], 'note_class_weight': [{0: 1, 1: 20}], 'model_version': [1], 'spec_conv_layers': [[{'filters': 64, 'kernel_size': (3, 3), 'layers': 2, 'max_pool': True, 'activation': 'relu'}, {'filters': 128, 'kernel_size': (3, 3), 'layers': 2, 'max_pool': True, 'activation': 'relu'}]], 'spec_dense_layers': [[{'units': 181, 'activation': 'relu', 'num_layers': 2}]], 'combined_layers': [[{'units': 19, 'activation': 'relu', 'num_layers': 3}]]}\n",
      "ERROR:root:Experiment skipped due to error. Model: {'experiment_num': [281], 'valid': [True], 'num_mels': [80], 'win_len': [10], 'chunk_radius': [9], 'batch_size': [400], 'learning_rate': [0.0001], 'note_class_weight': [{0: 1, 1: 20}], 'model_version': [1], 'spec_conv_layers': [[{'filters': 64, 'kernel_size': (3, 3), 'layers': 2, 'max_pool': True, 'activation': 'relu'}, {'filters': 128, 'kernel_size': (3, 3), 'layers': 2, 'max_pool': True, 'activation': 'relu'}, {'filters': 256, 'kernel_size': (3, 3), 'layers': 2, 'max_pool': True, 'activation': 'relu'}, {'filters': 512, 'kernel_size': (3, 3), 'layers': 2, 'max_pool': True, 'activation': 'relu'}]], 'spec_dense_layers': [[]], 'combined_layers': [[{'units': 19, 'activation': 'relu', 'num_layers': 3}]]}\n",
      "Error: Negative dimension size caused by subtracting 2 from 1 for 'max_pooling2d_72/MaxPool' (op: 'MaxPool') with input shapes: [?,16,1,256].\n",
      "INFO:root:Experiment skipped due to duplicates. Model: {'experiment_num': [282], 'valid': [True], 'num_mels': [80], 'win_len': [10], 'chunk_radius': [9], 'batch_size': [400], 'learning_rate': [0.001], 'note_class_weight': [{0: 1, 1: 10}], 'model_version': [1], 'spec_conv_layers': [[{'filters': 64, 'kernel_size': (1, 19), 'layers': 4, 'max_pool': False, 'activation': 'relu'}]], 'spec_dense_layers': [[{'units': 181, 'activation': 'relu', 'num_layers': 3}]], 'combined_layers': [[{'units': 19, 'activation': 'relu', 'num_layers': 3}]]}\n",
      "INFO:root:Experiment skipped due to duplicates. Model: {'experiment_num': [282], 'valid': [True], 'num_mels': [80], 'win_len': [10], 'chunk_radius': [9], 'batch_size': [400], 'learning_rate': [0.001], 'note_class_weight': [{0: 1, 1: 10}], 'model_version': [1], 'spec_conv_layers': [[{'filters': 64, 'kernel_size': (3, 3), 'layers': 2, 'max_pool': True, 'activation': 'relu'}, {'filters': 128, 'kernel_size': (3, 3), 'layers': 2, 'max_pool': True, 'activation': 'relu'}]], 'spec_dense_layers': [[{'units': 181, 'activation': 'relu', 'num_layers': 2}]], 'combined_layers': [[{'units': 19, 'activation': 'relu', 'num_layers': 3}]]}\n",
      "ERROR:root:Experiment skipped due to error. Model: {'experiment_num': [282], 'valid': [True], 'num_mels': [80], 'win_len': [10], 'chunk_radius': [9], 'batch_size': [400], 'learning_rate': [0.001], 'note_class_weight': [{0: 1, 1: 10}], 'model_version': [1], 'spec_conv_layers': [[{'filters': 64, 'kernel_size': (3, 3), 'layers': 2, 'max_pool': True, 'activation': 'relu'}, {'filters': 128, 'kernel_size': (3, 3), 'layers': 2, 'max_pool': True, 'activation': 'relu'}, {'filters': 256, 'kernel_size': (3, 3), 'layers': 2, 'max_pool': True, 'activation': 'relu'}, {'filters': 512, 'kernel_size': (3, 3), 'layers': 2, 'max_pool': True, 'activation': 'relu'}]], 'spec_dense_layers': [[]], 'combined_layers': [[{'units': 19, 'activation': 'relu', 'num_layers': 3}]]}\n",
      "Error: Negative dimension size caused by subtracting 2 from 1 for 'max_pooling2d_75/MaxPool' (op: 'MaxPool') with input shapes: [?,16,1,256].\n",
      "INFO:root:Experiment skipped due to duplicates. Model: {'experiment_num': [283], 'valid': [True], 'num_mels': [80], 'win_len': [10], 'chunk_radius': [9], 'batch_size': [400], 'learning_rate': [0.001], 'note_class_weight': [{0: 1, 1: 15}], 'model_version': [1], 'spec_conv_layers': [[{'filters': 64, 'kernel_size': (1, 19), 'layers': 4, 'max_pool': False, 'activation': 'relu'}]], 'spec_dense_layers': [[{'units': 181, 'activation': 'relu', 'num_layers': 3}]], 'combined_layers': [[{'units': 19, 'activation': 'relu', 'num_layers': 3}]]}\n",
      "INFO:root:Experiment skipped due to duplicates. Model: {'experiment_num': [283], 'valid': [True], 'num_mels': [80], 'win_len': [10], 'chunk_radius': [9], 'batch_size': [400], 'learning_rate': [0.001], 'note_class_weight': [{0: 1, 1: 15}], 'model_version': [1], 'spec_conv_layers': [[{'filters': 64, 'kernel_size': (3, 3), 'layers': 2, 'max_pool': True, 'activation': 'relu'}, {'filters': 128, 'kernel_size': (3, 3), 'layers': 2, 'max_pool': True, 'activation': 'relu'}]], 'spec_dense_layers': [[{'units': 181, 'activation': 'relu', 'num_layers': 2}]], 'combined_layers': [[{'units': 19, 'activation': 'relu', 'num_layers': 3}]]}\n",
      "ERROR:root:Experiment skipped due to error. Model: {'experiment_num': [283], 'valid': [True], 'num_mels': [80], 'win_len': [10], 'chunk_radius': [9], 'batch_size': [400], 'learning_rate': [0.001], 'note_class_weight': [{0: 1, 1: 15}], 'model_version': [1], 'spec_conv_layers': [[{'filters': 64, 'kernel_size': (3, 3), 'layers': 2, 'max_pool': True, 'activation': 'relu'}, {'filters': 128, 'kernel_size': (3, 3), 'layers': 2, 'max_pool': True, 'activation': 'relu'}, {'filters': 256, 'kernel_size': (3, 3), 'layers': 2, 'max_pool': True, 'activation': 'relu'}, {'filters': 512, 'kernel_size': (3, 3), 'layers': 2, 'max_pool': True, 'activation': 'relu'}]], 'spec_dense_layers': [[]], 'combined_layers': [[{'units': 19, 'activation': 'relu', 'num_layers': 3}]]}\n",
      "Error: Negative dimension size caused by subtracting 2 from 1 for 'max_pooling2d_78/MaxPool' (op: 'MaxPool') with input shapes: [?,16,1,256].\n",
      "INFO:root:Experiment skipped due to duplicates. Model: {'experiment_num': [284], 'valid': [True], 'num_mels': [80], 'win_len': [10], 'chunk_radius': [9], 'batch_size': [400], 'learning_rate': [0.001], 'note_class_weight': [{0: 1, 1: 20}], 'model_version': [1], 'spec_conv_layers': [[{'filters': 64, 'kernel_size': (1, 19), 'layers': 4, 'max_pool': False, 'activation': 'relu'}]], 'spec_dense_layers': [[{'units': 181, 'activation': 'relu', 'num_layers': 3}]], 'combined_layers': [[{'units': 19, 'activation': 'relu', 'num_layers': 3}]]}\n",
      "INFO:root:Experiment skipped due to duplicates. Model: {'experiment_num': [284], 'valid': [True], 'num_mels': [80], 'win_len': [10], 'chunk_radius': [9], 'batch_size': [400], 'learning_rate': [0.001], 'note_class_weight': [{0: 1, 1: 20}], 'model_version': [1], 'spec_conv_layers': [[{'filters': 64, 'kernel_size': (3, 3), 'layers': 2, 'max_pool': True, 'activation': 'relu'}, {'filters': 128, 'kernel_size': (3, 3), 'layers': 2, 'max_pool': True, 'activation': 'relu'}]], 'spec_dense_layers': [[{'units': 181, 'activation': 'relu', 'num_layers': 2}]], 'combined_layers': [[{'units': 19, 'activation': 'relu', 'num_layers': 3}]]}\n",
      "ERROR:root:Experiment skipped due to error. Model: {'experiment_num': [284], 'valid': [True], 'num_mels': [80], 'win_len': [10], 'chunk_radius': [9], 'batch_size': [400], 'learning_rate': [0.001], 'note_class_weight': [{0: 1, 1: 20}], 'model_version': [1], 'spec_conv_layers': [[{'filters': 64, 'kernel_size': (3, 3), 'layers': 2, 'max_pool': True, 'activation': 'relu'}, {'filters': 128, 'kernel_size': (3, 3), 'layers': 2, 'max_pool': True, 'activation': 'relu'}, {'filters': 256, 'kernel_size': (3, 3), 'layers': 2, 'max_pool': True, 'activation': 'relu'}, {'filters': 512, 'kernel_size': (3, 3), 'layers': 2, 'max_pool': True, 'activation': 'relu'}]], 'spec_dense_layers': [[]], 'combined_layers': [[{'units': 19, 'activation': 'relu', 'num_layers': 3}]]}\n",
      "Error: Negative dimension size caused by subtracting 2 from 1 for 'max_pooling2d_81/MaxPool' (op: 'MaxPool') with input shapes: [?,16,1,256].\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Experiment skipped due to duplicates. Model: {'experiment_num': [285], 'valid': [True], 'num_mels': [80], 'win_len': [10], 'chunk_radius': [11], 'batch_size': [400], 'learning_rate': [1e-05], 'note_class_weight': [{0: 1, 1: 10}], 'model_version': [1], 'spec_conv_layers': [[{'filters': 64, 'kernel_size': (1, 23), 'layers': 4, 'max_pool': False, 'activation': 'relu'}]], 'spec_dense_layers': [[{'units': 221, 'activation': 'relu', 'num_layers': 3}]], 'combined_layers': [[{'units': 23, 'activation': 'relu', 'num_layers': 3}]]}\n",
      "INFO:root:Experiment skipped due to duplicates. Model: {'experiment_num': [285], 'valid': [True], 'num_mels': [80], 'win_len': [10], 'chunk_radius': [11], 'batch_size': [400], 'learning_rate': [1e-05], 'note_class_weight': [{0: 1, 1: 10}], 'model_version': [1], 'spec_conv_layers': [[{'filters': 64, 'kernel_size': (3, 3), 'layers': 2, 'max_pool': True, 'activation': 'relu'}, {'filters': 128, 'kernel_size': (3, 3), 'layers': 2, 'max_pool': True, 'activation': 'relu'}]], 'spec_dense_layers': [[{'units': 221, 'activation': 'relu', 'num_layers': 2}]], 'combined_layers': [[{'units': 23, 'activation': 'relu', 'num_layers': 3}]]}\n",
      "ERROR:root:Experiment skipped due to error. Model: {'experiment_num': [285], 'valid': [True], 'num_mels': [80], 'win_len': [10], 'chunk_radius': [11], 'batch_size': [400], 'learning_rate': [1e-05], 'note_class_weight': [{0: 1, 1: 10}], 'model_version': [1], 'spec_conv_layers': [[{'filters': 64, 'kernel_size': (3, 3), 'layers': 2, 'max_pool': True, 'activation': 'relu'}, {'filters': 128, 'kernel_size': (3, 3), 'layers': 2, 'max_pool': True, 'activation': 'relu'}, {'filters': 256, 'kernel_size': (3, 3), 'layers': 2, 'max_pool': True, 'activation': 'relu'}, {'filters': 512, 'kernel_size': (3, 3), 'layers': 2, 'max_pool': True, 'activation': 'relu'}]], 'spec_dense_layers': [[]], 'combined_layers': [[{'units': 23, 'activation': 'relu', 'num_layers': 3}]]}\n",
      "Error: Negative dimension size caused by subtracting 3 from 1 for 'conv2d_289/convolution' (op: 'Conv2D') with input shapes: [?,8,1,256], [3,3,256,512].\n",
      "INFO:root:Experiment skipped due to duplicates. Model: {'experiment_num': [286], 'valid': [True], 'num_mels': [80], 'win_len': [10], 'chunk_radius': [11], 'batch_size': [400], 'learning_rate': [1e-05], 'note_class_weight': [{0: 1, 1: 15}], 'model_version': [1], 'spec_conv_layers': [[{'filters': 64, 'kernel_size': (1, 23), 'layers': 4, 'max_pool': False, 'activation': 'relu'}]], 'spec_dense_layers': [[{'units': 221, 'activation': 'relu', 'num_layers': 3}]], 'combined_layers': [[{'units': 23, 'activation': 'relu', 'num_layers': 3}]]}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\TJ\\.conda\\envs\\beatmap\\lib\\site-packages\\keras\\optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\TJ\\.conda\\envs\\beatmap\\lib\\site-packages\\keras\\optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\TJ\\.conda\\envs\\beatmap\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3376: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\TJ\\.conda\\envs\\beatmap\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3376: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\TJ\\.conda\\envs\\beatmap\\lib\\site-packages\\tensorflow_core\\python\\ops\\nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\TJ\\.conda\\envs\\beatmap\\lib\\site-packages\\tensorflow_core\\python\\ops\\nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\TJ\\.conda\\envs\\beatmap\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\TJ\\.conda\\envs\\beatmap\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "INFO:root:Indices are: [22400, 44800, 67200, 89200, 111200, 133200, 157200, 174400, 191600, 208800, 226000, 248800, 271600]\n",
      "INFO:root:Indices are: [14400, 28800, 43200, 57600, 77600, 101200, 124800]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\TJ\\.conda\\envs\\beatmap\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:973: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\TJ\\.conda\\envs\\beatmap\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:973: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\TJ\\.conda\\envs\\beatmap\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:2741: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\TJ\\.conda\\envs\\beatmap\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:2741: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\TJ\\.conda\\envs\\beatmap\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\TJ\\.conda\\envs\\beatmap\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\TJ\\.conda\\envs\\beatmap\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\TJ\\.conda\\envs\\beatmap\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\TJ\\.conda\\envs\\beatmap\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:190: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\TJ\\.conda\\envs\\beatmap\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:190: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\TJ\\.conda\\envs\\beatmap\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:199: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\TJ\\.conda\\envs\\beatmap\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:199: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\TJ\\.conda\\envs\\beatmap\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:206: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\TJ\\.conda\\envs\\beatmap\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:206: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "679/679 [==============================] - 96s 142ms/step - loss: 1.3300 - acc: 0.2448 - precision: 0.0698 - recall: 0.8398 - val_loss: 0.6988 - val_acc: 0.3824 - val_precision: 0.0726 - val_recall: 0.8637\n",
      "Epoch 2/50\n",
      "679/679 [==============================] - 89s 131ms/step - loss: 1.3147 - acc: 0.2634 - precision: 0.0772 - recall: 0.9245 - val_loss: 0.6965 - val_acc: 0.4986 - val_precision: 0.0808 - val_recall: 0.7779\n",
      "Epoch 3/50\n",
      "679/679 [==============================] - 89s 131ms/step - loss: 1.2788 - acc: 0.4879 - precision: 0.0929 - recall: 0.7694 - val_loss: 0.6521 - val_acc: 0.7026 - val_precision: 0.1120 - val_recall: 0.6327\n",
      "Epoch 4/50\n",
      "679/679 [==============================] - 89s 131ms/step - loss: 1.1787 - acc: 0.6510 - precision: 0.1259 - recall: 0.7201 - val_loss: 0.5693 - val_acc: 0.7818 - val_precision: 0.1419 - val_recall: 0.5838\n",
      "Epoch 5/50\n",
      "679/679 [==============================] - 89s 131ms/step - loss: 1.1861 - acc: 0.6434 - precision: 0.1224 - recall: 0.7122 - val_loss: 0.6178 - val_acc: 0.7132 - val_precision: 0.1126 - val_recall: 0.6087\n",
      "Epoch 6/50\n",
      "679/679 [==============================] - 89s 131ms/step - loss: 1.0851 - acc: 0.7310 - precision: 0.1574 - recall: 0.7050 - val_loss: 0.6450 - val_acc: 0.7014 - val_precision: 0.1173 - val_recall: 0.6750\n",
      "Epoch 7/50\n",
      "679/679 [==============================] - 89s 131ms/step - loss: 1.1909 - acc: 0.6446 - precision: 0.1205 - recall: 0.6946 - val_loss: 0.5589 - val_acc: 0.7545 - val_precision: 0.1127 - val_recall: 0.5008\n",
      "Epoch 8/50\n",
      "679/679 [==============================] - 89s 131ms/step - loss: 1.0684 - acc: 0.7810 - precision: 0.1809 - recall: 0.6556 - val_loss: 0.5382 - val_acc: 0.7933 - val_precision: 0.1470 - val_recall: 0.5705\n",
      "Epoch 9/50\n",
      "679/679 [==============================] - 89s 131ms/step - loss: 1.2238 - acc: 0.6072 - precision: 0.1076 - recall: 0.6773 - val_loss: 0.5446 - val_acc: 0.7751 - val_precision: 0.0996 - val_recall: 0.3816\n",
      "Epoch 10/50\n",
      "679/679 [==============================] - 89s 131ms/step - loss: 1.0964 - acc: 0.7645 - precision: 0.1715 - recall: 0.6689 - val_loss: 0.6128 - val_acc: 0.7650 - val_precision: 0.1367 - val_recall: 0.6121\n",
      "Epoch 11/50\n",
      "679/679 [==============================] - 88s 130ms/step - loss: 1.0946 - acc: 0.7547 - precision: 0.1656 - recall: 0.6712 - val_loss: 0.5052 - val_acc: 0.8273 - val_precision: 0.1601 - val_recall: 0.5002\n",
      "Epoch 12/50\n",
      "679/679 [==============================] - 88s 130ms/step - loss: 1.1250 - acc: 0.7300 - precision: 0.1504 - recall: 0.6639 - val_loss: 0.6191 - val_acc: 0.7413 - val_precision: 0.1263 - val_recall: 0.6220\n",
      "Epoch 13/50\n",
      "679/679 [==============================] - 88s 130ms/step - loss: 1.0745 - acc: 0.7822 - precision: 0.1820 - recall: 0.6565 - val_loss: 0.5786 - val_acc: 0.7791 - val_precision: 0.1427 - val_recall: 0.5986\n",
      "Epoch 14/50\n",
      "679/679 [==============================] - 88s 130ms/step - loss: 1.1407 - acc: 0.7139 - precision: 0.1436 - recall: 0.6708 - val_loss: 0.6508 - val_acc: 0.7050 - val_precision: 0.1131 - val_recall: 0.6340\n",
      "Epoch 15/50\n",
      "679/679 [==============================] - 88s 130ms/step - loss: 1.0544 - acc: 0.7910 - precision: 0.1905 - recall: 0.6655 - val_loss: 0.5964 - val_acc: 0.7634 - val_precision: 0.1379 - val_recall: 0.6247\n",
      "Epoch 16/50\n",
      "679/679 [==============================] - 88s 130ms/step - loss: 1.1645 - acc: 0.6849 - precision: 0.1316 - recall: 0.6728 - val_loss: 0.5649 - val_acc: 0.7675 - val_precision: 0.1241 - val_recall: 0.5291\n",
      "Epoch 17/50\n",
      "679/679 [==============================] - 88s 130ms/step - loss: 1.0563 - acc: 0.8034 - precision: 0.1970 - recall: 0.6419 - val_loss: 0.5593 - val_acc: 0.7922 - val_precision: 0.1486 - val_recall: 0.5838\n",
      "Epoch 18/50\n",
      "679/679 [==============================] - 88s 130ms/step - loss: 1.2053 - acc: 0.6349 - precision: 0.1148 - recall: 0.6741 - val_loss: 0.5960 - val_acc: 0.7276 - val_precision: 0.1056 - val_recall: 0.5260\n",
      "Epoch 19/50\n",
      "679/679 [==============================] - 88s 130ms/step - loss: 1.0731 - acc: 0.7905 - precision: 0.1893 - recall: 0.6607 - val_loss: 0.4887 - val_acc: 0.8583 - val_precision: 0.1820 - val_recall: 0.4474\n",
      "Epoch 20/50\n",
      "679/679 [==============================] - 88s 130ms/step - loss: 1.0692 - acc: 0.7851 - precision: 0.1849 - recall: 0.6602 - val_loss: 0.6361 - val_acc: 0.7421 - val_precision: 0.1293 - val_recall: 0.6396\n",
      "Epoch 21/50\n",
      "679/679 [==============================] - 88s 130ms/step - loss: 1.0983 - acc: 0.7587 - precision: 0.1663 - recall: 0.6604 - val_loss: 0.5893 - val_acc: 0.7738 - val_precision: 0.1360 - val_recall: 0.5777\n",
      "Epoch 22/50\n",
      "679/679 [==============================] - 88s 130ms/step - loss: 1.0519 - acc: 0.8039 - precision: 0.1982 - recall: 0.6456 - val_loss: 0.5669 - val_acc: 0.7974 - val_precision: 0.1520 - val_recall: 0.5823\n",
      "Epoch 23/50\n",
      "679/679 [==============================] - 88s 130ms/step - loss: 1.1235 - acc: 0.7295 - precision: 0.1506 - recall: 0.6662 - val_loss: 0.5917 - val_acc: 0.7587 - val_precision: 0.1267 - val_recall: 0.5714\n",
      "Epoch 24/50\n",
      "679/679 [==============================] - 88s 130ms/step - loss: 1.0396 - acc: 0.8055 - precision: 0.2014 - recall: 0.6550 - val_loss: 0.4600 - val_acc: 0.8456 - val_precision: 0.1777 - val_recall: 0.4945\n",
      "Epoch 25/50\n",
      "679/679 [==============================] - 88s 130ms/step - loss: 1.1538 - acc: 0.7007 - precision: 0.1376 - recall: 0.6697 - val_loss: 0.5640 - val_acc: 0.7707 - val_precision: 0.1292 - val_recall: 0.5484\n",
      "Epoch 26/50\n",
      "679/679 [==============================] - 88s 130ms/step - loss: 1.0414 - acc: 0.8189 - precision: 0.2103 - recall: 0.6315 - val_loss: 0.4966 - val_acc: 0.8308 - val_precision: 0.1676 - val_recall: 0.5198\n",
      "Epoch 27/50\n",
      "679/679 [==============================] - 88s 130ms/step - loss: 1.2006 - acc: 0.6434 - precision: 0.1162 - recall: 0.6651 - val_loss: 0.5472 - val_acc: 0.7690 - val_precision: 0.1017 - val_recall: 0.4058\n",
      "Epoch 28/50\n",
      "679/679 [==============================] - 88s 130ms/step - loss: 1.0512 - acc: 0.8041 - precision: 0.2004 - recall: 0.6569 - val_loss: 0.5524 - val_acc: 0.8165 - val_precision: 0.1611 - val_recall: 0.5515\n",
      "Epoch 29/50\n",
      "679/679 [==============================] - 88s 130ms/step - loss: 1.0486 - acc: 0.7941 - precision: 0.1926 - recall: 0.6626 - val_loss: 0.4823 - val_acc: 0.8475 - val_precision: 0.1768 - val_recall: 0.4815\n",
      "Epoch 30/50\n",
      "679/679 [==============================] - 88s 130ms/step - loss: 1.0776 - acc: 0.7731 - precision: 0.1770 - recall: 0.6663 - val_loss: 0.5876 - val_acc: 0.7852 - val_precision: 0.1430 - val_recall: 0.5784\n",
      "Epoch 31/50\n",
      "679/679 [==============================] - 88s 130ms/step - loss: 1.0375 - acc: 0.8164 - precision: 0.2085 - recall: 0.6358 - val_loss: 0.5525 - val_acc: 0.8111 - val_precision: 0.1599 - val_recall: 0.5686\n",
      "Epoch 32/50\n",
      "679/679 [==============================] - 88s 130ms/step - loss: 1.1110 - acc: 0.7419 - precision: 0.1571 - recall: 0.6654 - val_loss: 0.5789 - val_acc: 0.7731 - val_precision: 0.1308 - val_recall: 0.5500\n",
      "Epoch 33/50\n",
      "679/679 [==============================] - 88s 130ms/step - loss: 1.0281 - acc: 0.8184 - precision: 0.2127 - recall: 0.6467 - val_loss: 0.5608 - val_acc: 0.7973 - val_precision: 0.1537 - val_recall: 0.5923\n",
      "Epoch 34/50\n",
      "679/679 [==============================] - 88s 130ms/step - loss: 1.1429 - acc: 0.7065 - precision: 0.1405 - recall: 0.6725 - val_loss: 0.5886 - val_acc: 0.7403 - val_precision: 0.1193 - val_recall: 0.5794\n",
      "Epoch 35/50\n",
      "679/679 [==============================] - 88s 130ms/step - loss: 1.0278 - acc: 0.8255 - precision: 0.2171 - recall: 0.6292 - val_loss: 0.4814 - val_acc: 0.8445 - val_precision: 0.1757 - val_recall: 0.4914\n",
      "Epoch 36/50\n",
      "679/679 [==============================] - 88s 130ms/step - loss: 1.1948 - acc: 0.6535 - precision: 0.1193 - recall: 0.6650 - val_loss: 0.5500 - val_acc: 0.7642 - val_precision: 0.1078 - val_recall: 0.4488\n",
      "Epoch 37/50\n",
      "679/679 [==============================] - 88s 130ms/step - loss: 1.0389 - acc: 0.8140 - precision: 0.2102 - recall: 0.6576 - val_loss: 0.5758 - val_acc: 0.8041 - val_precision: 0.1545 - val_recall: 0.5692\n",
      "Epoch 38/50\n",
      "679/679 [==============================] - 88s 130ms/step - loss: 1.0337 - acc: 0.8072 - precision: 0.2028 - recall: 0.6539 - val_loss: 0.5327 - val_acc: 0.8177 - val_precision: 0.1613 - val_recall: 0.5474\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39/50\n",
      "679/679 [==============================] - 88s 130ms/step - loss: 1.0622 - acc: 0.7819 - precision: 0.1833 - recall: 0.6655 - val_loss: 0.6137 - val_acc: 0.7704 - val_precision: 0.1388 - val_recall: 0.6064\n",
      "Epoch 40/50\n",
      "679/679 [==============================] - 88s 130ms/step - loss: 1.0239 - acc: 0.8257 - precision: 0.2183 - recall: 0.6337 - val_loss: 0.5130 - val_acc: 0.8384 - val_precision: 0.1745 - val_recall: 0.5157\n",
      "Epoch 41/50\n",
      "679/679 [==============================] - 88s 130ms/step - loss: 1.0916 - acc: 0.7519 - precision: 0.1634 - recall: 0.6683 - val_loss: 0.5725 - val_acc: 0.7787 - val_precision: 0.1318 - val_recall: 0.5378\n",
      "Epoch 42/50\n",
      "679/679 [==============================] - 88s 130ms/step - loss: 1.0198 - acc: 0.8249 - precision: 0.2189 - recall: 0.6416 - val_loss: 0.5541 - val_acc: 0.8063 - val_precision: 0.1552 - val_recall: 0.5636\n",
      "Epoch 43/50\n",
      "679/679 [==============================] - 88s 130ms/step - loss: 1.1297 - acc: 0.7122 - precision: 0.1437 - recall: 0.6761 - val_loss: 0.4995 - val_acc: 0.8230 - val_precision: 0.1378 - val_recall: 0.4193\n",
      "Epoch 44/50\n",
      "679/679 [==============================] - 90s 133ms/step - loss: 1.0183 - acc: 0.8317 - precision: 0.2240 - recall: 0.6274 - val_loss: 0.5219 - val_acc: 0.8246 - val_precision: 0.1663 - val_recall: 0.5415\n",
      "Epoch 45/50\n",
      "679/679 [==============================] - 89s 132ms/step - loss: 1.1881 - acc: 0.6572 - precision: 0.1209 - recall: 0.6677 - val_loss: 0.5888 - val_acc: 0.7126 - val_precision: 0.0986 - val_recall: 0.5156\n",
      "Epoch 46/50\n",
      "679/679 [==============================] - 89s 131ms/step - loss: 1.0292 - acc: 0.8188 - precision: 0.2145 - recall: 0.6539 - val_loss: 0.4946 - val_acc: 0.8536 - val_precision: 0.1826 - val_recall: 0.4743\n",
      "Epoch 47/50\n",
      "679/679 [==============================] - 89s 131ms/step - loss: 1.0219 - acc: 0.8146 - precision: 0.2095 - recall: 0.6511 - val_loss: 0.5281 - val_acc: 0.8254 - val_precision: 0.1651 - val_recall: 0.5324\n",
      "Epoch 48/50\n",
      "679/679 [==============================] - 89s 131ms/step - loss: 1.0474 - acc: 0.7880 - precision: 0.1882 - recall: 0.6661 - val_loss: 0.5448 - val_acc: 0.8115 - val_precision: 0.1539 - val_recall: 0.5361\n",
      "Epoch 49/50\n",
      "679/679 [==============================] - 89s 131ms/step - loss: 1.0119 - acc: 0.8316 - precision: 0.2258 - recall: 0.6367 - val_loss: 0.5059 - val_acc: 0.8460 - val_precision: 0.1782 - val_recall: 0.4942\n",
      "Epoch 50/50\n",
      "679/679 [==============================] - 89s 131ms/step - loss: 1.0866 - acc: 0.7502 - precision: 0.1642 - recall: 0.6797 - val_loss: 0.5046 - val_acc: 0.8263 - val_precision: 0.1395 - val_recall: 0.4146\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Experiment skipped due to error. Model: {'experiment_num': [287], 'valid': [True], 'num_mels': [80], 'win_len': [10], 'chunk_radius': [11], 'batch_size': [400], 'learning_rate': [1e-05], 'note_class_weight': [{0: 1, 1: 15}], 'model_version': [1], 'spec_conv_layers': [[{'filters': 64, 'kernel_size': (3, 3), 'layers': 2, 'max_pool': True, 'activation': 'relu'}, {'filters': 128, 'kernel_size': (3, 3), 'layers': 2, 'max_pool': True, 'activation': 'relu'}, {'filters': 256, 'kernel_size': (3, 3), 'layers': 2, 'max_pool': True, 'activation': 'relu'}, {'filters': 512, 'kernel_size': (3, 3), 'layers': 2, 'max_pool': True, 'activation': 'relu'}]], 'spec_dense_layers': [[]], 'combined_layers': [[{'units': 23, 'activation': 'relu', 'num_layers': 3}]]}\n",
      "Error: Negative dimension size caused by subtracting 3 from 1 for 'conv2d_305/convolution' (op: 'Conv2D') with input shapes: [?,8,1,256], [3,3,256,512].\n",
      "INFO:root:Indices are: [22400, 44800, 67200, 89200, 111200, 133200, 157200, 174400, 191600, 208800, 226000, 248800, 271600]\n",
      "INFO:root:Indices are: [14400, 28800, 43200, 57600, 77600, 101200, 124800]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "679/679 [==============================] - 21s 31ms/step - loss: 1.5367 - acc: 0.1286 - precision: 0.0667 - recall: 0.9292 - val_loss: 0.8829 - val_acc: 0.0558 - val_precision: 0.0553 - val_recall: 1.0000\n",
      "Epoch 2/50\n",
      "679/679 [==============================] - 18s 27ms/step - loss: 1.5184 - acc: 0.0667 - precision: 0.0668 - recall: 1.0000 - val_loss: 0.8643 - val_acc: 0.0575 - val_precision: 0.0554 - val_recall: 1.0000\n",
      "Epoch 3/50\n",
      "679/679 [==============================] - 19s 27ms/step - loss: 1.5039 - acc: 0.0694 - precision: 0.0669 - recall: 0.9994 - val_loss: 0.7569 - val_acc: 0.1325 - val_precision: 0.0588 - val_recall: 0.9802\n",
      "Epoch 4/50\n",
      "679/679 [==============================] - 18s 27ms/step - loss: 1.4296 - acc: 0.2369 - precision: 0.0759 - recall: 0.9352 - val_loss: 0.6427 - val_acc: 0.7366 - val_precision: 0.1228 - val_recall: 0.6129\n",
      "Epoch 5/50\n",
      "679/679 [==============================] - 19s 27ms/step - loss: 1.3967 - acc: 0.4216 - precision: 0.0892 - recall: 0.8408 - val_loss: 0.7632 - val_acc: 0.4975 - val_precision: 0.0822 - val_recall: 0.7955\n",
      "Epoch 6/50\n",
      "679/679 [==============================] - 18s 27ms/step - loss: 1.2963 - acc: 0.5907 - precision: 0.1172 - recall: 0.7946 - val_loss: 0.8322 - val_acc: 0.5171 - val_precision: 0.0830 - val_recall: 0.7696\n",
      "Epoch 7/50\n",
      "679/679 [==============================] - 18s 27ms/step - loss: 1.3759 - acc: 0.4973 - precision: 0.0974 - recall: 0.7983 - val_loss: 0.7040 - val_acc: 0.6026 - val_precision: 0.0938 - val_recall: 0.7144\n",
      "Epoch 8/50\n",
      "679/679 [==============================] - 18s 27ms/step - loss: 1.2525 - acc: 0.6660 - precision: 0.1354 - recall: 0.7524 - val_loss: 0.6278 - val_acc: 0.7476 - val_precision: 0.1307 - val_recall: 0.6311\n",
      "Epoch 9/50\n",
      "679/679 [==============================] - 18s 27ms/step - loss: 1.4022 - acc: 0.4525 - precision: 0.0910 - recall: 0.8099 - val_loss: 0.7312 - val_acc: 0.5525 - val_precision: 0.0870 - val_recall: 0.7472\n",
      "Epoch 10/50\n",
      "679/679 [==============================] - 18s 26ms/step - loss: 1.2932 - acc: 0.6594 - precision: 0.1314 - recall: 0.7403 - val_loss: 0.7274 - val_acc: 0.6926 - val_precision: 0.1120 - val_recall: 0.6582\n",
      "Epoch 11/50\n",
      "679/679 [==============================] - 18s 27ms/step - loss: 1.2736 - acc: 0.6686 - precision: 0.1349 - recall: 0.7412 - val_loss: 0.6934 - val_acc: 0.7207 - val_precision: 0.1203 - val_recall: 0.6425\n",
      "Epoch 12/50\n",
      "679/679 [==============================] - 18s 27ms/step - loss: 1.3009 - acc: 0.6381 - precision: 0.1249 - recall: 0.7447 - val_loss: 0.7417 - val_acc: 0.6498 - val_precision: 0.1025 - val_recall: 0.6882\n",
      "Epoch 13/50\n",
      "679/679 [==============================] - 18s 27ms/step - loss: 1.2572 - acc: 0.6944 - precision: 0.1425 - recall: 0.7222 - val_loss: 0.7113 - val_acc: 0.7161 - val_precision: 0.1198 - val_recall: 0.6514\n",
      "Epoch 14/50\n",
      "679/679 [==============================] - 18s 27ms/step - loss: 1.3161 - acc: 0.5994 - precision: 0.1160 - recall: 0.7646 - val_loss: 0.7459 - val_acc: 0.6250 - val_precision: 0.0980 - val_recall: 0.7050\n",
      "Epoch 15/50\n",
      "679/679 [==============================] - 18s 27ms/step - loss: 1.2342 - acc: 0.7142 - precision: 0.1527 - recall: 0.7301 - val_loss: 0.6404 - val_acc: 0.7600 - val_precision: 0.1373 - val_recall: 0.6330\n",
      "Epoch 16/50\n",
      "679/679 [==============================] - 18s 27ms/step - loss: 1.3416 - acc: 0.5632 - precision: 0.1079 - recall: 0.7715 - val_loss: 0.7377 - val_acc: 0.6080 - val_precision: 0.0941 - val_recall: 0.7063\n",
      "Epoch 17/50\n",
      "679/679 [==============================] - 18s 27ms/step - loss: 1.2346 - acc: 0.7227 - precision: 0.1532 - recall: 0.7059 - val_loss: 0.7322 - val_acc: 0.7088 - val_precision: 0.1182 - val_recall: 0.6607\n",
      "Epoch 18/50\n",
      "679/679 [==============================] - 18s 27ms/step - loss: 1.3825 - acc: 0.5151 - precision: 0.0989 - recall: 0.7807 - val_loss: 0.8858 - val_acc: 0.4342 - val_precision: 0.0764 - val_recall: 0.8328\n",
      "Epoch 19/50\n",
      "679/679 [==============================] - 18s 27ms/step - loss: 1.2689 - acc: 0.6863 - precision: 0.1410 - recall: 0.7352 - val_loss: 0.6999 - val_acc: 0.7294 - val_precision: 0.1213 - val_recall: 0.6242\n",
      "Epoch 20/50\n",
      "679/679 [==============================] - 18s 27ms/step - loss: 1.2502 - acc: 0.7061 - precision: 0.1475 - recall: 0.7209 - val_loss: 0.7343 - val_acc: 0.7002 - val_precision: 0.1152 - val_recall: 0.6621\n",
      "Epoch 21/50\n",
      "679/679 [==============================] - 18s 27ms/step - loss: 1.2841 - acc: 0.6605 - precision: 0.1306 - recall: 0.7311 - val_loss: 0.7195 - val_acc: 0.6872 - val_precision: 0.1106 - val_recall: 0.6621\n",
      "Epoch 22/50\n",
      "679/679 [==============================] - 18s 27ms/step - loss: 1.2427 - acc: 0.7302 - precision: 0.1554 - recall: 0.6947 - val_loss: 0.7546 - val_acc: 0.6981 - val_precision: 0.1148 - val_recall: 0.6649\n",
      "Epoch 23/50\n",
      "679/679 [==============================] - 18s 27ms/step - loss: 1.2982 - acc: 0.6233 - precision: 0.1219 - recall: 0.7576 - val_loss: 0.5826 - val_acc: 0.7917 - val_precision: 0.1398 - val_recall: 0.5371\n",
      "Epoch 24/50\n",
      "679/679 [==============================] - 18s 27ms/step - loss: 1.2203 - acc: 0.7328 - precision: 0.1597 - recall: 0.7137 - val_loss: 0.6620 - val_acc: 0.7499 - val_precision: 0.1339 - val_recall: 0.6450\n",
      "Epoch 25/50\n",
      "679/679 [==============================] - 18s 27ms/step - loss: 1.3342 - acc: 0.5797 - precision: 0.1108 - recall: 0.7621 - val_loss: 0.7448 - val_acc: 0.6058 - val_precision: 0.0943 - val_recall: 0.7128\n",
      "Epoch 26/50\n",
      "679/679 [==============================] - 18s 27ms/step - loss: 1.2228 - acc: 0.7341 - precision: 0.1586 - recall: 0.7025 - val_loss: 0.6851 - val_acc: 0.7379 - val_precision: 0.1281 - val_recall: 0.6446\n",
      "Epoch 27/50\n",
      "679/679 [==============================] - 19s 28ms/step - loss: 1.3815 - acc: 0.5131 - precision: 0.0987 - recall: 0.7822 - val_loss: 0.8645 - val_acc: 0.4573 - val_precision: 0.0780 - val_recall: 0.8154\n",
      "Epoch 28/50\n",
      "679/679 [==============================] - 19s 28ms/step - loss: 1.2563 - acc: 0.7154 - precision: 0.1519 - recall: 0.7210 - val_loss: 0.6782 - val_acc: 0.7577 - val_precision: 0.1301 - val_recall: 0.5949\n",
      "Epoch 29/50\n",
      "679/679 [==============================] - 19s 28ms/step - loss: 1.2389 - acc: 0.7164 - precision: 0.1518 - recall: 0.7170 - val_loss: 0.6260 - val_acc: 0.7829 - val_precision: 0.1425 - val_recall: 0.5835\n",
      "Epoch 30/50\n",
      "679/679 [==============================] - 19s 28ms/step - loss: 1.2714 - acc: 0.6791 - precision: 0.1367 - recall: 0.7245 - val_loss: 0.6425 - val_acc: 0.7679 - val_precision: 0.1323 - val_recall: 0.5753\n",
      "Epoch 31/50\n",
      "679/679 [==============================] - 19s 28ms/step - loss: 1.2325 - acc: 0.7360 - precision: 0.1590 - recall: 0.6982 - val_loss: 0.6144 - val_acc: 0.7886 - val_precision: 0.1445 - val_recall: 0.5740\n",
      "Epoch 32/50\n",
      "679/679 [==============================] - 19s 28ms/step - loss: 1.2949 - acc: 0.6363 - precision: 0.1247 - recall: 0.7480 - val_loss: 0.6069 - val_acc: 0.7732 - val_precision: 0.1329 - val_recall: 0.5619\n",
      "Epoch 33/50\n",
      "679/679 [==============================] - 19s 28ms/step - loss: 1.2138 - acc: 0.7432 - precision: 0.1647 - recall: 0.7083 - val_loss: 0.6746 - val_acc: 0.7469 - val_precision: 0.1331 - val_recall: 0.6492\n",
      "Epoch 34/50\n",
      "679/679 [==============================] - 19s 27ms/step - loss: 1.3262 - acc: 0.5974 - precision: 0.1145 - recall: 0.7558 - val_loss: 0.6544 - val_acc: 0.7162 - val_precision: 0.1137 - val_recall: 0.6087\n",
      "Epoch 35/50\n",
      "679/679 [==============================] - 19s 28ms/step - loss: 1.2124 - acc: 0.7366 - precision: 0.1609 - recall: 0.7082 - val_loss: 0.5661 - val_acc: 0.8103 - val_precision: 0.1575 - val_recall: 0.5594\n",
      "Epoch 36/50\n",
      "679/679 [==============================] - 19s 28ms/step - loss: 1.3826 - acc: 0.5172 - precision: 0.0983 - recall: 0.7711 - val_loss: 0.7918 - val_acc: 0.5156 - val_precision: 0.0832 - val_recall: 0.7753\n",
      "Epoch 37/50\n",
      "679/679 [==============================] - 18s 27ms/step - loss: 1.2470 - acc: 0.7174 - precision: 0.1534 - recall: 0.7246 - val_loss: 0.6289 - val_acc: 0.7899 - val_precision: 0.1408 - val_recall: 0.5488\n",
      "Epoch 38/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "679/679 [==============================] - 19s 28ms/step - loss: 1.2319 - acc: 0.7269 - precision: 0.1562 - recall: 0.7114 - val_loss: 0.6205 - val_acc: 0.7873 - val_precision: 0.1446 - val_recall: 0.5796\n",
      "Epoch 39/50\n",
      "679/679 [==============================] - 18s 27ms/step - loss: 1.2623 - acc: 0.6889 - precision: 0.1401 - recall: 0.7212 - val_loss: 0.7198 - val_acc: 0.6981 - val_precision: 0.1128 - val_recall: 0.6500\n",
      "Epoch 40/50\n",
      "679/679 [==============================] - 19s 28ms/step - loss: 1.2234 - acc: 0.7462 - precision: 0.1640 - recall: 0.6930 - val_loss: 0.6651 - val_acc: 0.7625 - val_precision: 0.1355 - val_recall: 0.6129\n",
      "Epoch 41/50\n",
      "679/679 [==============================] - 19s 28ms/step - loss: 1.2852 - acc: 0.6428 - precision: 0.1275 - recall: 0.7530 - val_loss: 0.6275 - val_acc: 0.7508 - val_precision: 0.1259 - val_recall: 0.5904\n",
      "Epoch 42/50\n",
      "679/679 [==============================] - 19s 28ms/step - loss: 1.2057 - acc: 0.7432 - precision: 0.1660 - recall: 0.7166 - val_loss: 0.7591 - val_acc: 0.6805 - val_precision: 0.1106 - val_recall: 0.6793\n",
      "Epoch 43/50\n",
      "679/679 [==============================] - 19s 27ms/step - loss: 1.3221 - acc: 0.5968 - precision: 0.1147 - recall: 0.7591 - val_loss: 0.6163 - val_acc: 0.7462 - val_precision: 0.1212 - val_recall: 0.5749\n",
      "Epoch 44/50\n",
      "679/679 [==============================] - 19s 28ms/step - loss: 1.2072 - acc: 0.7318 - precision: 0.1592 - recall: 0.7141 - val_loss: 0.5596 - val_acc: 0.8091 - val_precision: 0.1563 - val_recall: 0.5578\n",
      "Epoch 45/50\n",
      "679/679 [==============================] - 19s 28ms/step - loss: 1.3761 - acc: 0.5259 - precision: 0.1002 - recall: 0.7731 - val_loss: 0.6776 - val_acc: 0.6468 - val_precision: 0.0984 - val_recall: 0.6602\n",
      "Epoch 46/50\n",
      "679/679 [==============================] - 19s 27ms/step - loss: 1.2337 - acc: 0.7225 - precision: 0.1563 - recall: 0.7271 - val_loss: 0.6521 - val_acc: 0.7711 - val_precision: 0.1350 - val_recall: 0.5809\n",
      "Epoch 47/50\n",
      "679/679 [==============================] - 19s 28ms/step - loss: 1.2207 - acc: 0.7262 - precision: 0.1565 - recall: 0.7156 - val_loss: 0.5839 - val_acc: 0.8023 - val_precision: 0.1501 - val_recall: 0.5531\n",
      "Epoch 48/50\n",
      "679/679 [==============================] - 19s 27ms/step - loss: 1.2531 - acc: 0.6906 - precision: 0.1417 - recall: 0.7277 - val_loss: 0.6403 - val_acc: 0.7583 - val_precision: 0.1293 - val_recall: 0.5882\n",
      "Epoch 49/50\n",
      "679/679 [==============================] - 19s 28ms/step - loss: 1.2147 - acc: 0.7398 - precision: 0.1616 - recall: 0.7012 - val_loss: 0.6810 - val_acc: 0.7264 - val_precision: 0.1232 - val_recall: 0.6459\n",
      "Epoch 50/50\n",
      "679/679 [==============================] - 19s 28ms/step - loss: 1.2819 - acc: 0.6442 - precision: 0.1276 - recall: 0.7505 - val_loss: 0.6290 - val_acc: 0.7592 - val_precision: 0.1289 - val_recall: 0.5829\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Indices are: [22400, 44800, 67200, 89200, 111200, 133200, 157200, 174400, 191600, 208800, 226000, 248800, 271600]\n",
      "INFO:root:Indices are: [14400, 28800, 43200, 57600, 77600, 101200, 124800]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "679/679 [==============================] - 89s 131ms/step - loss: 1.5261 - acc: 0.0661 - precision: 0.0667 - recall: 1.0000 - val_loss: 0.8085 - val_acc: 0.0553 - val_precision: 0.0553 - val_recall: 1.0000\n",
      "Epoch 2/50\n",
      "679/679 [==============================] - 88s 130ms/step - loss: 1.5036 - acc: 0.0676 - precision: 0.0668 - recall: 1.0000 - val_loss: 0.7765 - val_acc: 0.1506 - val_precision: 0.0608 - val_recall: 0.9949\n",
      "Epoch 3/50\n",
      "679/679 [==============================] - 89s 130ms/step - loss: 1.4538 - acc: 0.1914 - precision: 0.0726 - recall: 0.9469 - val_loss: 0.7879 - val_acc: 0.4200 - val_precision: 0.0776 - val_recall: 0.8721\n",
      "Epoch 4/50\n",
      "679/679 [==============================] - 89s 130ms/step - loss: 1.3373 - acc: 0.5114 - precision: 0.1003 - recall: 0.8020 - val_loss: 0.7001 - val_acc: 0.7301 - val_precision: 0.1218 - val_recall: 0.6249\n",
      "Epoch 5/50\n",
      "679/679 [==============================] - 88s 130ms/step - loss: 1.3705 - acc: 0.4995 - precision: 0.0979 - recall: 0.8000 - val_loss: 0.6994 - val_acc: 0.6858 - val_precision: 0.1014 - val_recall: 0.5958\n",
      "Epoch 6/50\n",
      "679/679 [==============================] - 88s 130ms/step - loss: 1.2550 - acc: 0.6702 - precision: 0.1359 - recall: 0.7446 - val_loss: 0.7296 - val_acc: 0.7006 - val_precision: 0.1169 - val_recall: 0.6738\n",
      "Epoch 7/50\n",
      "679/679 [==============================] - 88s 130ms/step - loss: 1.3731 - acc: 0.5112 - precision: 0.0988 - recall: 0.7866 - val_loss: 0.7637 - val_acc: 0.5571 - val_precision: 0.0890 - val_recall: 0.7590\n",
      "Epoch 8/50\n",
      "679/679 [==============================] - 88s 130ms/step - loss: 1.2277 - acc: 0.7335 - precision: 0.1566 - recall: 0.6912 - val_loss: 0.6865 - val_acc: 0.7440 - val_precision: 0.1289 - val_recall: 0.6311\n",
      "Epoch 9/50\n",
      "679/679 [==============================] - 88s 130ms/step - loss: 1.4026 - acc: 0.4495 - precision: 0.0900 - recall: 0.8039 - val_loss: 0.7084 - val_acc: 0.5986 - val_precision: 0.0867 - val_recall: 0.6573\n",
      "Epoch 10/50\n",
      "679/679 [==============================] - 88s 130ms/step - loss: 1.2405 - acc: 0.7213 - precision: 0.1530 - recall: 0.7094 - val_loss: 0.5979 - val_acc: 0.8223 - val_precision: 0.1583 - val_recall: 0.5131\n",
      "Epoch 11/50\n",
      "679/679 [==============================] - 89s 131ms/step - loss: 1.2489 - acc: 0.7012 - precision: 0.1455 - recall: 0.7225 - val_loss: 0.6563 - val_acc: 0.7671 - val_precision: 0.1362 - val_recall: 0.6015\n",
      "Epoch 12/50\n",
      "679/679 [==============================] - 89s 130ms/step - loss: 1.2785 - acc: 0.6664 - precision: 0.1321 - recall: 0.7262 - val_loss: 0.7455 - val_acc: 0.6785 - val_precision: 0.1108 - val_recall: 0.6860\n",
      "Epoch 13/50\n",
      "679/679 [==============================] - 89s 130ms/step - loss: 1.2217 - acc: 0.7451 - precision: 0.1631 - recall: 0.6914 - val_loss: 0.6627 - val_acc: 0.7584 - val_precision: 0.1346 - val_recall: 0.6210\n",
      "Epoch 14/50\n",
      "679/679 [==============================] - 88s 130ms/step - loss: 1.3089 - acc: 0.6362 - precision: 0.1223 - recall: 0.7287 - val_loss: 0.7524 - val_acc: 0.6389 - val_precision: 0.1019 - val_recall: 0.7079\n",
      "Epoch 15/50\n",
      "679/679 [==============================] - 89s 131ms/step - loss: 1.2088 - acc: 0.7476 - precision: 0.1667 - recall: 0.7049 - val_loss: 0.5988 - val_acc: 0.7854 - val_precision: 0.1475 - val_recall: 0.6033\n",
      "Epoch 16/50\n",
      "679/679 [==============================] - 89s 130ms/step - loss: 1.3418 - acc: 0.5795 - precision: 0.1096 - recall: 0.7522 - val_loss: 0.6689 - val_acc: 0.7201 - val_precision: 0.1158 - val_recall: 0.6125\n",
      "Epoch 17/50\n",
      "679/679 [==============================] - 89s 131ms/step - loss: 1.2038 - acc: 0.7577 - precision: 0.1708 - recall: 0.6912 - val_loss: 0.6487 - val_acc: 0.7578 - val_precision: 0.1367 - val_recall: 0.6359\n",
      "Epoch 18/50\n",
      "679/679 [==============================] - 89s 130ms/step - loss: 1.3825 - acc: 0.5020 - precision: 0.0969 - recall: 0.7844 - val_loss: 0.6889 - val_acc: 0.6567 - val_precision: 0.0963 - val_recall: 0.6220\n",
      "Epoch 19/50\n",
      "679/679 [==============================] - 89s 131ms/step - loss: 1.2187 - acc: 0.7428 - precision: 0.1648 - recall: 0.7099 - val_loss: 0.6307 - val_acc: 0.8008 - val_precision: 0.1516 - val_recall: 0.5667\n",
      "Epoch 20/50\n",
      "679/679 [==============================] - 88s 130ms/step - loss: 1.2159 - acc: 0.7342 - precision: 0.1597 - recall: 0.7084 - val_loss: 0.6455 - val_acc: 0.7616 - val_precision: 0.1361 - val_recall: 0.6197\n",
      "Epoch 21/50\n",
      "679/679 [==============================] - 89s 130ms/step - loss: 1.2523 - acc: 0.6900 - precision: 0.1416 - recall: 0.7276 - val_loss: 0.7041 - val_acc: 0.7146 - val_precision: 0.1209 - val_recall: 0.6640\n",
      "Epoch 22/50\n",
      "679/679 [==============================] - 89s 130ms/step - loss: 1.2025 - acc: 0.7553 - precision: 0.1693 - recall: 0.6917 - val_loss: 0.6259 - val_acc: 0.7646 - val_precision: 0.1384 - val_recall: 0.6235\n",
      "Epoch 23/50\n",
      "679/679 [==============================] - 89s 131ms/step - loss: 1.2835 - acc: 0.6507 - precision: 0.1284 - recall: 0.7394 - val_loss: 0.6824 - val_acc: 0.7233 - val_precision: 0.1203 - val_recall: 0.6350\n",
      "Epoch 24/50\n",
      "679/679 [==============================] - 89s 130ms/step - loss: 1.1891 - acc: 0.7690 - precision: 0.1786 - recall: 0.6926 - val_loss: 0.5878 - val_acc: 0.7996 - val_precision: 0.1541 - val_recall: 0.5845\n",
      "Epoch 25/50\n",
      "679/679 [==============================] - 89s 131ms/step - loss: 1.3223 - acc: 0.6029 - precision: 0.1157 - recall: 0.7529 - val_loss: 0.6426 - val_acc: 0.7380 - val_precision: 0.1233 - val_recall: 0.6124\n",
      "Epoch 26/50\n",
      "679/679 [==============================] - 89s 130ms/step - loss: 1.1909 - acc: 0.7658 - precision: 0.1758 - recall: 0.6896 - val_loss: 0.6351 - val_acc: 0.7753 - val_precision: 0.1441 - val_recall: 0.6207\n",
      "Epoch 27/50\n",
      "679/679 [==============================] - 88s 130ms/step - loss: 1.3746 - acc: 0.5321 - precision: 0.1006 - recall: 0.7647 - val_loss: 0.6983 - val_acc: 0.6368 - val_precision: 0.0957 - val_recall: 0.6593\n",
      "Epoch 28/50\n",
      "679/679 [==============================] - 89s 130ms/step - loss: 1.2017 - acc: 0.7566 - precision: 0.1730 - recall: 0.7090 - val_loss: 0.6935 - val_acc: 0.7592 - val_precision: 0.1333 - val_recall: 0.6103\n",
      "Epoch 29/50\n",
      "679/679 [==============================] - 88s 130ms/step - loss: 1.1967 - acc: 0.7444 - precision: 0.1660 - recall: 0.7123 - val_loss: 0.6031 - val_acc: 0.7784 - val_precision: 0.1438 - val_recall: 0.6075\n",
      "Epoch 30/50\n",
      "679/679 [==============================] - 89s 130ms/step - loss: 1.2299 - acc: 0.7082 - precision: 0.1493 - recall: 0.7263 - val_loss: 0.7229 - val_acc: 0.6836 - val_precision: 0.1129 - val_recall: 0.6895\n",
      "Epoch 31/50\n",
      "679/679 [==============================] - 89s 130ms/step - loss: 1.1913 - acc: 0.7590 - precision: 0.1723 - recall: 0.6956 - val_loss: 0.6277 - val_acc: 0.7852 - val_precision: 0.1481 - val_recall: 0.6078\n",
      "Epoch 32/50\n",
      "679/679 [==============================] - 89s 130ms/step - loss: 1.2615 - acc: 0.6727 - precision: 0.1363 - recall: 0.7400 - val_loss: 0.6740 - val_acc: 0.7222 - val_precision: 0.1204 - val_recall: 0.6387\n",
      "Epoch 33/50\n",
      "679/679 [==============================] - 88s 130ms/step - loss: 1.1761 - acc: 0.7765 - precision: 0.1840 - recall: 0.6933 - val_loss: 0.6930 - val_acc: 0.7307 - val_precision: 0.1275 - val_recall: 0.6629\n",
      "Epoch 34/50\n",
      "679/679 [==============================] - 89s 131ms/step - loss: 1.3016 - acc: 0.6173 - precision: 0.1201 - recall: 0.7564 - val_loss: 0.6798 - val_acc: 0.6730 - val_precision: 0.1070 - val_recall: 0.6692\n",
      "Epoch 35/50\n",
      "679/679 [==============================] - 89s 130ms/step - loss: 1.1796 - acc: 0.7692 - precision: 0.1784 - recall: 0.6913 - val_loss: 0.6344 - val_acc: 0.7644 - val_precision: 0.1405 - val_recall: 0.6374\n",
      "Epoch 36/50\n",
      "679/679 [==============================] - 89s 130ms/step - loss: 1.3640 - acc: 0.5341 - precision: 0.1025 - recall: 0.7787 - val_loss: 0.7613 - val_acc: 0.5629 - val_precision: 0.0871 - val_recall: 0.7286\n",
      "Epoch 37/50\n",
      "679/679 [==============================] - 88s 130ms/step - loss: 1.1896 - acc: 0.7657 - precision: 0.1789 - recall: 0.7080 - val_loss: 0.6672 - val_acc: 0.7745 - val_precision: 0.1382 - val_recall: 0.5885\n",
      "Epoch 38/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "679/679 [==============================] - 89s 130ms/step - loss: 1.1840 - acc: 0.7457 - precision: 0.1670 - recall: 0.7134 - val_loss: 0.6128 - val_acc: 0.7766 - val_precision: 0.1430 - val_recall: 0.6090\n",
      "Epoch 39/50\n",
      "679/679 [==============================] - 88s 130ms/step - loss: 1.2119 - acc: 0.7139 - precision: 0.1529 - recall: 0.7323 - val_loss: 0.6294 - val_acc: 0.7674 - val_precision: 0.1380 - val_recall: 0.6116\n",
      "Epoch 40/50\n",
      "679/679 [==============================] - 88s 130ms/step - loss: 1.1762 - acc: 0.7663 - precision: 0.1775 - recall: 0.6977 - val_loss: 0.5982 - val_acc: 0.7909 - val_precision: 0.1511 - val_recall: 0.6027\n",
      "Epoch 41/50\n",
      "679/679 [==============================] - 88s 130ms/step - loss: 1.2476 - acc: 0.6761 - precision: 0.1382 - recall: 0.7441 - val_loss: 0.5865 - val_acc: 0.7859 - val_precision: 0.1378 - val_recall: 0.5465\n",
      "Epoch 42/50\n",
      "679/679 [==============================] - 88s 130ms/step - loss: 1.1650 - acc: 0.7808 - precision: 0.1881 - recall: 0.6986 - val_loss: 0.7451 - val_acc: 0.6975 - val_precision: 0.1157 - val_recall: 0.6731\n",
      "Epoch 43/50\n",
      "679/679 [==============================] - 88s 130ms/step - loss: 1.2892 - acc: 0.6279 - precision: 0.1235 - recall: 0.7585 - val_loss: 0.7007 - val_acc: 0.6567 - val_precision: 0.1037 - val_recall: 0.6816\n",
      "Epoch 44/50\n",
      "679/679 [==============================] - 89s 130ms/step - loss: 1.1705 - acc: 0.7722 - precision: 0.1816 - recall: 0.6973 - val_loss: 0.5888 - val_acc: 0.7912 - val_precision: 0.1515 - val_recall: 0.6036\n",
      "Epoch 45/50\n",
      "679/679 [==============================] - 88s 130ms/step - loss: 1.3545 - acc: 0.5499 - precision: 0.1056 - recall: 0.7770 - val_loss: 0.7175 - val_acc: 0.6053 - val_precision: 0.0935 - val_recall: 0.7068\n",
      "Epoch 46/50\n",
      "679/679 [==============================] - 89s 130ms/step - loss: 1.1785 - acc: 0.7703 - precision: 0.1822 - recall: 0.7083 - val_loss: 0.6978 - val_acc: 0.7585 - val_precision: 0.1299 - val_recall: 0.5914\n",
      "Epoch 47/50\n",
      "679/679 [==============================] - 89s 130ms/step - loss: 1.1726 - acc: 0.7495 - precision: 0.1701 - recall: 0.7183 - val_loss: 0.5668 - val_acc: 0.7953 - val_precision: 0.1512 - val_recall: 0.5860\n",
      "Epoch 48/50\n",
      "679/679 [==============================] - 89s 130ms/step - loss: 1.1999 - acc: 0.7218 - precision: 0.1569 - recall: 0.7329 - val_loss: 0.7024 - val_acc: 0.7108 - val_precision: 0.1200 - val_recall: 0.6681\n",
      "Epoch 49/50\n",
      "679/679 [==============================] - 89s 130ms/step - loss: 1.1692 - acc: 0.7613 - precision: 0.1752 - recall: 0.7037 - val_loss: 0.5751 - val_acc: 0.8054 - val_precision: 0.1577 - val_recall: 0.5806\n",
      "Epoch 50/50\n",
      "679/679 [==============================] - 89s 130ms/step - loss: 1.2372 - acc: 0.6842 - precision: 0.1413 - recall: 0.7432 - val_loss: 0.6805 - val_acc: 0.7231 - val_precision: 0.1206 - val_recall: 0.6374\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Experiment skipped due to error. Model: {'experiment_num': [290], 'valid': [True], 'num_mels': [80], 'win_len': [10], 'chunk_radius': [11], 'batch_size': [400], 'learning_rate': [1e-05], 'note_class_weight': [{0: 1, 1: 20}], 'model_version': [1], 'spec_conv_layers': [[{'filters': 64, 'kernel_size': (3, 3), 'layers': 2, 'max_pool': True, 'activation': 'relu'}, {'filters': 128, 'kernel_size': (3, 3), 'layers': 2, 'max_pool': True, 'activation': 'relu'}, {'filters': 256, 'kernel_size': (3, 3), 'layers': 2, 'max_pool': True, 'activation': 'relu'}, {'filters': 512, 'kernel_size': (3, 3), 'layers': 2, 'max_pool': True, 'activation': 'relu'}]], 'spec_dense_layers': [[]], 'combined_layers': [[{'units': 23, 'activation': 'relu', 'num_layers': 3}]]}\n",
      "Error: Negative dimension size caused by subtracting 3 from 1 for 'conv2d_326/convolution' (op: 'Conv2D') with input shapes: [?,8,1,256], [3,3,256,512].\n",
      "INFO:root:Indices are: [22400, 44800, 67200, 89200, 111200, 133200, 157200, 174400, 191600, 208800, 226000, 248800, 271600]\n",
      "INFO:root:Indices are: [14400, 28800, 43200, 57600, 77600, 101200, 124800]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "679/679 [==============================] - 19s 27ms/step - loss: 1.0292 - acc: 0.8435 - precision: 0.1409 - recall: 0.2659 - val_loss: 0.3975 - val_acc: 0.8558 - val_precision: 0.1683 - val_recall: 0.4083\n",
      "Epoch 2/50\n",
      "679/679 [==============================] - 18s 27ms/step - loss: 0.9071 - acc: 0.8132 - precision: 0.1933 - recall: 0.5750 - val_loss: 0.4187 - val_acc: 0.8501 - val_precision: 0.1700 - val_recall: 0.4412\n",
      "Epoch 3/50\n",
      "679/679 [==============================] - 18s 27ms/step - loss: 0.9225 - acc: 0.8112 - precision: 0.1867 - recall: 0.5527 - val_loss: 0.4377 - val_acc: 0.8314 - val_precision: 0.1604 - val_recall: 0.4843\n",
      "Epoch 4/50\n",
      "679/679 [==============================] - 18s 27ms/step - loss: 0.8812 - acc: 0.8384 - precision: 0.2209 - recall: 0.5721 - val_loss: 0.4291 - val_acc: 0.8557 - val_precision: 0.1732 - val_recall: 0.4267\n",
      "Epoch 5/50\n",
      "679/679 [==============================] - 18s 26ms/step - loss: 0.9366 - acc: 0.7981 - precision: 0.1727 - recall: 0.5418 - val_loss: 0.4560 - val_acc: 0.8299 - val_precision: 0.1559 - val_recall: 0.4709\n",
      "Epoch 6/50\n",
      "679/679 [==============================] - 18s 27ms/step - loss: 0.8672 - acc: 0.8423 - precision: 0.2297 - recall: 0.5888 - val_loss: 0.5250 - val_acc: 0.8022 - val_precision: 0.1547 - val_recall: 0.5777\n",
      "Epoch 7/50\n",
      "679/679 [==============================] - 18s 26ms/step - loss: 0.9488 - acc: 0.7913 - precision: 0.1635 - recall: 0.5240 - val_loss: 0.4466 - val_acc: 0.8262 - val_precision: 0.1472 - val_recall: 0.4477\n",
      "Epoch 8/50\n",
      "679/679 [==============================] - 18s 27ms/step - loss: 0.8518 - acc: 0.8552 - precision: 0.2468 - recall: 0.5801 - val_loss: 0.5680 - val_acc: 0.7961 - val_precision: 0.1501 - val_recall: 0.5768\n",
      "Epoch 9/50\n",
      "679/679 [==============================] - 18s 26ms/step - loss: 0.9885 - acc: 0.7789 - precision: 0.1380 - recall: 0.4466 - val_loss: 0.5609 - val_acc: 0.7162 - val_precision: 0.1123 - val_recall: 0.5987\n",
      "Epoch 10/50\n",
      "679/679 [==============================] - 18s 26ms/step - loss: 0.8704 - acc: 0.8295 - precision: 0.2146 - recall: 0.5938 - val_loss: 0.4735 - val_acc: 0.8375 - val_precision: 0.1647 - val_recall: 0.4767\n",
      "Epoch 11/50\n",
      "679/679 [==============================] - 18s 26ms/step - loss: 0.8503 - acc: 0.8455 - precision: 0.2347 - recall: 0.5915 - val_loss: 0.4101 - val_acc: 0.8654 - val_precision: 0.1723 - val_recall: 0.3777\n",
      "Epoch 12/50\n",
      "679/679 [==============================] - 18s 27ms/step - loss: 0.8786 - acc: 0.8276 - precision: 0.2084 - recall: 0.5745 - val_loss: 0.4533 - val_acc: 0.8272 - val_precision: 0.1535 - val_recall: 0.4711\n",
      "Epoch 13/50\n",
      "679/679 [==============================] - 18s 26ms/step - loss: 0.8419 - acc: 0.8534 - precision: 0.2450 - recall: 0.5850 - val_loss: 0.3292 - val_acc: 0.9021 - val_precision: 0.1918 - val_recall: 0.2402\n",
      "Epoch 14/50\n",
      "679/679 [==============================] - 18s 27ms/step - loss: 0.9131 - acc: 0.8135 - precision: 0.1878 - recall: 0.5474 - val_loss: 0.3904 - val_acc: 0.8596 - val_precision: 0.1600 - val_recall: 0.3623\n",
      "Epoch 15/50\n",
      "679/679 [==============================] - 18s 27ms/step - loss: 0.8229 - acc: 0.8583 - precision: 0.2549 - recall: 0.5947 - val_loss: 0.4071 - val_acc: 0.8547 - val_precision: 0.1812 - val_recall: 0.4626\n",
      "Epoch 16/50\n",
      "679/679 [==============================] - 18s 27ms/step - loss: 0.9229 - acc: 0.8048 - precision: 0.1770 - recall: 0.5351 - val_loss: 0.4201 - val_acc: 0.8601 - val_precision: 0.1537 - val_recall: 0.3398\n",
      "Epoch 17/50\n",
      "679/679 [==============================] - 18s 27ms/step - loss: 0.8194 - acc: 0.8613 - precision: 0.2587 - recall: 0.5883 - val_loss: 0.4289 - val_acc: 0.8650 - val_precision: 0.1825 - val_recall: 0.4147\n",
      "Epoch 18/50\n",
      "679/679 [==============================] - 18s 27ms/step - loss: 0.9609 - acc: 0.7842 - precision: 0.1506 - recall: 0.4882 - val_loss: 0.3911 - val_acc: 0.8830 - val_precision: 0.1386 - val_recall: 0.2143\n",
      "Epoch 19/50\n",
      "679/679 [==============================] - 18s 27ms/step - loss: 0.8251 - acc: 0.8529 - precision: 0.2468 - recall: 0.5967 - val_loss: 0.3961 - val_acc: 0.8706 - val_precision: 0.1877 - val_recall: 0.4029\n",
      "Epoch 20/50\n",
      "679/679 [==============================] - 18s 27ms/step - loss: 0.8185 - acc: 0.8559 - precision: 0.2509 - recall: 0.5940 - val_loss: 0.3272 - val_acc: 0.8991 - val_precision: 0.1974 - val_recall: 0.2694\n",
      "Epoch 21/50\n",
      "679/679 [==============================] - 18s 27ms/step - loss: 0.8407 - acc: 0.8491 - precision: 0.2363 - recall: 0.5747 - val_loss: 0.2881 - val_acc: 0.9053 - val_precision: 0.1682 - val_recall: 0.1809\n",
      "Epoch 22/50\n",
      "679/679 [==============================] - 18s 26ms/step - loss: 0.8094 - acc: 0.8664 - precision: 0.2666 - recall: 0.5836 - val_loss: 0.3679 - val_acc: 0.8723 - val_precision: 0.1767 - val_recall: 0.3582\n",
      "Epoch 23/50\n",
      "679/679 [==============================] - 18s 27ms/step - loss: 0.8725 - acc: 0.8266 - precision: 0.2066 - recall: 0.5716 - val_loss: 0.4482 - val_acc: 0.8196 - val_precision: 0.1473 - val_recall: 0.4727\n",
      "Epoch 24/50\n",
      "679/679 [==============================] - 18s 26ms/step - loss: 0.7935 - acc: 0.8622 - precision: 0.2644 - recall: 0.6082 - val_loss: 0.4946 - val_acc: 0.8127 - val_precision: 0.1558 - val_recall: 0.5408\n",
      "Epoch 25/50\n",
      "679/679 [==============================] - 18s 27ms/step - loss: 0.8815 - acc: 0.8161 - precision: 0.1941 - recall: 0.5653 - val_loss: 0.3942 - val_acc: 0.8819 - val_precision: 0.1548 - val_recall: 0.2550\n",
      "Epoch 26/50\n",
      "679/679 [==============================] - 18s 27ms/step - loss: 0.7804 - acc: 0.8644 - precision: 0.2668 - recall: 0.6011 - val_loss: 0.3568 - val_acc: 0.8826 - val_precision: 0.1877 - val_recall: 0.3377\n",
      "Epoch 27/50\n",
      "679/679 [==============================] - 18s 27ms/step - loss: 0.9434 - acc: 0.7895 - precision: 0.1580 - recall: 0.5048 - val_loss: 0.5398 - val_acc: 0.7220 - val_precision: 0.1040 - val_recall: 0.5294\n",
      "Epoch 28/50\n",
      "679/679 [==============================] - 18s 26ms/step - loss: 0.7901 - acc: 0.8550 - precision: 0.2520 - recall: 0.6068 - val_loss: 0.3885 - val_acc: 0.8744 - val_precision: 0.1895 - val_recall: 0.3887\n",
      "Epoch 29/50\n",
      "679/679 [==============================] - 18s 27ms/step - loss: 0.7811 - acc: 0.8586 - precision: 0.2583 - recall: 0.6083 - val_loss: 0.3512 - val_acc: 0.8738 - val_precision: 0.1734 - val_recall: 0.3409\n",
      "Epoch 30/50\n",
      "679/679 [==============================] - 18s 26ms/step - loss: 0.8132 - acc: 0.8485 - precision: 0.2388 - recall: 0.5905 - val_loss: 0.4126 - val_acc: 0.8471 - val_precision: 0.1562 - val_recall: 0.4013\n",
      "Epoch 31/50\n",
      "679/679 [==============================] - 19s 27ms/step - loss: 0.7711 - acc: 0.8672 - precision: 0.2734 - recall: 0.6088 - val_loss: 0.3818 - val_acc: 0.8770 - val_precision: 0.1710 - val_recall: 0.3187\n",
      "Epoch 32/50\n",
      "679/679 [==============================] - 18s 26ms/step - loss: 0.8389 - acc: 0.8314 - precision: 0.2156 - recall: 0.5876 - val_loss: 0.5166 - val_acc: 0.7898 - val_precision: 0.1234 - val_recall: 0.4592\n",
      "Epoch 33/50\n",
      "679/679 [==============================] - 18s 27ms/step - loss: 0.7451 - acc: 0.8616 - precision: 0.2681 - recall: 0.6320 - val_loss: 0.3529 - val_acc: 0.8736 - val_precision: 0.1877 - val_recall: 0.3868\n",
      "Epoch 34/50\n",
      "679/679 [==============================] - 18s 26ms/step - loss: 0.8583 - acc: 0.8135 - precision: 0.1956 - recall: 0.5853 - val_loss: 0.4466 - val_acc: 0.8117 - val_precision: 0.1282 - val_recall: 0.4153\n",
      "Epoch 35/50\n",
      "679/679 [==============================] - 18s 27ms/step - loss: 0.7379 - acc: 0.8611 - precision: 0.2685 - recall: 0.6390 - val_loss: 0.3701 - val_acc: 0.8717 - val_precision: 0.1711 - val_recall: 0.3434\n",
      "Epoch 36/50\n",
      "679/679 [==============================] - 18s 27ms/step - loss: 0.9253 - acc: 0.7860 - precision: 0.1631 - recall: 0.5416 - val_loss: 0.4461 - val_acc: 0.8091 - val_precision: 0.1254 - val_recall: 0.4108\n",
      "Epoch 37/50\n",
      "679/679 [==============================] - 18s 26ms/step - loss: 0.7407 - acc: 0.8604 - precision: 0.2672 - recall: 0.6379 - val_loss: 0.2943 - val_acc: 0.8902 - val_precision: 0.1904 - val_recall: 0.3035\n",
      "Epoch 38/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "679/679 [==============================] - 18s 27ms/step - loss: 0.7449 - acc: 0.8600 - precision: 0.2668 - recall: 0.6389 - val_loss: 0.3606 - val_acc: 0.8624 - val_precision: 0.1649 - val_recall: 0.3664\n",
      "Epoch 39/50\n",
      "679/679 [==============================] - 18s 26ms/step - loss: 0.7784 - acc: 0.8471 - precision: 0.2433 - recall: 0.6221 - val_loss: 0.5127 - val_acc: 0.7903 - val_precision: 0.1323 - val_recall: 0.5029\n",
      "Epoch 40/50\n",
      "679/679 [==============================] - 18s 27ms/step - loss: 0.7252 - acc: 0.8596 - precision: 0.2686 - recall: 0.6518 - val_loss: 0.3444 - val_acc: 0.8824 - val_precision: 0.1765 - val_recall: 0.3074\n",
      "Epoch 41/50\n",
      "679/679 [==============================] - 18s 26ms/step - loss: 0.8067 - acc: 0.8226 - precision: 0.2141 - recall: 0.6305 - val_loss: 0.4383 - val_acc: 0.8090 - val_precision: 0.1282 - val_recall: 0.4237\n",
      "Epoch 42/50\n",
      "679/679 [==============================] - 18s 26ms/step - loss: 0.7049 - acc: 0.8528 - precision: 0.2615 - recall: 0.6719 - val_loss: 0.4471 - val_acc: 0.8231 - val_precision: 0.1570 - val_recall: 0.5034\n",
      "Epoch 43/50\n",
      "679/679 [==============================] - 18s 26ms/step - loss: 0.8239 - acc: 0.8138 - precision: 0.2028 - recall: 0.6201 - val_loss: 0.4953 - val_acc: 0.7518 - val_precision: 0.1088 - val_recall: 0.4851\n",
      "Epoch 44/50\n",
      "679/679 [==============================] - 18s 26ms/step - loss: 0.6985 - acc: 0.8541 - precision: 0.2655 - recall: 0.6828 - val_loss: 0.4043 - val_acc: 0.8399 - val_precision: 0.1500 - val_recall: 0.4068\n",
      "Epoch 45/50\n",
      "679/679 [==============================] - 18s 26ms/step - loss: 0.9012 - acc: 0.7785 - precision: 0.1644 - recall: 0.5758 - val_loss: 0.3975 - val_acc: 0.8123 - val_precision: 0.0989 - val_recall: 0.2954\n",
      "Epoch 46/50\n",
      "679/679 [==============================] - 19s 28ms/step - loss: 0.7116 - acc: 0.8508 - precision: 0.2581 - recall: 0.6705 - val_loss: 0.3755 - val_acc: 0.8697 - val_precision: 0.1758 - val_recall: 0.3680\n",
      "Epoch 47/50\n",
      "679/679 [==============================] - 18s 27ms/step - loss: 0.7034 - acc: 0.8512 - precision: 0.2598 - recall: 0.6762 - val_loss: 0.3451 - val_acc: 0.8770 - val_precision: 0.1740 - val_recall: 0.3270\n",
      "Epoch 48/50\n",
      "679/679 [==============================] - 18s 27ms/step - loss: 0.7325 - acc: 0.8407 - precision: 0.2435 - recall: 0.6688 - val_loss: 0.3481 - val_acc: 0.8667 - val_precision: 0.1423 - val_recall: 0.2809\n",
      "Epoch 49/50\n",
      "679/679 [==============================] - 18s 26ms/step - loss: 0.6926 - acc: 0.8517 - precision: 0.2637 - recall: 0.6934 - val_loss: 0.3956 - val_acc: 0.8491 - val_precision: 0.1585 - val_recall: 0.4016\n",
      "Epoch 50/50\n",
      "679/679 [==============================] - 18s 27ms/step - loss: 0.7684 - acc: 0.8249 - precision: 0.2230 - recall: 0.6634 - val_loss: 0.4898 - val_acc: 0.7822 - val_precision: 0.1137 - val_recall: 0.4329\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Indices are: [22400, 44800, 67200, 89200, 111200, 133200, 157200, 174400, 191600, 208800, 226000, 248800, 271600]\n",
      "INFO:root:Indices are: [14400, 28800, 43200, 57600, 77600, 101200, 124800]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "679/679 [==============================] - 89s 132ms/step - loss: 1.0808 - acc: 0.9305 - precision: 0.0575 - recall: 0.0033 - val_loss: 0.5676 - val_acc: 0.9447 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 2/50\n",
      "679/679 [==============================] - 89s 130ms/step - loss: 0.9753 - acc: 0.8628 - precision: 0.2017 - recall: 0.3636 - val_loss: 0.3688 - val_acc: 0.8584 - val_precision: 0.1787 - val_recall: 0.4342\n",
      "Epoch 3/50\n",
      "679/679 [==============================] - 89s 130ms/step - loss: 0.9172 - acc: 0.8111 - precision: 0.1896 - recall: 0.5671 - val_loss: 0.4942 - val_acc: 0.8085 - val_precision: 0.1499 - val_recall: 0.5276\n",
      "Epoch 4/50\n",
      "679/679 [==============================] - 89s 131ms/step - loss: 0.8550 - acc: 0.8504 - precision: 0.2399 - recall: 0.5823 - val_loss: 0.3874 - val_acc: 0.8679 - val_precision: 0.1864 - val_recall: 0.4131\n",
      "Epoch 5/50\n",
      "679/679 [==============================] - 89s 130ms/step - loss: 0.9104 - acc: 0.8090 - precision: 0.1894 - recall: 0.5760 - val_loss: 0.4256 - val_acc: 0.8448 - val_precision: 0.1650 - val_recall: 0.4450\n",
      "Epoch 6/50\n",
      "679/679 [==============================] - 89s 131ms/step - loss: 0.8394 - acc: 0.8581 - precision: 0.2551 - recall: 0.5976 - val_loss: 0.3532 - val_acc: 0.8866 - val_precision: 0.2097 - val_recall: 0.3797\n",
      "Epoch 7/50\n",
      "679/679 [==============================] - 89s 130ms/step - loss: 0.9239 - acc: 0.7996 - precision: 0.1764 - recall: 0.5536 - val_loss: 0.5106 - val_acc: 0.7871 - val_precision: 0.1349 - val_recall: 0.5266\n",
      "Epoch 8/50\n",
      "679/679 [==============================] - 89s 130ms/step - loss: 0.8355 - acc: 0.8568 - precision: 0.2529 - recall: 0.5967 - val_loss: 0.4709 - val_acc: 0.8366 - val_precision: 0.1683 - val_recall: 0.4964\n",
      "Epoch 9/50\n",
      "679/679 [==============================] - 89s 130ms/step - loss: 0.9705 - acc: 0.7824 - precision: 0.1516 - recall: 0.4985 - val_loss: 0.4410 - val_acc: 0.8369 - val_precision: 0.1276 - val_recall: 0.3341\n",
      "Epoch 10/50\n",
      "679/679 [==============================] - 88s 130ms/step - loss: 0.8359 - acc: 0.8532 - precision: 0.2490 - recall: 0.6053 - val_loss: 0.4761 - val_acc: 0.8255 - val_precision: 0.1570 - val_recall: 0.4938\n",
      "Epoch 11/50\n",
      "679/679 [==============================] - 88s 130ms/step - loss: 0.8215 - acc: 0.8561 - precision: 0.2522 - recall: 0.5991 - val_loss: 0.2871 - val_acc: 0.8972 - val_precision: 0.2073 - val_recall: 0.3043\n",
      "Epoch 12/50\n",
      "679/679 [==============================] - 88s 130ms/step - loss: 0.8376 - acc: 0.8471 - precision: 0.2373 - recall: 0.5929 - val_loss: 0.3391 - val_acc: 0.8854 - val_precision: 0.1857 - val_recall: 0.3175\n",
      "Epoch 13/50\n",
      "679/679 [==============================] - 89s 130ms/step - loss: 0.8134 - acc: 0.8659 - precision: 0.2678 - recall: 0.5932 - val_loss: 0.4263 - val_acc: 0.8549 - val_precision: 0.1832 - val_recall: 0.4701\n",
      "Epoch 14/50\n",
      "679/679 [==============================] - 89s 130ms/step - loss: 0.8598 - acc: 0.8332 - precision: 0.2188 - recall: 0.5927 - val_loss: 0.5551 - val_acc: 0.7623 - val_precision: 0.1310 - val_recall: 0.5859\n",
      "Epoch 15/50\n",
      "679/679 [==============================] - 89s 131ms/step - loss: 0.7987 - acc: 0.8594 - precision: 0.2603 - recall: 0.6119 - val_loss: 0.4338 - val_acc: 0.8555 - val_precision: 0.1652 - val_recall: 0.3980\n",
      "Epoch 16/50\n",
      "679/679 [==============================] - 89s 130ms/step - loss: 0.8886 - acc: 0.8157 - precision: 0.1953 - recall: 0.5733 - val_loss: 0.4456 - val_acc: 0.8282 - val_precision: 0.1543 - val_recall: 0.4704\n",
      "Epoch 17/50\n",
      "679/679 [==============================] - 89s 131ms/step - loss: 0.8021 - acc: 0.8674 - precision: 0.2719 - recall: 0.5996 - val_loss: 0.4593 - val_acc: 0.8528 - val_precision: 0.1790 - val_recall: 0.4638\n",
      "Epoch 18/50\n",
      "679/679 [==============================] - 88s 130ms/step - loss: 0.9489 - acc: 0.7886 - precision: 0.1617 - recall: 0.5253 - val_loss: 0.4870 - val_acc: 0.8078 - val_precision: 0.1305 - val_recall: 0.4377\n",
      "Epoch 19/50\n",
      "679/679 [==============================] - 89s 131ms/step - loss: 0.8118 - acc: 0.8560 - precision: 0.2548 - recall: 0.6122 - val_loss: 0.3830 - val_acc: 0.8705 - val_precision: 0.1947 - val_recall: 0.4280\n",
      "Epoch 20/50\n",
      "679/679 [==============================] - 89s 131ms/step - loss: 0.7852 - acc: 0.8653 - precision: 0.2696 - recall: 0.6068 - val_loss: 0.2871 - val_acc: 0.9023 - val_precision: 0.2057 - val_recall: 0.2682\n",
      "Epoch 21/50\n",
      "679/679 [==============================] - 89s 131ms/step - loss: 0.8079 - acc: 0.8565 - precision: 0.2531 - recall: 0.5995 - val_loss: 0.3257 - val_acc: 0.8898 - val_precision: 0.1787 - val_recall: 0.2762\n",
      "Epoch 22/50\n",
      "679/679 [==============================] - 89s 131ms/step - loss: 0.7776 - acc: 0.8735 - precision: 0.2853 - recall: 0.6062 - val_loss: 0.4186 - val_acc: 0.8652 - val_precision: 0.1849 - val_recall: 0.4219\n",
      "Epoch 23/50\n",
      "679/679 [==============================] - 89s 130ms/step - loss: 0.8289 - acc: 0.8424 - precision: 0.2323 - recall: 0.6005 - val_loss: 0.4669 - val_acc: 0.8263 - val_precision: 0.1466 - val_recall: 0.4442\n",
      "Epoch 24/50\n",
      "679/679 [==============================] - 89s 131ms/step - loss: 0.7678 - acc: 0.8677 - precision: 0.2763 - recall: 0.6173 - val_loss: 0.4228 - val_acc: 0.8543 - val_precision: 0.1843 - val_recall: 0.4778\n",
      "Epoch 25/50\n",
      "679/679 [==============================] - 89s 131ms/step - loss: 0.8616 - acc: 0.8295 - precision: 0.2103 - recall: 0.5732 - val_loss: 0.4266 - val_acc: 0.8264 - val_precision: 0.1481 - val_recall: 0.4504\n",
      "Epoch 26/50\n",
      "679/679 [==============================] - 89s 131ms/step - loss: 0.7681 - acc: 0.8682 - precision: 0.2788 - recall: 0.6246 - val_loss: 0.3231 - val_acc: 0.8863 - val_precision: 0.2138 - val_recall: 0.3948\n",
      "Epoch 27/50\n",
      "679/679 [==============================] - 89s 130ms/step - loss: 0.9250 - acc: 0.7966 - precision: 0.1685 - recall: 0.5279 - val_loss: 0.5630 - val_acc: 0.7249 - val_precision: 0.1061 - val_recall: 0.5354\n",
      "Epoch 28/50\n",
      "679/679 [==============================] - 89s 131ms/step - loss: 0.7691 - acc: 0.8649 - precision: 0.2722 - recall: 0.6226 - val_loss: 0.3527 - val_acc: 0.8884 - val_precision: 0.2125 - val_recall: 0.3765\n",
      "Epoch 29/50\n",
      "679/679 [==============================] - 88s 130ms/step - loss: 0.7535 - acc: 0.8655 - precision: 0.2745 - recall: 0.6299 - val_loss: 0.3955 - val_acc: 0.8591 - val_precision: 0.1810 - val_recall: 0.4395\n",
      "Epoch 30/50\n",
      "679/679 [==============================] - 89s 131ms/step - loss: 0.7780 - acc: 0.8581 - precision: 0.2595 - recall: 0.6183 - val_loss: 0.3551 - val_acc: 0.8744 - val_precision: 0.1789 - val_recall: 0.3547\n",
      "Epoch 31/50\n",
      "679/679 [==============================] - 88s 130ms/step - loss: 0.7359 - acc: 0.8711 - precision: 0.2862 - recall: 0.6356 - val_loss: 0.4695 - val_acc: 0.8350 - val_precision: 0.1643 - val_recall: 0.4857\n",
      "Epoch 32/50\n",
      "679/679 [==============================] - 88s 130ms/step - loss: 0.8034 - acc: 0.8454 - precision: 0.2389 - recall: 0.6119 - val_loss: 0.3537 - val_acc: 0.8632 - val_precision: 0.1672 - val_recall: 0.3707\n",
      "Epoch 33/50\n",
      "679/679 [==============================] - 88s 130ms/step - loss: 0.7279 - acc: 0.8628 - precision: 0.2737 - recall: 0.6502 - val_loss: 0.4202 - val_acc: 0.8529 - val_precision: 0.1756 - val_recall: 0.4497\n",
      "Epoch 34/50\n",
      "679/679 [==============================] - 89s 130ms/step - loss: 0.8342 - acc: 0.8313 - precision: 0.2159 - recall: 0.5900 - val_loss: 0.3851 - val_acc: 0.8448 - val_precision: 0.1576 - val_recall: 0.4159\n",
      "Epoch 35/50\n",
      "679/679 [==============================] - 89s 131ms/step - loss: 0.7328 - acc: 0.8642 - precision: 0.2768 - recall: 0.6527 - val_loss: 0.3834 - val_acc: 0.8655 - val_precision: 0.1865 - val_recall: 0.4263\n",
      "Epoch 36/50\n",
      "679/679 [==============================] - 88s 130ms/step - loss: 0.9023 - acc: 0.7981 - precision: 0.1741 - recall: 0.5489 - val_loss: 0.4513 - val_acc: 0.7876 - val_precision: 0.1159 - val_recall: 0.4289\n",
      "Epoch 37/50\n",
      "679/679 [==============================] - 89s 131ms/step - loss: 0.7391 - acc: 0.8611 - precision: 0.2695 - recall: 0.6432 - val_loss: 0.3872 - val_acc: 0.8837 - val_precision: 0.1716 - val_recall: 0.2885\n",
      "Epoch 38/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "679/679 [==============================] - 88s 130ms/step - loss: 0.7189 - acc: 0.8593 - precision: 0.2689 - recall: 0.6562 - val_loss: 0.3672 - val_acc: 0.8768 - val_precision: 0.1791 - val_recall: 0.3433\n",
      "Epoch 39/50\n",
      "679/679 [==============================] - 88s 130ms/step - loss: 0.7474 - acc: 0.8540 - precision: 0.2574 - recall: 0.6408 - val_loss: 0.3668 - val_acc: 0.8596 - val_precision: 0.1733 - val_recall: 0.4089\n",
      "Epoch 40/50\n",
      "679/679 [==============================] - 89s 130ms/step - loss: 0.7085 - acc: 0.8651 - precision: 0.2810 - recall: 0.6669 - val_loss: 0.3820 - val_acc: 0.8748 - val_precision: 0.1863 - val_recall: 0.3758\n",
      "Epoch 41/50\n",
      "679/679 [==============================] - 89s 131ms/step - loss: 0.7759 - acc: 0.8420 - precision: 0.2383 - recall: 0.6321 - val_loss: 0.4207 - val_acc: 0.8385 - val_precision: 0.1533 - val_recall: 0.4251\n",
      "Epoch 42/50\n",
      "679/679 [==============================] - 89s 131ms/step - loss: 0.7026 - acc: 0.8621 - precision: 0.2768 - recall: 0.6724 - val_loss: 0.3702 - val_acc: 0.8658 - val_precision: 0.1890 - val_recall: 0.4338\n",
      "Epoch 43/50\n",
      "679/679 [==============================] - 89s 130ms/step - loss: 0.8112 - acc: 0.8279 - precision: 0.2174 - recall: 0.6160 - val_loss: 0.3926 - val_acc: 0.8398 - val_precision: 0.1518 - val_recall: 0.4141\n",
      "Epoch 44/50\n",
      "679/679 [==============================] - 89s 130ms/step - loss: 0.7009 - acc: 0.8582 - precision: 0.2715 - recall: 0.6796 - val_loss: 0.3988 - val_acc: 0.8461 - val_precision: 0.1512 - val_recall: 0.3871\n",
      "Epoch 45/50\n",
      "679/679 [==============================] - 89s 130ms/step - loss: 0.8883 - acc: 0.7916 - precision: 0.1734 - recall: 0.5710 - val_loss: 0.4204 - val_acc: 0.8179 - val_precision: 0.1292 - val_recall: 0.4001\n",
      "Epoch 46/50\n",
      "679/679 [==============================] - 89s 131ms/step - loss: 0.7009 - acc: 0.8625 - precision: 0.2764 - recall: 0.6663 - val_loss: 0.3480 - val_acc: 0.8792 - val_precision: 0.1793 - val_recall: 0.3316\n",
      "Epoch 47/50\n",
      "679/679 [==============================] - 89s 131ms/step - loss: 0.6982 - acc: 0.8580 - precision: 0.2709 - recall: 0.6783 - val_loss: 0.3413 - val_acc: 0.8785 - val_precision: 0.1880 - val_recall: 0.3610\n",
      "Epoch 48/50\n",
      "679/679 [==============================] - 89s 131ms/step - loss: 0.7266 - acc: 0.8492 - precision: 0.2535 - recall: 0.6587 - val_loss: 0.3637 - val_acc: 0.8699 - val_precision: 0.1709 - val_recall: 0.3516\n",
      "Epoch 49/50\n",
      "679/679 [==============================] - 89s 131ms/step - loss: 0.6764 - acc: 0.8633 - precision: 0.2826 - recall: 0.6937 - val_loss: 0.3980 - val_acc: 0.8628 - val_precision: 0.1466 - val_recall: 0.3073\n",
      "Epoch 50/50\n",
      "679/679 [==============================] - 89s 131ms/step - loss: 0.7604 - acc: 0.8351 - precision: 0.2317 - recall: 0.6451 - val_loss: 0.3382 - val_acc: 0.8759 - val_precision: 0.1673 - val_recall: 0.3133\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Experiment skipped due to error. Model: {'experiment_num': [293], 'valid': [True], 'num_mels': [80], 'win_len': [10], 'chunk_radius': [11], 'batch_size': [400], 'learning_rate': [0.0001], 'note_class_weight': [{0: 1, 1: 10}], 'model_version': [1], 'spec_conv_layers': [[{'filters': 64, 'kernel_size': (3, 3), 'layers': 2, 'max_pool': True, 'activation': 'relu'}, {'filters': 128, 'kernel_size': (3, 3), 'layers': 2, 'max_pool': True, 'activation': 'relu'}, {'filters': 256, 'kernel_size': (3, 3), 'layers': 2, 'max_pool': True, 'activation': 'relu'}, {'filters': 512, 'kernel_size': (3, 3), 'layers': 2, 'max_pool': True, 'activation': 'relu'}]], 'spec_dense_layers': [[]], 'combined_layers': [[{'units': 23, 'activation': 'relu', 'num_layers': 3}]]}\n",
      "Error: Negative dimension size caused by subtracting 3 from 1 for 'conv2d_347/convolution' (op: 'Conv2D') with input shapes: [?,8,1,256], [3,3,256,512].\n",
      "INFO:root:Indices are: [22400, 44800, 67200, 89200, 111200, 133200, 157200, 174400, 191600, 208800, 226000, 248800, 271600]\n",
      "INFO:root:Indices are: [14400, 28800, 43200, 57600, 77600, 101200, 124800]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "679/679 [==============================] - 19s 28ms/step - loss: 1.2373 - acc: 0.5161 - precision: 0.0947 - recall: 0.7343 - val_loss: 0.6519 - val_acc: 0.7469 - val_precision: 0.1301 - val_recall: 0.6295\n",
      "Epoch 2/50\n",
      "679/679 [==============================] - 18s 27ms/step - loss: 1.1085 - acc: 0.7683 - precision: 0.1697 - recall: 0.6436 - val_loss: 0.9403 - val_acc: 0.5249 - val_precision: 0.0857 - val_recall: 0.7858\n",
      "Epoch 3/50\n",
      "679/679 [==============================] - 18s 27ms/step - loss: 1.1371 - acc: 0.7495 - precision: 0.1567 - recall: 0.6365 - val_loss: 0.5155 - val_acc: 0.8386 - val_precision: 0.1627 - val_recall: 0.4629\n",
      "Epoch 4/50\n",
      "679/679 [==============================] - 18s 27ms/step - loss: 1.0866 - acc: 0.8014 - precision: 0.1912 - recall: 0.6207 - val_loss: 0.4598 - val_acc: 0.8599 - val_precision: 0.1787 - val_recall: 0.4267\n",
      "Epoch 5/50\n",
      "679/679 [==============================] - 18s 27ms/step - loss: 1.1487 - acc: 0.7215 - precision: 0.1445 - recall: 0.6531 - val_loss: 0.4823 - val_acc: 0.8599 - val_precision: 0.1594 - val_recall: 0.3591\n",
      "Epoch 6/50\n",
      "679/679 [==============================] - 18s 27ms/step - loss: 1.0723 - acc: 0.7919 - precision: 0.1899 - recall: 0.6580 - val_loss: 0.4532 - val_acc: 0.8468 - val_precision: 0.1786 - val_recall: 0.4923\n",
      "Epoch 7/50\n",
      "679/679 [==============================] - 18s 27ms/step - loss: 1.1805 - acc: 0.6843 - precision: 0.1293 - recall: 0.6584 - val_loss: 0.5981 - val_acc: 0.7355 - val_precision: 0.1202 - val_recall: 0.5992\n",
      "Epoch 8/50\n",
      "679/679 [==============================] - 18s 27ms/step - loss: 1.0539 - acc: 0.8039 - precision: 0.1977 - recall: 0.6429 - val_loss: 0.4679 - val_acc: 0.8453 - val_precision: 0.1761 - val_recall: 0.4891\n",
      "Epoch 9/50\n",
      "679/679 [==============================] - 18s 27ms/step - loss: 1.2189 - acc: 0.6261 - precision: 0.1101 - recall: 0.6570 - val_loss: 0.5046 - val_acc: 0.8057 - val_precision: 0.1209 - val_recall: 0.4011\n",
      "Epoch 10/50\n",
      "679/679 [==============================] - 18s 27ms/step - loss: 1.0713 - acc: 0.7774 - precision: 0.1805 - recall: 0.6682 - val_loss: 0.4501 - val_acc: 0.8633 - val_precision: 0.1827 - val_recall: 0.4238\n",
      "Epoch 11/50\n",
      "679/679 [==============================] - 19s 28ms/step - loss: 1.0577 - acc: 0.8007 - precision: 0.1964 - recall: 0.6517 - val_loss: 0.4999 - val_acc: 0.8435 - val_precision: 0.1653 - val_recall: 0.4523\n",
      "Epoch 12/50\n",
      "679/679 [==============================] - 19s 28ms/step - loss: 1.0812 - acc: 0.7697 - precision: 0.1736 - recall: 0.6610 - val_loss: 0.6182 - val_acc: 0.7861 - val_precision: 0.1396 - val_recall: 0.5558\n",
      "Epoch 13/50\n",
      "679/679 [==============================] - 19s 27ms/step - loss: 1.0433 - acc: 0.8151 - precision: 0.2068 - recall: 0.6336 - val_loss: 0.5360 - val_acc: 0.8296 - val_precision: 0.1630 - val_recall: 0.5037\n",
      "Epoch 14/50\n",
      "679/679 [==============================] - 18s 27ms/step - loss: 1.1176 - acc: 0.7375 - precision: 0.1533 - recall: 0.6566 - val_loss: 0.5885 - val_acc: 0.7736 - val_precision: 0.1283 - val_recall: 0.5342\n",
      "Epoch 15/50\n",
      "679/679 [==============================] - 18s 27ms/step - loss: 1.0170 - acc: 0.8147 - precision: 0.2120 - recall: 0.6638 - val_loss: 0.5351 - val_acc: 0.8047 - val_precision: 0.1577 - val_recall: 0.5835\n",
      "Epoch 16/50\n",
      "679/679 [==============================] - 19s 28ms/step - loss: 1.1398 - acc: 0.7025 - precision: 0.1380 - recall: 0.6669 - val_loss: 0.5438 - val_acc: 0.7679 - val_precision: 0.1245 - val_recall: 0.5302\n",
      "Epoch 17/50\n",
      "679/679 [==============================] - 19s 28ms/step - loss: 1.0226 - acc: 0.8160 - precision: 0.2113 - recall: 0.6527 - val_loss: 0.4589 - val_acc: 0.8580 - val_precision: 0.1774 - val_recall: 0.4316\n",
      "Epoch 18/50\n",
      "679/679 [==============================] - 19s 27ms/step - loss: 1.1926 - acc: 0.6562 - precision: 0.1189 - recall: 0.6550 - val_loss: 0.5630 - val_acc: 0.7133 - val_precision: 0.0991 - val_recall: 0.5178\n",
      "Epoch 19/50\n",
      "679/679 [==============================] - 19s 27ms/step - loss: 1.0212 - acc: 0.7966 - precision: 0.1971 - recall: 0.6758 - val_loss: 0.5038 - val_acc: 0.8460 - val_precision: 0.1687 - val_recall: 0.4545\n",
      "Epoch 20/50\n",
      "679/679 [==============================] - 19s 27ms/step - loss: 1.0040 - acc: 0.8097 - precision: 0.2079 - recall: 0.6685 - val_loss: 0.6473 - val_acc: 0.7563 - val_precision: 0.1320 - val_recall: 0.6118\n",
      "Epoch 21/50\n",
      "679/679 [==============================] - 19s 28ms/step - loss: 1.0439 - acc: 0.7751 - precision: 0.1786 - recall: 0.6674 - val_loss: 0.4572 - val_acc: 0.8376 - val_precision: 0.1422 - val_recall: 0.3849\n",
      "Epoch 22/50\n",
      "679/679 [==============================] - 18s 27ms/step - loss: 0.9974 - acc: 0.8119 - precision: 0.2092 - recall: 0.6640 - val_loss: 0.5393 - val_acc: 0.8283 - val_precision: 0.1599 - val_recall: 0.4952\n",
      "Epoch 23/50\n",
      "679/679 [==============================] - 18s 27ms/step - loss: 1.0824 - acc: 0.7356 - precision: 0.1566 - recall: 0.6839 - val_loss: 0.5552 - val_acc: 0.7698 - val_precision: 0.1267 - val_recall: 0.5370\n",
      "Epoch 24/50\n",
      "679/679 [==============================] - 18s 27ms/step - loss: 0.9769 - acc: 0.8132 - precision: 0.2132 - recall: 0.6788 - val_loss: 0.3773 - val_acc: 0.8706 - val_precision: 0.1853 - val_recall: 0.3950\n",
      "Epoch 25/50\n",
      "679/679 [==============================] - 18s 27ms/step - loss: 1.1024 - acc: 0.7268 - precision: 0.1507 - recall: 0.6755 - val_loss: 0.4818 - val_acc: 0.8145 - val_precision: 0.1238 - val_recall: 0.3876\n",
      "Epoch 26/50\n",
      "679/679 [==============================] - 18s 27ms/step - loss: 0.9758 - acc: 0.7982 - precision: 0.2012 - recall: 0.6910 - val_loss: 0.5029 - val_acc: 0.8122 - val_precision: 0.1528 - val_recall: 0.5277\n",
      "Epoch 27/50\n",
      "679/679 [==============================] - 19s 27ms/step - loss: 1.1647 - acc: 0.6420 - precision: 0.1206 - recall: 0.7010 - val_loss: 0.6062 - val_acc: 0.6694 - val_precision: 0.0889 - val_recall: 0.5389\n",
      "Epoch 28/50\n",
      "679/679 [==============================] - 18s 27ms/step - loss: 0.9689 - acc: 0.7905 - precision: 0.1974 - recall: 0.7065 - val_loss: 0.5179 - val_acc: 0.8167 - val_precision: 0.1561 - val_recall: 0.5254\n",
      "Epoch 29/50\n",
      "679/679 [==============================] - 18s 27ms/step - loss: 0.9620 - acc: 0.7922 - precision: 0.1985 - recall: 0.7057 - val_loss: 0.4024 - val_acc: 0.8506 - val_precision: 0.1638 - val_recall: 0.4147\n",
      "Epoch 30/50\n",
      "679/679 [==============================] - 19s 27ms/step - loss: 0.9959 - acc: 0.7679 - precision: 0.1797 - recall: 0.7044 - val_loss: 0.5950 - val_acc: 0.7528 - val_precision: 0.1196 - val_recall: 0.5459\n",
      "Epoch 31/50\n",
      "679/679 [==============================] - 19s 28ms/step - loss: 0.9486 - acc: 0.7910 - precision: 0.1994 - recall: 0.7167 - val_loss: 0.5028 - val_acc: 0.7870 - val_precision: 0.1308 - val_recall: 0.5058\n",
      "Epoch 32/50\n",
      "679/679 [==============================] - 19s 28ms/step - loss: 1.0335 - acc: 0.7376 - precision: 0.1622 - recall: 0.7128 - val_loss: 0.5038 - val_acc: 0.7987 - val_precision: 0.1223 - val_recall: 0.4280\n",
      "Epoch 33/50\n",
      "679/679 [==============================] - 18s 27ms/step - loss: 0.9210 - acc: 0.7884 - precision: 0.1999 - recall: 0.7327 - val_loss: 0.5616 - val_acc: 0.7582 - val_precision: 0.1303 - val_recall: 0.5945\n",
      "Epoch 34/50\n",
      "679/679 [==============================] - 18s 27ms/step - loss: 1.0555 - acc: 0.7134 - precision: 0.1508 - recall: 0.7205 - val_loss: 0.5705 - val_acc: 0.7293 - val_precision: 0.1054 - val_recall: 0.5207\n",
      "Epoch 35/50\n",
      "679/679 [==============================] - 18s 27ms/step - loss: 0.9138 - acc: 0.7771 - precision: 0.1941 - recall: 0.7525 - val_loss: 0.4894 - val_acc: 0.8075 - val_precision: 0.1349 - val_recall: 0.4589\n",
      "Epoch 36/50\n",
      "679/679 [==============================] - 18s 27ms/step - loss: 1.1474 - acc: 0.6414 - precision: 0.1226 - recall: 0.7185 - val_loss: 0.6967 - val_acc: 0.5832 - val_precision: 0.0842 - val_recall: 0.6624\n",
      "Epoch 37/50\n",
      "679/679 [==============================] - 18s 27ms/step - loss: 0.9221 - acc: 0.7818 - precision: 0.1962 - recall: 0.7424 - val_loss: 0.4595 - val_acc: 0.8228 - val_precision: 0.1576 - val_recall: 0.5075\n",
      "Epoch 38/50\n",
      "679/679 [==============================] - 18s 27ms/step - loss: 0.9147 - acc: 0.7739 - precision: 0.1921 - recall: 0.7549 - val_loss: 0.4499 - val_acc: 0.8318 - val_precision: 0.1412 - val_recall: 0.4020\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39/50\n",
      "679/679 [==============================] - 18s 27ms/step - loss: 0.9523 - acc: 0.7607 - precision: 0.1818 - recall: 0.7483 - val_loss: 0.4427 - val_acc: 0.8252 - val_precision: 0.1279 - val_recall: 0.3715\n",
      "Epoch 40/50\n",
      "679/679 [==============================] - 18s 27ms/step - loss: 0.8987 - acc: 0.7800 - precision: 0.1975 - recall: 0.7600 - val_loss: 0.3779 - val_acc: 0.8630 - val_precision: 0.1608 - val_recall: 0.3503\n",
      "Epoch 41/50\n",
      "679/679 [==============================] - 18s 27ms/step - loss: 1.0035 - acc: 0.7301 - precision: 0.1629 - recall: 0.7445 - val_loss: 0.4962 - val_acc: 0.7653 - val_precision: 0.0956 - val_recall: 0.3837\n",
      "Epoch 42/50\n",
      "679/679 [==============================] - 18s 27ms/step - loss: 0.8714 - acc: 0.7757 - precision: 0.1973 - recall: 0.7793 - val_loss: 0.5071 - val_acc: 0.7671 - val_precision: 0.1250 - val_recall: 0.5358\n",
      "Epoch 43/50\n",
      "679/679 [==============================] - 18s 27ms/step - loss: 1.0258 - acc: 0.7044 - precision: 0.1516 - recall: 0.7554 - val_loss: 0.4669 - val_acc: 0.7991 - val_precision: 0.0976 - val_recall: 0.3197\n",
      "Epoch 44/50\n",
      "679/679 [==============================] - 19s 28ms/step - loss: 0.8664 - acc: 0.7764 - precision: 0.1982 - recall: 0.7825 - val_loss: 0.4579 - val_acc: 0.8079 - val_precision: 0.1328 - val_recall: 0.4478\n",
      "Epoch 45/50\n",
      "679/679 [==============================] - 19s 28ms/step - loss: 1.1184 - acc: 0.6481 - precision: 0.1261 - recall: 0.7279 - val_loss: 0.7011 - val_acc: 0.5779 - val_precision: 0.0826 - val_recall: 0.6564\n",
      "Epoch 46/50\n",
      "679/679 [==============================] - 19s 28ms/step - loss: 0.8721 - acc: 0.7726 - precision: 0.1956 - recall: 0.7832 - val_loss: 0.3091 - val_acc: 0.8793 - val_precision: 0.1786 - val_recall: 0.3289\n",
      "Epoch 47/50\n",
      "679/679 [==============================] - 18s 27ms/step - loss: 0.8816 - acc: 0.7649 - precision: 0.1897 - recall: 0.7812 - val_loss: 0.3068 - val_acc: 0.8830 - val_precision: 0.1704 - val_recall: 0.2887\n",
      "Epoch 48/50\n",
      "679/679 [==============================] - 19s 27ms/step - loss: 0.9127 - acc: 0.7514 - precision: 0.1809 - recall: 0.7824 - val_loss: 0.4479 - val_acc: 0.8035 - val_precision: 0.1118 - val_recall: 0.3680\n",
      "Epoch 49/50\n",
      "679/679 [==============================] - 18s 27ms/step - loss: 0.8514 - acc: 0.7734 - precision: 0.1991 - recall: 0.8030 - val_loss: 0.4326 - val_acc: 0.8077 - val_precision: 0.1251 - val_recall: 0.4140\n",
      "Epoch 50/50\n",
      "679/679 [==============================] - 19s 27ms/step - loss: 0.9615 - acc: 0.7353 - precision: 0.1688 - recall: 0.7657 - val_loss: 0.5918 - val_acc: 0.7149 - val_precision: 0.0978 - val_recall: 0.5058\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Indices are: [22400, 44800, 67200, 89200, 111200, 133200, 157200, 174400, 191600, 208800, 226000, 248800, 271600]\n",
      "INFO:root:Indices are: [14400, 28800, 43200, 57600, 77600, 101200, 124800]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "679/679 [==============================] - 89s 132ms/step - loss: 1.2986 - acc: 0.3597 - precision: 0.0766 - recall: 0.7819 - val_loss: 0.6820 - val_acc: 0.6981 - val_precision: 0.1138 - val_recall: 0.6576\n",
      "Epoch 2/50\n",
      "679/679 [==============================] - 89s 131ms/step - loss: 1.1034 - acc: 0.7437 - precision: 0.1593 - recall: 0.6726 - val_loss: 0.5532 - val_acc: 0.8281 - val_precision: 0.1638 - val_recall: 0.5144\n",
      "Epoch 3/50\n",
      "679/679 [==============================] - 89s 131ms/step - loss: 1.1069 - acc: 0.7533 - precision: 0.1632 - recall: 0.6619 - val_loss: 0.5597 - val_acc: 0.7879 - val_precision: 0.1446 - val_recall: 0.5772\n",
      "Epoch 4/50\n",
      "679/679 [==============================] - 89s 131ms/step - loss: 1.0496 - acc: 0.8006 - precision: 0.1974 - recall: 0.6574 - val_loss: 0.4549 - val_acc: 0.8773 - val_precision: 0.1893 - val_recall: 0.3717\n",
      "Epoch 5/50\n",
      "679/679 [==============================] - 89s 130ms/step - loss: 1.1117 - acc: 0.7479 - precision: 0.1610 - recall: 0.6682 - val_loss: 0.4586 - val_acc: 0.8520 - val_precision: 0.1588 - val_recall: 0.3904\n",
      "Epoch 6/50\n",
      "679/679 [==============================] - 88s 130ms/step - loss: 1.0297 - acc: 0.8105 - precision: 0.2075 - recall: 0.6624 - val_loss: 0.4543 - val_acc: 0.8358 - val_precision: 0.1719 - val_recall: 0.5162\n",
      "Epoch 7/50\n",
      "679/679 [==============================] - 89s 131ms/step - loss: 1.1372 - acc: 0.7033 - precision: 0.1409 - recall: 0.6839 - val_loss: 0.7246 - val_acc: 0.6201 - val_precision: 0.0968 - val_recall: 0.7046\n",
      "Epoch 8/50\n",
      "679/679 [==============================] - 89s 131ms/step - loss: 1.0273 - acc: 0.8071 - precision: 0.2047 - recall: 0.6647 - val_loss: 0.4590 - val_acc: 0.8376 - val_precision: 0.1764 - val_recall: 0.5283\n",
      "Epoch 9/50\n",
      "679/679 [==============================] - 89s 131ms/step - loss: 1.1972 - acc: 0.6494 - precision: 0.1180 - recall: 0.6649 - val_loss: 0.6101 - val_acc: 0.6852 - val_precision: 0.1031 - val_recall: 0.6103\n",
      "Epoch 10/50\n",
      "679/679 [==============================] - 89s 131ms/step - loss: 1.0231 - acc: 0.8061 - precision: 0.2048 - recall: 0.6703 - val_loss: 0.4623 - val_acc: 0.8516 - val_precision: 0.1735 - val_recall: 0.4477\n",
      "Epoch 11/50\n",
      "679/679 [==============================] - 89s 131ms/step - loss: 1.0146 - acc: 0.8063 - precision: 0.2046 - recall: 0.6681 - val_loss: 0.4812 - val_acc: 0.8283 - val_precision: 0.1651 - val_recall: 0.5191\n",
      "Epoch 12/50\n",
      "679/679 [==============================] - 89s 131ms/step - loss: 1.0301 - acc: 0.7772 - precision: 0.1838 - recall: 0.6890 - val_loss: 0.5328 - val_acc: 0.8127 - val_precision: 0.1561 - val_recall: 0.5422\n",
      "Epoch 13/50\n",
      "679/679 [==============================] - 88s 130ms/step - loss: 0.9895 - acc: 0.8221 - precision: 0.2200 - recall: 0.6640 - val_loss: 0.4933 - val_acc: 0.8351 - val_precision: 0.1713 - val_recall: 0.5171\n",
      "Epoch 14/50\n",
      "679/679 [==============================] - 89s 131ms/step - loss: 1.0664 - acc: 0.7547 - precision: 0.1683 - recall: 0.6876 - val_loss: 0.5254 - val_acc: 0.8120 - val_precision: 0.1460 - val_recall: 0.4951\n",
      "Epoch 15/50\n",
      "679/679 [==============================] - 89s 131ms/step - loss: 0.9914 - acc: 0.8089 - precision: 0.2094 - recall: 0.6810 - val_loss: 0.5335 - val_acc: 0.8143 - val_precision: 0.1496 - val_recall: 0.5034\n",
      "Epoch 16/50\n",
      "679/679 [==============================] - 89s 131ms/step - loss: 1.0925 - acc: 0.7246 - precision: 0.1528 - recall: 0.6958 - val_loss: 0.6086 - val_acc: 0.7310 - val_precision: 0.1219 - val_recall: 0.6230\n",
      "Epoch 17/50\n",
      "679/679 [==============================] - 89s 131ms/step - loss: 0.9799 - acc: 0.8155 - precision: 0.2163 - recall: 0.6822 - val_loss: 0.5428 - val_acc: 0.7473 - val_precision: 0.1004 - val_recall: 0.4491\n",
      "Epoch 18/50\n",
      "679/679 [==============================] - 89s 131ms/step - loss: 1.1667 - acc: 0.6548 - precision: 0.1229 - recall: 0.6882 - val_loss: 0.6051 - val_acc: 0.7004 - val_precision: 0.1056 - val_recall: 0.5914\n",
      "Epoch 19/50\n",
      "679/679 [==============================] - 89s 131ms/step - loss: 0.9852 - acc: 0.8105 - precision: 0.2119 - recall: 0.6854 - val_loss: 0.4025 - val_acc: 0.8767 - val_precision: 0.1990 - val_recall: 0.4070\n",
      "Epoch 20/50\n",
      "679/679 [==============================] - 89s 131ms/step - loss: 0.9692 - acc: 0.8104 - precision: 0.2121 - recall: 0.6882 - val_loss: 0.5469 - val_acc: 0.7843 - val_precision: 0.1368 - val_recall: 0.5466\n",
      "Epoch 21/50\n",
      "679/679 [==============================] - 89s 131ms/step - loss: 0.9877 - acc: 0.7846 - precision: 0.1902 - recall: 0.6929 - val_loss: 0.5298 - val_acc: 0.8119 - val_precision: 0.1547 - val_recall: 0.5383\n",
      "Epoch 22/50\n",
      "679/679 [==============================] - 89s 131ms/step - loss: 0.9491 - acc: 0.8171 - precision: 0.2196 - recall: 0.6917 - val_loss: 0.4409 - val_acc: 0.8312 - val_precision: 0.1464 - val_recall: 0.4253\n",
      "Epoch 23/50\n",
      "679/679 [==============================] - 89s 131ms/step - loss: 1.0258 - acc: 0.7665 - precision: 0.1773 - recall: 0.6955 - val_loss: 0.6398 - val_acc: 0.7350 - val_precision: 0.1214 - val_recall: 0.6084\n",
      "Epoch 24/50\n",
      "679/679 [==============================] - 89s 131ms/step - loss: 0.9530 - acc: 0.8021 - precision: 0.2081 - recall: 0.7103 - val_loss: 0.4876 - val_acc: 0.8032 - val_precision: 0.1487 - val_recall: 0.5422\n",
      "Epoch 25/50\n",
      "679/679 [==============================] - 89s 131ms/step - loss: 1.0599 - acc: 0.7312 - precision: 0.1587 - recall: 0.7127 - val_loss: 0.6866 - val_acc: 0.6474 - val_precision: 0.0972 - val_recall: 0.6488\n",
      "Epoch 26/50\n",
      "679/679 [==============================] - 89s 131ms/step - loss: 0.9427 - acc: 0.7966 - precision: 0.2054 - recall: 0.7240 - val_loss: 0.4058 - val_acc: 0.8530 - val_precision: 0.1748 - val_recall: 0.4465\n",
      "Epoch 27/50\n",
      "679/679 [==============================] - 89s 131ms/step - loss: 1.1359 - acc: 0.6725 - precision: 0.1298 - recall: 0.6932 - val_loss: 0.5756 - val_acc: 0.7238 - val_precision: 0.1080 - val_recall: 0.5510\n",
      "Epoch 28/50\n",
      "679/679 [==============================] - 89s 131ms/step - loss: 0.9393 - acc: 0.7989 - precision: 0.2074 - recall: 0.7226 - val_loss: 0.4381 - val_acc: 0.8631 - val_precision: 0.1875 - val_recall: 0.4434\n",
      "Epoch 29/50\n",
      "679/679 [==============================] - 89s 131ms/step - loss: 0.9283 - acc: 0.7882 - precision: 0.2001 - recall: 0.7353 - val_loss: 0.5084 - val_acc: 0.7936 - val_precision: 0.1362 - val_recall: 0.5116\n",
      "Epoch 30/50\n",
      "679/679 [==============================] - 89s 131ms/step - loss: 0.9546 - acc: 0.7773 - precision: 0.1888 - recall: 0.7184 - val_loss: 0.4533 - val_acc: 0.8366 - val_precision: 0.1559 - val_recall: 0.4430\n",
      "Epoch 31/50\n",
      "679/679 [==============================] - 89s 130ms/step - loss: 0.9125 - acc: 0.8026 - precision: 0.2123 - recall: 0.7326 - val_loss: 0.4018 - val_acc: 0.8419 - val_precision: 0.1389 - val_recall: 0.3579\n",
      "Epoch 32/50\n",
      "679/679 [==============================] - 89s 131ms/step - loss: 0.9983 - acc: 0.7612 - precision: 0.1776 - recall: 0.7198 - val_loss: 0.4963 - val_acc: 0.8136 - val_precision: 0.1469 - val_recall: 0.4936\n",
      "Epoch 33/50\n",
      "679/679 [==============================] - 89s 131ms/step - loss: 0.9216 - acc: 0.7990 - precision: 0.2089 - recall: 0.7315 - val_loss: 0.5952 - val_acc: 0.7032 - val_precision: 0.1008 - val_recall: 0.5515\n",
      "Epoch 34/50\n",
      "679/679 [==============================] - 89s 131ms/step - loss: 1.0309 - acc: 0.7407 - precision: 0.1638 - recall: 0.7113 - val_loss: 0.4600 - val_acc: 0.8242 - val_precision: 0.1416 - val_recall: 0.4305\n",
      "Epoch 35/50\n",
      "679/679 [==============================] - 89s 131ms/step - loss: 0.9093 - acc: 0.7928 - precision: 0.2070 - recall: 0.7535 - val_loss: 0.4268 - val_acc: 0.8328 - val_precision: 0.1575 - val_recall: 0.4657\n",
      "Epoch 36/50\n",
      "679/679 [==============================] - 89s 131ms/step - loss: 1.1081 - acc: 0.6807 - precision: 0.1354 - recall: 0.7110 - val_loss: 0.6297 - val_acc: 0.6443 - val_precision: 0.0893 - val_recall: 0.5907\n",
      "Epoch 37/50\n",
      "679/679 [==============================] - 89s 131ms/step - loss: 0.9023 - acc: 0.7892 - precision: 0.2025 - recall: 0.7443 - val_loss: 0.5242 - val_acc: 0.7873 - val_precision: 0.1374 - val_recall: 0.5396\n",
      "Epoch 38/50\n",
      "679/679 [==============================] - 89s 131ms/step - loss: 0.8893 - acc: 0.7862 - precision: 0.2031 - recall: 0.7639 - val_loss: 0.4233 - val_acc: 0.8266 - val_precision: 0.1566 - val_recall: 0.4872\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39/50\n",
      "679/679 [==============================] - 89s 131ms/step - loss: 0.9268 - acc: 0.7777 - precision: 0.1926 - recall: 0.7402 - val_loss: 0.5127 - val_acc: 0.8059 - val_precision: 0.1454 - val_recall: 0.5149\n",
      "Epoch 40/50\n",
      "679/679 [==============================] - 89s 131ms/step - loss: 0.8763 - acc: 0.7978 - precision: 0.2118 - recall: 0.7567 - val_loss: 0.5536 - val_acc: 0.7132 - val_precision: 0.1024 - val_recall: 0.5396\n",
      "Epoch 41/50\n",
      "679/679 [==============================] - 89s 131ms/step - loss: 0.9667 - acc: 0.7643 - precision: 0.1815 - recall: 0.7311 - val_loss: 0.4656 - val_acc: 0.8219 - val_precision: 0.1483 - val_recall: 0.4687\n",
      "Epoch 42/50\n",
      "679/679 [==============================] - 89s 130ms/step - loss: 0.8808 - acc: 0.7878 - precision: 0.2043 - recall: 0.7626 - val_loss: 0.5274 - val_acc: 0.7600 - val_precision: 0.1248 - val_recall: 0.5561\n",
      "Epoch 43/50\n",
      "679/679 [==============================] - 89s 131ms/step - loss: 1.0024 - acc: 0.7420 - precision: 0.1674 - recall: 0.7303 - val_loss: 0.5514 - val_acc: 0.7588 - val_precision: 0.1164 - val_recall: 0.5106\n",
      "Epoch 44/50\n",
      "679/679 [==============================] - 89s 130ms/step - loss: 0.8717 - acc: 0.7900 - precision: 0.2074 - recall: 0.7713 - val_loss: 0.4668 - val_acc: 0.7880 - val_precision: 0.1107 - val_recall: 0.4030\n",
      "Epoch 45/50\n",
      "679/679 [==============================] - 89s 130ms/step - loss: 1.0859 - acc: 0.6850 - precision: 0.1388 - recall: 0.7229 - val_loss: 0.4811 - val_acc: 0.7725 - val_precision: 0.1120 - val_recall: 0.4496\n",
      "Epoch 46/50\n",
      "679/679 [==============================] - 89s 131ms/step - loss: 0.8681 - acc: 0.7884 - precision: 0.2059 - recall: 0.7698 - val_loss: 0.4548 - val_acc: 0.8326 - val_precision: 0.1601 - val_recall: 0.4778\n",
      "Epoch 47/50\n",
      "679/679 [==============================] - 89s 131ms/step - loss: 0.8527 - acc: 0.7909 - precision: 0.2090 - recall: 0.7764 - val_loss: 0.4899 - val_acc: 0.7820 - val_precision: 0.1243 - val_recall: 0.4873\n",
      "Epoch 48/50\n",
      "679/679 [==============================] - 89s 131ms/step - loss: 0.9066 - acc: 0.7794 - precision: 0.1957 - recall: 0.7509 - val_loss: 0.5416 - val_acc: 0.7809 - val_precision: 0.1385 - val_recall: 0.5680\n",
      "Epoch 49/50\n",
      "679/679 [==============================] - 89s 131ms/step - loss: 0.8440 - acc: 0.7960 - precision: 0.2131 - recall: 0.7746 - val_loss: 0.4180 - val_acc: 0.8216 - val_precision: 0.1388 - val_recall: 0.4280\n",
      "Epoch 50/50\n",
      "679/679 [==============================] - 89s 131ms/step - loss: 0.9315 - acc: 0.7658 - precision: 0.1856 - recall: 0.7504 - val_loss: 0.4412 - val_acc: 0.8322 - val_precision: 0.1461 - val_recall: 0.4201\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Experiment skipped due to error. Model: {'experiment_num': [296], 'valid': [True], 'num_mels': [80], 'win_len': [10], 'chunk_radius': [11], 'batch_size': [400], 'learning_rate': [0.0001], 'note_class_weight': [{0: 1, 1: 15}], 'model_version': [1], 'spec_conv_layers': [[{'filters': 64, 'kernel_size': (3, 3), 'layers': 2, 'max_pool': True, 'activation': 'relu'}, {'filters': 128, 'kernel_size': (3, 3), 'layers': 2, 'max_pool': True, 'activation': 'relu'}, {'filters': 256, 'kernel_size': (3, 3), 'layers': 2, 'max_pool': True, 'activation': 'relu'}, {'filters': 512, 'kernel_size': (3, 3), 'layers': 2, 'max_pool': True, 'activation': 'relu'}]], 'spec_dense_layers': [[]], 'combined_layers': [[{'units': 23, 'activation': 'relu', 'num_layers': 3}]]}\n",
      "Error: Negative dimension size caused by subtracting 3 from 1 for 'conv2d_368/convolution' (op: 'Conv2D') with input shapes: [?,8,1,256], [3,3,256,512].\n",
      "INFO:root:Indices are: [22400, 44800, 67200, 89200, 111200, 133200, 157200, 174400, 191600, 208800, 226000, 248800, 271600]\n",
      "INFO:root:Indices are: [14400, 28800, 43200, 57600, 77600, 101200, 124800]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "679/679 [==============================] - 19s 28ms/step - loss: 1.4393 - acc: 0.3141 - precision: 0.0793 - recall: 0.8785 - val_loss: 0.7241 - val_acc: 0.6780 - val_precision: 0.1092 - val_recall: 0.6741\n",
      "Epoch 2/50\n",
      "679/679 [==============================] - 19s 27ms/step - loss: 1.2796 - acc: 0.7082 - precision: 0.1461 - recall: 0.7050 - val_loss: 0.7321 - val_acc: 0.7398 - val_precision: 0.1275 - val_recall: 0.6348\n",
      "Epoch 3/50\n",
      "679/679 [==============================] - 18s 27ms/step - loss: 1.2946 - acc: 0.6928 - precision: 0.1388 - recall: 0.7010 - val_loss: 0.6838 - val_acc: 0.7882 - val_precision: 0.1408 - val_recall: 0.5551\n",
      "Epoch 4/50\n",
      "679/679 [==============================] - 19s 28ms/step - loss: 1.2509 - acc: 0.7459 - precision: 0.1611 - recall: 0.6763 - val_loss: 0.7737 - val_acc: 0.7407 - val_precision: 0.1286 - val_recall: 0.6390\n",
      "Epoch 5/50\n",
      "679/679 [==============================] - 19s 28ms/step - loss: 1.3308 - acc: 0.6168 - precision: 0.1167 - recall: 0.7299 - val_loss: 0.6396 - val_acc: 0.7794 - val_precision: 0.1357 - val_recall: 0.5572\n",
      "Epoch 6/50\n",
      "679/679 [==============================] - 19s 28ms/step - loss: 1.2212 - acc: 0.7451 - precision: 0.1661 - recall: 0.7100 - val_loss: 0.6373 - val_acc: 0.7563 - val_precision: 0.1357 - val_recall: 0.6348\n",
      "Epoch 7/50\n",
      "679/679 [==============================] - 18s 27ms/step - loss: 1.3471 - acc: 0.5806 - precision: 0.1104 - recall: 0.7572 - val_loss: 0.5197 - val_acc: 0.8231 - val_precision: 0.1473 - val_recall: 0.4597\n",
      "Epoch 8/50\n",
      "679/679 [==============================] - 19s 27ms/step - loss: 1.2075 - acc: 0.7326 - precision: 0.1607 - recall: 0.7214 - val_loss: 0.6779 - val_acc: 0.7593 - val_precision: 0.1346 - val_recall: 0.6181\n",
      "Epoch 9/50\n",
      "679/679 [==============================] - 19s 29ms/step - loss: 1.3900 - acc: 0.5091 - precision: 0.0967 - recall: 0.7702 - val_loss: 0.8659 - val_acc: 0.4013 - val_precision: 0.0754 - val_recall: 0.8729\n",
      "Epoch 10/50\n",
      "679/679 [==============================] - 19s 28ms/step - loss: 1.2190 - acc: 0.7114 - precision: 0.1515 - recall: 0.7310 - val_loss: 0.6483 - val_acc: 0.7462 - val_precision: 0.1294 - val_recall: 0.6270\n",
      "Epoch 11/50\n",
      "679/679 [==============================] - 19s 29ms/step - loss: 1.2060 - acc: 0.7225 - precision: 0.1575 - recall: 0.7350 - val_loss: 0.5640 - val_acc: 0.8018 - val_precision: 0.1466 - val_recall: 0.5362\n",
      "Epoch 12/50\n",
      "679/679 [==============================] - 19s 28ms/step - loss: 1.2411 - acc: 0.6828 - precision: 0.1393 - recall: 0.7333 - val_loss: 0.6176 - val_acc: 0.7786 - val_precision: 0.1374 - val_recall: 0.5695\n",
      "Epoch 13/50\n",
      "679/679 [==============================] - 18s 27ms/step - loss: 1.1826 - acc: 0.7348 - precision: 0.1622 - recall: 0.7229 - val_loss: 0.5850 - val_acc: 0.8117 - val_precision: 0.1523 - val_recall: 0.5273\n",
      "Epoch 14/50\n",
      "679/679 [==============================] - 19s 27ms/step - loss: 1.2683 - acc: 0.6648 - precision: 0.1323 - recall: 0.7323 - val_loss: 0.6468 - val_acc: 0.7350 - val_precision: 0.1224 - val_recall: 0.6150\n",
      "Epoch 15/50\n",
      "679/679 [==============================] - 19s 28ms/step - loss: 1.1545 - acc: 0.7453 - precision: 0.1704 - recall: 0.7375 - val_loss: 0.4887 - val_acc: 0.8255 - val_precision: 0.1603 - val_recall: 0.5089\n",
      "Epoch 16/50\n",
      "679/679 [==============================] - 19s 28ms/step - loss: 1.3012 - acc: 0.6010 - precision: 0.1172 - recall: 0.7704 - val_loss: 0.6083 - val_acc: 0.7266 - val_precision: 0.1174 - val_recall: 0.6058\n",
      "Epoch 17/50\n",
      "679/679 [==============================] - 19s 28ms/step - loss: 1.1494 - acc: 0.7282 - precision: 0.1629 - recall: 0.7516 - val_loss: 0.5268 - val_acc: 0.8014 - val_precision: 0.1438 - val_recall: 0.5235\n",
      "Epoch 18/50\n",
      "679/679 [==============================] - 19s 28ms/step - loss: 1.3684 - acc: 0.5062 - precision: 0.1001 - recall: 0.8098 - val_loss: 0.7162 - val_acc: 0.5996 - val_precision: 0.0918 - val_recall: 0.7018\n",
      "Epoch 19/50\n",
      "679/679 [==============================] - 19s 28ms/step - loss: 1.1552 - acc: 0.7285 - precision: 0.1618 - recall: 0.7434 - val_loss: 0.5135 - val_acc: 0.8308 - val_precision: 0.1488 - val_recall: 0.4367\n",
      "Epoch 20/50\n",
      "679/679 [==============================] - 19s 28ms/step - loss: 1.1441 - acc: 0.7312 - precision: 0.1644 - recall: 0.7509 - val_loss: 0.6162 - val_acc: 0.7285 - val_precision: 0.1167 - val_recall: 0.5957prec - ETA: 1s - loss: 1.1395 \n",
      "Epoch 21/50\n",
      "679/679 [==============================] - 19s 28ms/step - loss: 1.1780 - acc: 0.6830 - precision: 0.1453 - recall: 0.7771 - val_loss: 0.4815 - val_acc: 0.8239 - val_precision: 0.1433 - val_recall: 0.4392\n",
      "Epoch 22/50\n",
      "679/679 [==============================] - 19s 28ms/step - loss: 1.1256 - acc: 0.7093 - precision: 0.1571 - recall: 0.7786 - val_loss: 0.5399 - val_acc: 0.7656 - val_precision: 0.1234 - val_recall: 0.5310\n",
      "Epoch 23/50\n",
      "679/679 [==============================] - 19s 29ms/step - loss: 1.2183 - acc: 0.6487 - precision: 0.1333 - recall: 0.7846 - val_loss: 0.5781 - val_acc: 0.7411 - val_precision: 0.1062 - val_recall: 0.4970- precision: 0.1336 - recall: 0.\n",
      "Epoch 24/50\n",
      "679/679 [==============================] - 19s 28ms/step - loss: 1.0995 - acc: 0.7112 - precision: 0.1592 - recall: 0.7864 - val_loss: 0.6189 - val_acc: 0.6989 - val_precision: 0.1054 - val_recall: 0.5942\n",
      "Epoch 25/50\n",
      "679/679 [==============================] - 19s 28ms/step - loss: 1.2518 - acc: 0.6075 - precision: 0.1221 - recall: 0.7973 - val_loss: 0.7777 - val_acc: 0.5340 - val_precision: 0.0822 - val_recall: 0.7314\n",
      "Epoch 26/50\n",
      "679/679 [==============================] - 20s 29ms/step - loss: 1.0987 - acc: 0.7044 - precision: 0.1574 - recall: 0.7972 - val_loss: 0.6022 - val_acc: 0.7441 - val_precision: 0.1231 - val_recall: 0.5929\n",
      "Epoch 27/50\n",
      "679/679 [==============================] - 19s 28ms/step - loss: 1.3126 - acc: 0.5469 - precision: 0.1084 - recall: 0.8094 - val_loss: 0.7208 - val_acc: 0.5741 - val_precision: 0.0842 - val_recall: 0.6794\n",
      "Epoch 28/50\n",
      "679/679 [==============================] - 19s 28ms/step - loss: 1.0807 - acc: 0.7132 - precision: 0.1617 - recall: 0.7977 - val_loss: 0.5595 - val_acc: 0.7603 - val_precision: 0.1209 - val_recall: 0.5320\n",
      "Epoch 29/50\n",
      "679/679 [==============================] - 19s 28ms/step - loss: 1.0735 - acc: 0.7074 - precision: 0.1605 - recall: 0.8098 - val_loss: 0.5717 - val_acc: 0.7384 - val_precision: 0.1129 - val_recall: 0.5443\n",
      "Epoch 30/50\n",
      "679/679 [==============================] - 19s 28ms/step - loss: 1.1140 - acc: 0.6763 - precision: 0.1479 - recall: 0.8187 - val_loss: 0.5110 - val_acc: 0.7922 - val_precision: 0.1205 - val_recall: 0.4379\n",
      "Epoch 31/50\n",
      "679/679 [==============================] - 19s 28ms/step - loss: 1.0689 - acc: 0.7111 - precision: 0.1622 - recall: 0.8090 - val_loss: 0.6700 - val_acc: 0.6333 - val_precision: 0.0856 - val_recall: 0.5818\n",
      "Epoch 32/50\n",
      "679/679 [==============================] - 19s 28ms/step - loss: 1.1711 - acc: 0.6488 - precision: 0.1364 - recall: 0.8091 - val_loss: 0.5473 - val_acc: 0.7331 - val_precision: 0.1026 - val_recall: 0.4942\n",
      "Epoch 33/50\n",
      "679/679 [==============================] - 20s 29ms/step - loss: 1.0246 - acc: 0.7135 - precision: 0.1658 - recall: 0.8266 - val_loss: 0.5802 - val_acc: 0.7139 - val_precision: 0.1100 - val_recall: 0.5886\n",
      "Epoch 34/50\n",
      "679/679 [==============================] - 19s 29ms/step - loss: 1.1968 - acc: 0.6199 - precision: 0.1294 - recall: 0.8283 - val_loss: 0.4458 - val_acc: 0.8097 - val_precision: 0.1261 - val_recall: 0.4119\n",
      "Epoch 35/50\n",
      "679/679 [==============================] - 19s 28ms/step - loss: 1.0102 - acc: 0.7192 - precision: 0.1700 - recall: 0.8356 - val_loss: 0.4146 - val_acc: 0.8374 - val_precision: 0.1553 - val_recall: 0.4376\n",
      "Epoch 36/50\n",
      "679/679 [==============================] - 20s 29ms/step - loss: 1.2821 - acc: 0.5569 - precision: 0.1119 - recall: 0.8215 - val_loss: 0.6791 - val_acc: 0.6226 - val_precision: 0.0851 - val_recall: 0.5982\n",
      "Epoch 37/50\n",
      "679/679 [==============================] - 19s 28ms/step - loss: 1.0180 - acc: 0.7101 - precision: 0.1658 - recall: 0.8382 - val_loss: 0.5645 - val_acc: 0.7429 - val_precision: 0.1182 - val_recall: 0.5657\n",
      "Epoch 38/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "679/679 [==============================] - 19s 29ms/step - loss: 1.0116 - acc: 0.7074 - precision: 0.1648 - recall: 0.8419 - val_loss: 0.4811 - val_acc: 0.7965 - val_precision: 0.1257 - val_recall: 0.4503\n",
      "Epoch 39/50\n",
      "679/679 [==============================] - 19s 28ms/step - loss: 1.0588 - acc: 0.6898 - precision: 0.1561 - recall: 0.8384 - val_loss: 0.5581 - val_acc: 0.7360 - val_precision: 0.1019 - val_recall: 0.4834\n",
      "Epoch 40/50\n",
      "679/679 [==============================] - 19s 28ms/step - loss: 0.9938 - acc: 0.7164 - precision: 0.1692 - recall: 0.8409 - val_loss: 0.5310 - val_acc: 0.7584 - val_precision: 0.1082 - val_recall: 0.4654\n",
      "Epoch 41/50\n",
      "679/679 [==============================] - 19s 28ms/step - loss: 1.1104 - acc: 0.6637 - precision: 0.1447 - recall: 0.8314 - val_loss: 0.4921 - val_acc: 0.7706 - val_precision: 0.1121 - val_recall: 0.4553\n",
      "Epoch 42/50\n",
      "679/679 [==============================] - 19s 28ms/step - loss: 0.9551 - acc: 0.7198 - precision: 0.1732 - recall: 0.8576 - val_loss: 0.4385 - val_acc: 0.8250 - val_precision: 0.1479 - val_recall: 0.4548\n",
      "Epoch 43/50\n",
      "679/679 [==============================] - 21s 31ms/step - loss: 1.1372 - acc: 0.6515 - precision: 0.1404 - recall: 0.8339 - val_loss: 0.6282 - val_acc: 0.6830 - val_precision: 0.0902 - val_recall: 0.5212\n",
      "Epoch 44/50\n",
      "679/679 [==============================] - 22s 32ms/step - loss: 0.9380 - acc: 0.7268 - precision: 0.1780 - recall: 0.8652 - val_loss: 0.4594 - val_acc: 0.7990 - val_precision: 0.1270 - val_recall: 0.4488\n",
      "Epoch 45/50\n",
      "679/679 [==============================] - 20s 30ms/step - loss: 1.2338 - acc: 0.5904 - precision: 0.1210 - recall: 0.8282 - val_loss: 0.5594 - val_acc: 0.6995 - val_precision: 0.0856 - val_recall: 0.4582\n",
      "Epoch 46/50\n",
      "679/679 [==============================] - 19s 28ms/step - loss: 0.9608 - acc: 0.7257 - precision: 0.1759 - recall: 0.8544 - val_loss: 0.4050 - val_acc: 0.8370 - val_precision: 0.1480 - val_recall: 0.4099\n",
      "Epoch 47/50\n",
      "679/679 [==============================] - 19s 28ms/step - loss: 0.9650 - acc: 0.7143 - precision: 0.1706 - recall: 0.8596 - val_loss: 0.4085 - val_acc: 0.8341 - val_precision: 0.1413 - val_recall: 0.3942\n",
      "Epoch 48/50\n",
      "679/679 [==============================] - 19s 28ms/step - loss: 0.9998 - acc: 0.7047 - precision: 0.1648 - recall: 0.8519 - val_loss: 0.5917 - val_acc: 0.7257 - val_precision: 0.1014 - val_recall: 0.5037\n",
      "Epoch 49/50\n",
      "679/679 [==============================] - 19s 28ms/step - loss: 0.9242 - acc: 0.7371 - precision: 0.1836 - recall: 0.8632 - val_loss: 0.4670 - val_acc: 0.8180 - val_precision: 0.1272 - val_recall: 0.3913\n",
      "Epoch 50/50\n",
      "679/679 [==============================] - 19s 28ms/step - loss: 1.0484 - acc: 0.6872 - precision: 0.1556 - recall: 0.8434 - val_loss: 0.5450 - val_acc: 0.7273 - val_precision: 0.0966 - val_recall: 0.4711\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Indices are: [22400, 44800, 67200, 89200, 111200, 133200, 157200, 174400, 191600, 208800, 226000, 248800, 271600]\n",
      "INFO:root:Indices are: [14400, 28800, 43200, 57600, 77600, 101200, 124800]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "679/679 [==============================] - 90s 133ms/step - loss: 1.5156 - acc: 0.1526 - precision: 0.0692 - recall: 0.9428 - val_loss: 0.8707 - val_acc: 0.1440 - val_precision: 0.0601 - val_recall: 0.9902\n",
      "Epoch 2/50\n",
      "679/679 [==============================] - 89s 131ms/step - loss: 1.3093 - acc: 0.6015 - precision: 0.1171 - recall: 0.7684 - val_loss: 0.7306 - val_acc: 0.7019 - val_precision: 0.1170 - val_recall: 0.6715\n",
      "Epoch 3/50\n",
      "679/679 [==============================] - 89s 131ms/step - loss: 1.2980 - acc: 0.6598 - precision: 0.1301 - recall: 0.7285 - val_loss: 0.6143 - val_acc: 0.7081 - val_precision: 0.1118 - val_recall: 0.6169\n",
      "Epoch 4/50\n",
      "679/679 [==============================] - 89s 131ms/step - loss: 1.2345 - acc: 0.7352 - precision: 0.1590 - recall: 0.7006 - val_loss: 0.5882 - val_acc: 0.8016 - val_precision: 0.1549 - val_recall: 0.5812\n",
      "Epoch 5/50\n",
      "679/679 [==============================] - 89s 131ms/step - loss: 1.2948 - acc: 0.6453 - precision: 0.1273 - recall: 0.7453 - val_loss: 0.6487 - val_acc: 0.7379 - val_precision: 0.1183 - val_recall: 0.5799\n",
      "Epoch 6/50\n",
      "679/679 [==============================] - 89s 131ms/step - loss: 1.2057 - acc: 0.7442 - precision: 0.1643 - recall: 0.7022 - val_loss: 0.6082 - val_acc: 0.7770 - val_precision: 0.1422 - val_recall: 0.6034\n",
      "Epoch 7/50\n",
      "679/679 [==============================] - 89s 131ms/step - loss: 1.3213 - acc: 0.5970 - precision: 0.1152 - recall: 0.7627 - val_loss: 0.6758 - val_acc: 0.6451 - val_precision: 0.1010 - val_recall: 0.6864\n",
      "Epoch 8/50\n",
      "679/679 [==============================] - 89s 131ms/step - loss: 1.1840 - acc: 0.7547 - precision: 0.1702 - recall: 0.6996 - val_loss: 0.7656 - val_acc: 0.6882 - val_precision: 0.1128 - val_recall: 0.6760\n",
      "Epoch 9/50\n",
      "679/679 [==============================] - 89s 131ms/step - loss: 1.3718 - acc: 0.5163 - precision: 0.1002 - recall: 0.7913 - val_loss: 0.7506 - val_acc: 0.5148 - val_precision: 0.0809 - val_recall: 0.7508\n",
      "Epoch 10/50\n",
      "679/679 [==============================] - 89s 132ms/step - loss: 1.1918 - acc: 0.7350 - precision: 0.1632 - recall: 0.7287 - val_loss: 0.6958 - val_acc: 0.7449 - val_precision: 0.1221 - val_recall: 0.5840\n",
      "Epoch 11/50\n",
      "679/679 [==============================] - 89s 131ms/step - loss: 1.1712 - acc: 0.7193 - precision: 0.1565 - recall: 0.7397 - val_loss: 0.6700 - val_acc: 0.7117 - val_precision: 0.1153 - val_recall: 0.6323\n",
      "Epoch 12/50\n",
      "679/679 [==============================] - 89s 131ms/step - loss: 1.1956 - acc: 0.6891 - precision: 0.1445 - recall: 0.7522 - val_loss: 0.6404 - val_acc: 0.7295 - val_precision: 0.1248 - val_recall: 0.6479\n",
      "Epoch 13/50\n",
      "679/679 [==============================] - 89s 131ms/step - loss: 1.1461 - acc: 0.7286 - precision: 0.1620 - recall: 0.7436 - val_loss: 0.5648 - val_acc: 0.7598 - val_precision: 0.1279 - val_recall: 0.5753\n",
      "Epoch 14/50\n",
      "679/679 [==============================] - 89s 131ms/step - loss: 1.2266 - acc: 0.6589 - precision: 0.1350 - recall: 0.7695 - val_loss: 0.6768 - val_acc: 0.6617 - val_precision: 0.1048 - val_recall: 0.6790\n",
      "Epoch 15/50\n",
      "679/679 [==============================] - 89s 131ms/step - loss: 1.1344 - acc: 0.7357 - precision: 0.1660 - recall: 0.7450 - val_loss: 0.6193 - val_acc: 0.7119 - val_precision: 0.1150 - val_recall: 0.6290\n",
      "Epoch 16/50\n",
      "679/679 [==============================] - 89s 131ms/step - loss: 1.2748 - acc: 0.6016 - precision: 0.1196 - recall: 0.7899 - val_loss: 0.6714 - val_acc: 0.6678 - val_precision: 0.1048 - val_recall: 0.6640\n",
      "Epoch 17/50\n",
      "679/679 [==============================] - 89s 131ms/step - loss: 1.1326 - acc: 0.7417 - precision: 0.1703 - recall: 0.7506 - val_loss: 0.6823 - val_acc: 0.6798 - val_precision: 0.0998 - val_recall: 0.5979\n",
      "Epoch 18/50\n",
      "679/679 [==============================] - 89s 131ms/step - loss: 1.3400 - acc: 0.5127 - precision: 0.1019 - recall: 0.8150 - val_loss: 0.5322 - val_acc: 0.7085 - val_precision: 0.0944 - val_recall: 0.4977\n",
      "Epoch 19/50\n",
      "679/679 [==============================] - 89s 131ms/step - loss: 1.1399 - acc: 0.7319 - precision: 0.1662 - recall: 0.7608 - val_loss: 0.7894 - val_acc: 0.5706 - val_precision: 0.0783 - val_recall: 0.6285\n",
      "Epoch 20/50\n",
      "679/679 [==============================] - 89s 131ms/step - loss: 1.1239 - acc: 0.7166 - precision: 0.1597 - recall: 0.7710 - val_loss: 0.6251 - val_acc: 0.7160 - val_precision: 0.1138 - val_recall: 0.6100\n",
      "Epoch 21/50\n",
      "679/679 [==============================] - 89s 131ms/step - loss: 1.1522 - acc: 0.6779 - precision: 0.1443 - recall: 0.7855 - val_loss: 0.5902 - val_acc: 0.7367 - val_precision: 0.1200 - val_recall: 0.5942\n",
      "Epoch 22/50\n",
      "679/679 [==============================] - 89s 131ms/step - loss: 1.1046 - acc: 0.7300 - precision: 0.1672 - recall: 0.7739 - val_loss: 0.7451 - val_acc: 0.5998 - val_precision: 0.0862 - val_recall: 0.6501\n",
      "Epoch 23/50\n",
      "679/679 [==============================] - 89s 131ms/step - loss: 1.1874 - acc: 0.6470 - precision: 0.1350 - recall: 0.8025 - val_loss: 0.5390 - val_acc: 0.7714 - val_precision: 0.1274 - val_recall: 0.5361\n",
      "Epoch 24/50\n",
      "679/679 [==============================] - 89s 131ms/step - loss: 1.1017 - acc: 0.7265 - precision: 0.1658 - recall: 0.7781 - val_loss: 0.5649 - val_acc: 0.7241 - val_precision: 0.1116 - val_recall: 0.5734\n",
      "Epoch 25/50\n",
      "679/679 [==============================] - 89s 132ms/step - loss: 1.2402 - acc: 0.5962 - precision: 0.1203 - recall: 0.8086 - val_loss: 0.6634 - val_acc: 0.6640 - val_precision: 0.1002 - val_recall: 0.6365\n",
      "Epoch 26/50\n",
      "679/679 [==============================] - 89s 131ms/step - loss: 1.0988 - acc: 0.7349 - precision: 0.1702 - recall: 0.7761 - val_loss: 0.4915 - val_acc: 0.7691 - val_precision: 0.1283 - val_recall: 0.5482\n",
      "Epoch 27/50\n",
      "679/679 [==============================] - 89s 131ms/step - loss: 1.3181 - acc: 0.5312 - precision: 0.1057 - recall: 0.8150 - val_loss: 0.7585 - val_acc: 0.5163 - val_precision: 0.0783 - val_recall: 0.7194\n",
      "Epoch 28/50\n",
      "679/679 [==============================] - 89s 131ms/step - loss: 1.1090 - acc: 0.7080 - precision: 0.1593 - recall: 0.7979 - val_loss: 0.5344 - val_acc: 0.7504 - val_precision: 0.1191 - val_recall: 0.5498\n",
      "Epoch 29/50\n",
      "679/679 [==============================] - 89s 131ms/step - loss: 1.0741 - acc: 0.7128 - precision: 0.1623 - recall: 0.8032 - val_loss: 0.5212 - val_acc: 0.7453 - val_precision: 0.1168 - val_recall: 0.5501\n",
      "Epoch 30/50\n",
      "679/679 [==============================] - 89s 131ms/step - loss: 1.1085 - acc: 0.6767 - precision: 0.1473 - recall: 0.8124 - val_loss: 0.5102 - val_acc: 0.7624 - val_precision: 0.1269 - val_recall: 0.5611\n",
      "Epoch 31/50\n",
      "679/679 [==============================] - 89s 131ms/step - loss: 1.0610 - acc: 0.7091 - precision: 0.1617 - recall: 0.8119 - val_loss: 0.5284 - val_acc: 0.7402 - val_precision: 0.1201 - val_recall: 0.5847\n",
      "Epoch 32/50\n",
      "679/679 [==============================] - 89s 131ms/step - loss: 1.1512 - acc: 0.6604 - precision: 0.1400 - recall: 0.8047 - val_loss: 0.5444 - val_acc: 0.7520 - val_precision: 0.1184 - val_recall: 0.5408\n",
      "Epoch 33/50\n",
      "679/679 [==============================] - 89s 131ms/step - loss: 1.0538 - acc: 0.7079 - precision: 0.1619 - recall: 0.8178 - val_loss: 0.5571 - val_acc: 0.7325 - val_precision: 0.1188 - val_recall: 0.5982\n",
      "Epoch 34/50\n",
      "679/679 [==============================] - 89s 131ms/step - loss: 1.2006 - acc: 0.6189 - precision: 0.1275 - recall: 0.8138 - val_loss: 0.5859 - val_acc: 0.7274 - val_precision: 0.1108 - val_recall: 0.5598\n",
      "Epoch 35/50\n",
      "679/679 [==============================] - 89s 131ms/step - loss: 1.0586 - acc: 0.7302 - precision: 0.1714 - recall: 0.8016 - val_loss: 0.5863 - val_acc: 0.7334 - val_precision: 0.1174 - val_recall: 0.5864\n",
      "Epoch 36/50\n",
      "679/679 [==============================] - 89s 131ms/step - loss: 1.2898 - acc: 0.5461 - precision: 0.1093 - recall: 0.8198 - val_loss: 0.7767 - val_acc: 0.5475 - val_precision: 0.0823 - val_recall: 0.7082\n",
      "Epoch 37/50\n",
      "679/679 [==============================] - 89s 131ms/step - loss: 1.0603 - acc: 0.7093 - precision: 0.1634 - recall: 0.8237 - val_loss: 0.6087 - val_acc: 0.7221 - val_precision: 0.1145 - val_recall: 0.5979\n",
      "Epoch 38/50\n",
      "679/679 [==============================] - 89s 131ms/step - loss: 1.0350 - acc: 0.7097 - precision: 0.1639 - recall: 0.8267 - val_loss: 0.4637 - val_acc: 0.8144 - val_precision: 0.1527 - val_recall: 0.5185\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39/50\n",
      "679/679 [==============================] - 89s 131ms/step - loss: 1.0717 - acc: 0.6857 - precision: 0.1526 - recall: 0.8248 - val_loss: 0.6835 - val_acc: 0.6395 - val_precision: 0.0923 - val_recall: 0.6252\n",
      "Epoch 40/50\n",
      "679/679 [==============================] - 89s 131ms/step - loss: 1.0106 - acc: 0.7249 - precision: 0.1718 - recall: 0.8271 - val_loss: 0.5830 - val_acc: 0.7348 - val_precision: 0.1200 - val_recall: 0.6001\n",
      "Epoch 41/50\n",
      "679/679 [==============================] - 89s 131ms/step - loss: 1.1130 - acc: 0.6657 - precision: 0.1446 - recall: 0.8252 - val_loss: 0.5275 - val_acc: 0.7815 - val_precision: 0.1241 - val_recall: 0.4876\n",
      "Epoch 42/50\n",
      "679/679 [==============================] - 89s 131ms/step - loss: 0.9985 - acc: 0.7169 - precision: 0.1690 - recall: 0.8376 - val_loss: 0.5328 - val_acc: 0.7349 - val_precision: 0.1146 - val_recall: 0.5643\n",
      "Epoch 43/50\n",
      "679/679 [==============================] - 89s 131ms/step - loss: 1.1630 - acc: 0.6431 - precision: 0.1356 - recall: 0.8172 - val_loss: 0.5380 - val_acc: 0.7328 - val_precision: 0.1009 - val_recall: 0.4848\n",
      "Epoch 44/50\n",
      "679/679 [==============================] - 89s 131ms/step - loss: 0.9960 - acc: 0.7233 - precision: 0.1727 - recall: 0.8398 - val_loss: 0.5790 - val_acc: 0.6985 - val_precision: 0.0867 - val_recall: 0.4674\n",
      "Epoch 45/50\n",
      "679/679 [==============================] - 89s 131ms/step - loss: 1.2616 - acc: 0.5608 - precision: 0.1132 - recall: 0.8256 - val_loss: 0.6177 - val_acc: 0.6632 - val_precision: 0.0871 - val_recall: 0.5374\n",
      "Epoch 46/50\n",
      "679/679 [==============================] - 89s 131ms/step - loss: 1.0133 - acc: 0.7157 - precision: 0.1678 - recall: 0.8323 - val_loss: 0.4587 - val_acc: 0.8132 - val_precision: 0.1470 - val_recall: 0.4955\n",
      "Epoch 47/50\n",
      "679/679 [==============================] - 89s 131ms/step - loss: 0.9840 - acc: 0.7212 - precision: 0.1716 - recall: 0.8402 - val_loss: 0.5191 - val_acc: 0.7348 - val_precision: 0.1033 - val_recall: 0.4945\n",
      "Epoch 48/50\n",
      "679/679 [==============================] - 89s 131ms/step - loss: 1.0298 - acc: 0.6972 - precision: 0.1598 - recall: 0.8405 - val_loss: 0.4909 - val_acc: 0.7978 - val_precision: 0.1322 - val_recall: 0.4780\n",
      "Epoch 49/50\n",
      "679/679 [==============================] - 89s 131ms/step - loss: 0.9545 - acc: 0.7265 - precision: 0.1759 - recall: 0.8510 - val_loss: 0.4467 - val_acc: 0.8047 - val_precision: 0.1166 - val_recall: 0.3852\n",
      "Epoch 50/50\n",
      "679/679 [==============================] - 89s 131ms/step - loss: 1.0779 - acc: 0.6734 - precision: 0.1490 - recall: 0.8365 - val_loss: 0.5731 - val_acc: 0.7423 - val_precision: 0.1101 - val_recall: 0.5171\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Experiment skipped due to error. Model: {'experiment_num': [299], 'valid': [True], 'num_mels': [80], 'win_len': [10], 'chunk_radius': [11], 'batch_size': [400], 'learning_rate': [0.0001], 'note_class_weight': [{0: 1, 1: 20}], 'model_version': [1], 'spec_conv_layers': [[{'filters': 64, 'kernel_size': (3, 3), 'layers': 2, 'max_pool': True, 'activation': 'relu'}, {'filters': 128, 'kernel_size': (3, 3), 'layers': 2, 'max_pool': True, 'activation': 'relu'}, {'filters': 256, 'kernel_size': (3, 3), 'layers': 2, 'max_pool': True, 'activation': 'relu'}, {'filters': 512, 'kernel_size': (3, 3), 'layers': 2, 'max_pool': True, 'activation': 'relu'}]], 'spec_dense_layers': [[]], 'combined_layers': [[{'units': 23, 'activation': 'relu', 'num_layers': 3}]]}\n",
      "Error: Negative dimension size caused by subtracting 3 from 1 for 'conv2d_389/convolution' (op: 'Conv2D') with input shapes: [?,8,1,256], [3,3,256,512].\n",
      "INFO:root:Indices are: [22400, 44800, 67200, 89200, 111200, 133200, 157200, 174400, 191600, 208800, 226000, 248800, 271600]\n",
      "INFO:root:Indices are: [14400, 28800, 43200, 57600, 77600, 101200, 124800]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "679/679 [==============================] - 20s 29ms/step - loss: 1.0814 - acc: 0.9185 - precision: 0.0819 - recall: 0.0206 - val_loss: 0.5624 - val_acc: 0.9447 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 2/50\n",
      "679/679 [==============================] - 19s 28ms/step - loss: 1.0823 - acc: 0.9339 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 0.5487 - val_acc: 0.9447 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 3/50\n",
      "679/679 [==============================] - 19s 28ms/step - loss: 1.0908 - acc: 0.9286 - precision: 0.0766 - recall: 0.0066 - val_loss: 0.5596 - val_acc: 0.9447 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 4/50\n",
      "679/679 [==============================] - 19s 28ms/step - loss: 1.0818 - acc: 0.9339 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 0.5623 - val_acc: 0.9447 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 5/50\n",
      "679/679 [==============================] - 19s 28ms/step - loss: 1.0853 - acc: 0.9339 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 0.5596 - val_acc: 0.9447 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 6/50\n",
      "679/679 [==============================] - 19s 28ms/step - loss: 1.0824 - acc: 0.9339 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 0.5476 - val_acc: 0.9447 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 ETA: 3s - l - ETA: 1s - loss: 1.0873 - acc: 0.9333 - precis\n",
      "Epoch 7/50\n",
      "679/679 [==============================] - 19s 28ms/step - loss: 1.0825 - acc: 0.9339 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 0.5720 - val_acc: 0.9447 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 8/50\n",
      "679/679 [==============================] - 20s 29ms/step - loss: 1.0826 - acc: 0.9339 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 0.5542 - val_acc: 0.9447 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 9/50\n",
      "679/679 [==============================] - 19s 28ms/step - loss: 1.0825 - acc: 0.9339 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 0.5568 - val_acc: 0.9447 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 10/50\n",
      "679/679 [==============================] - 19s 28ms/step - loss: 1.0826 - acc: 0.9339 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 0.5523 - val_acc: 0.9447 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 11/50\n",
      "679/679 [==============================] - 20s 29ms/step - loss: 1.0826 - acc: 0.9339 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 0.5728 - val_acc: 0.9447 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 12/50\n",
      "679/679 [==============================] - 19s 29ms/step - loss: 1.0826 - acc: 0.9339 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 0.5570 - val_acc: 0.9447 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 13/50\n",
      "679/679 [==============================] - 19s 29ms/step - loss: 1.0823 - acc: 0.9339 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 0.5670 - val_acc: 0.9447 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 14/50\n",
      "679/679 [==============================] - 20s 29ms/step - loss: 1.0826 - acc: 0.9339 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 0.5496 - val_acc: 0.9447 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 15/50\n",
      "679/679 [==============================] - 19s 28ms/step - loss: 1.0824 - acc: 0.9339 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 0.5384 - val_acc: 0.9447 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 16/50\n",
      "679/679 [==============================] - 20s 29ms/step - loss: 1.0825 - acc: 0.9339 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 0.5503 - val_acc: 0.9447 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 17/50\n",
      "679/679 [==============================] - 19s 28ms/step - loss: 1.0824 - acc: 0.9339 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 0.5444 - val_acc: 0.9447 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 18/50\n",
      "679/679 [==============================] - 19s 28ms/step - loss: 1.0823 - acc: 0.9339 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 0.5428 - val_acc: 0.9447 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 19/50\n",
      "679/679 [==============================] - 20s 29ms/step - loss: 1.0825 - acc: 0.9339 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 0.5347 - val_acc: 0.9447 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 20/50\n",
      "679/679 [==============================] - 20s 29ms/step - loss: 1.0824 - acc: 0.9339 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 0.5623 - val_acc: 0.9447 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 21/50\n",
      "679/679 [==============================] - 20s 30ms/step - loss: 1.0825 - acc: 0.9339 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 0.5626 - val_acc: 0.9447 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 22/50\n",
      "679/679 [==============================] - 20s 29ms/step - loss: 1.0825 - acc: 0.9339 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 0.5516 - val_acc: 0.9447 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 23/50\n",
      "679/679 [==============================] - 19s 28ms/step - loss: 1.0826 - acc: 0.9339 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 0.5509 - val_acc: 0.9447 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 24/50\n",
      "679/679 [==============================] - 19s 28ms/step - loss: 1.0825 - acc: 0.9339 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 0.5428 - val_acc: 0.9447 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 25/50\n",
      "679/679 [==============================] - 20s 29ms/step - loss: 1.0825 - acc: 0.9339 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 0.5445 - val_acc: 0.9447 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 26/50\n",
      "679/679 [==============================] - 19s 28ms/step - loss: 1.0826 - acc: 0.9339 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 0.5467 - val_acc: 0.9447 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 27/50\n",
      "679/679 [==============================] - 19s 29ms/step - loss: 1.0824 - acc: 0.9339 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 0.5500 - val_acc: 0.9447 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 28/50\n",
      "679/679 [==============================] - 19s 28ms/step - loss: 1.0825 - acc: 0.9339 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 0.5545 - val_acc: 0.9447 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 29/50\n",
      "679/679 [==============================] - 20s 29ms/step - loss: 1.0819 - acc: 0.9339 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 0.5793 - val_acc: 0.9447 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 30/50\n",
      "679/679 [==============================] - 20s 29ms/step - loss: 1.0825 - acc: 0.9339 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 0.5490 - val_acc: 0.9447 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 31/50\n",
      "679/679 [==============================] - 20s 29ms/step - loss: 1.0825 - acc: 0.9339 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 0.5536 - val_acc: 0.9447 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 32/50\n",
      "679/679 [==============================] - 19s 29ms/step - loss: 1.0824 - acc: 0.9339 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 0.5402 - val_acc: 0.9447 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 33/50\n",
      "679/679 [==============================] - 19s 29ms/step - loss: 1.0826 - acc: 0.9339 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 0.5469 - val_acc: 0.9447 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 34/50\n",
      "679/679 [==============================] - 19s 29ms/step - loss: 1.0825 - acc: 0.9339 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 0.5472 - val_acc: 0.9447 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 35/50\n",
      "679/679 [==============================] - 20s 29ms/step - loss: 1.0825 - acc: 0.9339 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 0.5587 - val_acc: 0.9447 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 36/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "679/679 [==============================] - 20s 29ms/step - loss: 1.0825 - acc: 0.9339 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 0.5503 - val_acc: 0.9447 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 37/50\n",
      "679/679 [==============================] - 20s 29ms/step - loss: 1.0825 - acc: 0.9339 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 0.5623 - val_acc: 0.9447 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 38/50\n",
      "679/679 [==============================] - 20s 29ms/step - loss: 1.0825 - acc: 0.9339 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 0.5626 - val_acc: 0.9447 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 39/50\n",
      "679/679 [==============================] - 19s 29ms/step - loss: 1.0825 - acc: 0.9339 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 0.5506 - val_acc: 0.9447 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 40/50\n",
      "679/679 [==============================] - 20s 29ms/step - loss: 1.0824 - acc: 0.9339 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 0.5462 - val_acc: 0.9447 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 41/50\n",
      "679/679 [==============================] - 19s 29ms/step - loss: 1.0823 - acc: 0.9339 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 0.5548 - val_acc: 0.9447 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 42/50\n",
      "679/679 [==============================] - 19s 28ms/step - loss: 1.0824 - acc: 0.9339 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 0.5591 - val_acc: 0.9447 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 43/50\n",
      "679/679 [==============================] - 19s 28ms/step - loss: 1.0825 - acc: 0.9339 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 0.5551 - val_acc: 0.9447 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 44/50\n",
      "679/679 [==============================] - 20s 30ms/step - loss: 1.0825 - acc: 0.9339 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 0.5474 - val_acc: 0.9447 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 45/50\n",
      "679/679 [==============================] - 20s 29ms/step - loss: 1.0825 - acc: 0.9339 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 0.5597 - val_acc: 0.9447 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 46/50\n",
      "679/679 [==============================] - 19s 28ms/step - loss: 1.0824 - acc: 0.9339 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 0.5492 - val_acc: 0.9447 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 47/50\n",
      "679/679 [==============================] - 20s 29ms/step - loss: 1.0824 - acc: 0.9339 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 0.5501 - val_acc: 0.9447 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 48/50\n",
      "679/679 [==============================] - 19s 28ms/step - loss: 1.0825 - acc: 0.9339 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 0.5551 - val_acc: 0.9447 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 49/50\n",
      "679/679 [==============================] - 19s 28ms/step - loss: 1.0825 - acc: 0.9339 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 0.5633 - val_acc: 0.9447 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 50/50\n",
      "679/679 [==============================] - 19s 29ms/step - loss: 1.0824 - acc: 0.9339 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 0.5530 - val_acc: 0.9447 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Indices are: [22400, 44800, 67200, 89200, 111200, 133200, 157200, 174400, 191600, 208800, 226000, 248800, 271600]\n",
      "INFO:root:Indices are: [14400, 28800, 43200, 57600, 77600, 101200, 124800]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "679/679 [==============================] - 91s 133ms/step - loss: 1.0881 - acc: 0.9339 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 0.5537 - val_acc: 0.9447 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 2/50\n",
      "679/679 [==============================] - 91s 134ms/step - loss: 1.0714 - acc: 0.8996 - precision: 0.1012 - recall: 0.0658 - val_loss: 0.5036 - val_acc: 0.9447 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 3/50\n",
      "679/679 [==============================] - 90s 132ms/step - loss: 1.0721 - acc: 0.9339 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 0.5433 - val_acc: 0.9447 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 4/50\n",
      "679/679 [==============================] - 90s 133ms/step - loss: 1.0688 - acc: 0.9226 - precision: 0.0764 - recall: 0.0153 - val_loss: 0.5096 - val_acc: 0.9447 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 5/50\n",
      "679/679 [==============================] - 90s 132ms/step - loss: 1.0678 - acc: 0.9248 - precision: 0.0596 - recall: 0.0093 - val_loss: 0.4988 - val_acc: 0.9447 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 6/50\n",
      "679/679 [==============================] - 89s 131ms/step - loss: 1.0664 - acc: 0.9243 - precision: 0.0724 - recall: 0.0123 - val_loss: 0.5724 - val_acc: 0.9447 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 7/50\n",
      "679/679 [==============================] - 89s 132ms/step - loss: 1.0643 - acc: 0.9339 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 0.5398 - val_acc: 0.9447 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 8/50\n",
      "679/679 [==============================] - 90s 132ms/step - loss: 1.0635 - acc: 0.9236 - precision: 0.0647 - recall: 0.0115 - val_loss: 0.5786 - val_acc: 0.9447 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 9/50\n",
      "679/679 [==============================] - 89s 132ms/step - loss: 1.0623 - acc: 0.9302 - precision: 0.0842 - recall: 0.0056 - val_loss: 0.4910 - val_acc: 0.9447 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 10/50\n",
      "679/679 [==============================] - 90s 132ms/step - loss: 1.0618 - acc: 0.9339 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 0.5298 - val_acc: 0.9447 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 11/50\n",
      "679/679 [==============================] - 89s 132ms/step - loss: 1.0612 - acc: 0.8916 - precision: 0.0894 - recall: 0.0697 - val_loss: 0.5189 - val_acc: 0.9447 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 12/50\n",
      "679/679 [==============================] - 89s 132ms/step - loss: 1.0600 - acc: 0.9339 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 0.4965 - val_acc: 0.9447 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 13/50\n",
      "679/679 [==============================] - 89s 132ms/step - loss: 1.0606 - acc: 0.9160 - precision: 0.0826 - recall: 0.0268 - val_loss: 0.5270 - val_acc: 0.9447 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 14/50\n",
      "679/679 [==============================] - 89s 132ms/step - loss: 1.0598 - acc: 0.9251 - precision: 0.0768 - recall: 0.0120 - val_loss: 0.5285 - val_acc: 0.9447 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 15/50\n",
      "679/679 [==============================] - 89s 132ms/step - loss: 1.0606 - acc: 0.9291 - precision: 0.0919 - recall: 0.0082 - val_loss: 0.5129 - val_acc: 0.9447 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 16/50\n",
      "679/679 [==============================] - 90s 132ms/step - loss: 1.0599 - acc: 0.9314 - precision: 0.0862 - recall: 0.0038 - val_loss: 0.5432 - val_acc: 0.9447 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 17/50\n",
      "679/679 [==============================] - 89s 131ms/step - loss: 1.0596 - acc: 0.9339 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 0.5157 - val_acc: 0.9447 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 18/50\n",
      "679/679 [==============================] - 89s 131ms/step - loss: 1.0601 - acc: 0.9291 - precision: 0.0919 - recall: 0.0082 - val_loss: 0.5818 - val_acc: 0.7391 - val_precision: 0.0858 - val_recall: 0.3854\n",
      "Epoch 19/50\n",
      "679/679 [==============================] - 89s 130ms/step - loss: 1.0601 - acc: 0.9039 - precision: 0.0848 - recall: 0.0463 - val_loss: 0.5250 - val_acc: 0.9447 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 20/50\n",
      "679/679 [==============================] - 89s 131ms/step - loss: 1.0596 - acc: 0.9339 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 0.5143 - val_acc: 0.9447 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 21/50\n",
      "679/679 [==============================] - 89s 131ms/step - loss: 1.0598 - acc: 0.9339 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 0.5673 - val_acc: 0.9447 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 22/50\n",
      "679/679 [==============================] - 89s 130ms/step - loss: 1.0598 - acc: 0.9339 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 0.5116 - val_acc: 0.9447 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 23/50\n",
      "679/679 [==============================] - 89s 131ms/step - loss: 1.0597 - acc: 0.9339 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 0.4994 - val_acc: 0.9447 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 24/50\n",
      "679/679 [==============================] - 89s 131ms/step - loss: 1.0600 - acc: 0.9339 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 0.5129 - val_acc: 0.9447 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 25/50\n",
      "679/679 [==============================] - 89s 131ms/step - loss: 1.0591 - acc: 0.9278 - precision: 0.0855 - recall: 0.0095 - val_loss: 0.5025 - val_acc: 0.9447 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 26/50\n",
      "679/679 [==============================] - 89s 131ms/step - loss: 1.0600 - acc: 0.9339 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 0.4985 - val_acc: 0.9447 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 27/50\n",
      "679/679 [==============================] - 89s 131ms/step - loss: 1.0591 - acc: 0.9339 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 0.5164 - val_acc: 0.9447 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 28/50\n",
      "679/679 [==============================] - 89s 131ms/step - loss: 1.0589 - acc: 0.9339 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 0.5473 - val_acc: 0.9447 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 29/50\n",
      "679/679 [==============================] - 89s 131ms/step - loss: 1.0589 - acc: 0.9240 - precision: 0.0822 - recall: 0.0147 - val_loss: 0.5138 - val_acc: 0.9447 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 30/50\n",
      "679/679 [==============================] - 89s 131ms/step - loss: 1.0590 - acc: 0.9339 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 0.5338 - val_acc: 0.9447 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 31/50\n",
      "679/679 [==============================] - 89s 131ms/step - loss: 1.0600 - acc: 0.9339 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 0.5323 - val_acc: 0.9447 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 32/50\n",
      "679/679 [==============================] - 89s 131ms/step - loss: 1.0587 - acc: 0.9339 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 0.4650 - val_acc: 0.9447 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 33/50\n",
      "679/679 [==============================] - 89s 130ms/step - loss: 1.0592 - acc: 0.9240 - precision: 0.0636 - recall: 0.0109 - val_loss: 0.5169 - val_acc: 0.9447 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 34/50\n",
      "679/679 [==============================] - 89s 131ms/step - loss: 1.0583 - acc: 0.9339 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 0.5317 - val_acc: 0.9447 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 35/50\n",
      "679/679 [==============================] - 88s 130ms/step - loss: 1.0596 - acc: 0.9339 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 0.5231 - val_acc: 0.9447 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 36/50\n",
      "679/679 [==============================] - 89s 130ms/step - loss: 1.0584 - acc: 0.9339 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 0.5092 - val_acc: 0.9447 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37/50\n",
      "679/679 [==============================] - 89s 131ms/step - loss: 1.0582 - acc: 0.9339 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 0.5178 - val_acc: 0.9447 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 38/50\n",
      "679/679 [==============================] - 89s 130ms/step - loss: 1.0592 - acc: 0.9339 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 0.5224 - val_acc: 0.9447 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 39/50\n",
      "679/679 [==============================] - 89s 130ms/step - loss: 1.0590 - acc: 0.9339 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 0.5110 - val_acc: 0.9447 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 40/50\n",
      "679/679 [==============================] - 89s 130ms/step - loss: 1.0587 - acc: 0.9339 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 0.5725 - val_acc: 0.9447 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 41/50\n",
      "679/679 [==============================] - 89s 131ms/step - loss: 1.0592 - acc: 0.9147 - precision: 0.0840 - recall: 0.0293 - val_loss: 0.5118 - val_acc: 0.9447 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 42/50\n",
      "679/679 [==============================] - 89s 130ms/step - loss: 1.0590 - acc: 0.9339 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 0.5502 - val_acc: 0.9447 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 43/50\n",
      "679/679 [==============================] - 89s 131ms/step - loss: 1.0593 - acc: 0.9339 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 0.5327 - val_acc: 0.9447 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 44/50\n",
      "679/679 [==============================] - 89s 131ms/step - loss: 1.0590 - acc: 0.9264 - precision: 0.0746 - recall: 0.0100 - val_loss: 0.5509 - val_acc: 0.9447 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 45/50\n",
      "679/679 [==============================] - 89s 131ms/step - loss: 1.0587 - acc: 0.9253 - precision: 0.0821 - recall: 0.0128 - val_loss: 0.5425 - val_acc: 0.9447 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 46/50\n",
      "679/679 [==============================] - 89s 131ms/step - loss: 1.0583 - acc: 0.9339 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 0.5139 - val_acc: 0.9447 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 47/50\n",
      "679/679 [==============================] - 89s 131ms/step - loss: 1.0582 - acc: 0.9192 - precision: 0.0769 - recall: 0.0202 - val_loss: 0.5286 - val_acc: 0.9447 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 48/50\n",
      "679/679 [==============================] - 89s 131ms/step - loss: 1.0593 - acc: 0.9339 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 0.5124 - val_acc: 0.9447 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 49/50\n",
      "679/679 [==============================] - 89s 131ms/step - loss: 1.0587 - acc: 0.9339 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 0.5344 - val_acc: 0.9447 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 50/50\n",
      "679/679 [==============================] - 89s 131ms/step - loss: 1.0586 - acc: 0.9339 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 0.5228 - val_acc: 0.9447 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Experiment skipped due to error. Model: {'experiment_num': [302], 'valid': [True], 'num_mels': [80], 'win_len': [10], 'chunk_radius': [11], 'batch_size': [400], 'learning_rate': [0.001], 'note_class_weight': [{0: 1, 1: 10}], 'model_version': [1], 'spec_conv_layers': [[{'filters': 64, 'kernel_size': (3, 3), 'layers': 2, 'max_pool': True, 'activation': 'relu'}, {'filters': 128, 'kernel_size': (3, 3), 'layers': 2, 'max_pool': True, 'activation': 'relu'}, {'filters': 256, 'kernel_size': (3, 3), 'layers': 2, 'max_pool': True, 'activation': 'relu'}, {'filters': 512, 'kernel_size': (3, 3), 'layers': 2, 'max_pool': True, 'activation': 'relu'}]], 'spec_dense_layers': [[]], 'combined_layers': [[{'units': 23, 'activation': 'relu', 'num_layers': 3}]]}\n",
      "Error: Negative dimension size caused by subtracting 3 from 1 for 'conv2d_410/convolution' (op: 'Conv2D') with input shapes: [?,8,1,256], [3,3,256,512].\n",
      "INFO:root:Indices are: [22400, 44800, 67200, 89200, 111200, 133200, 157200, 174400, 191600, 208800, 226000, 248800, 271600]\n",
      "INFO:root:Indices are: [14400, 28800, 43200, 57600, 77600, 101200, 124800]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "679/679 [==============================] - 20s 29ms/step - loss: 1.3320 - acc: 0.2601 - precision: 0.0693 - recall: 0.8183 - val_loss: 0.7246 - val_acc: 0.2180 - val_precision: 0.0612 - val_recall: 0.9171297 - acc: 0.2601 - precision: 0.0689 - \n",
      "Epoch 2/50\n",
      "679/679 [==============================] - 20s 29ms/step - loss: 1.3191 - acc: 0.3517 - precision: 0.0738 - recall: 0.7621 - val_loss: 0.5948 - val_acc: 0.7368 - val_precision: 0.0848 - val_recall: 0.3841\n",
      "Epoch 3/50\n",
      "679/679 [==============================] - 20s 29ms/step - loss: 1.3120 - acc: 0.3771 - precision: 0.0761 - recall: 0.7562 - val_loss: 0.6379 - val_acc: 0.4973 - val_precision: 0.0717 - val_recall: 0.6772\n",
      "Epoch 4/50\n",
      "679/679 [==============================] - 20s 29ms/step - loss: 1.3097 - acc: 0.3660 - precision: 0.0760 - recall: 0.7700 - val_loss: 0.6950 - val_acc: 0.4917 - val_precision: 0.0714 - val_recall: 0.6829\n",
      "Epoch 5/50\n",
      "679/679 [==============================] - 19s 28ms/step - loss: 1.3082 - acc: 0.3643 - precision: 0.0755 - recall: 0.7665 - val_loss: 0.7253 - val_acc: 0.4742 - val_precision: 0.0700 - val_recall: 0.6933\n",
      "Epoch 6/50\n",
      "679/679 [==============================] - 19s 28ms/step - loss: 1.3074 - acc: 0.3875 - precision: 0.0767 - recall: 0.7486 - val_loss: 0.7830 - val_acc: 0.2186 - val_precision: 0.0617 - val_recall: 0.9249\n",
      "Epoch 7/50\n",
      "679/679 [==============================] - 19s 28ms/step - loss: 1.3066 - acc: 0.3882 - precision: 0.0766 - recall: 0.7473 - val_loss: 0.6977 - val_acc: 0.4814 - val_precision: 0.0708 - val_recall: 0.6911\n",
      "Epoch 8/50\n",
      "679/679 [==============================] - 19s 28ms/step - loss: 1.3058 - acc: 0.3735 - precision: 0.0764 - recall: 0.7649 - val_loss: 0.6857 - val_acc: 0.4849 - val_precision: 0.0710 - val_recall: 0.6889\n",
      "Epoch 9/50\n",
      "679/679 [==============================] - 19s 28ms/step - loss: 1.3057 - acc: 0.3854 - precision: 0.0767 - recall: 0.7522 - val_loss: 0.7123 - val_acc: 0.4787 - val_precision: 0.0705 - val_recall: 0.6924\n",
      "Epoch 10/50\n",
      "679/679 [==============================] - 19s 28ms/step - loss: 1.3064 - acc: 0.3598 - precision: 0.0762 - recall: 0.7804 - val_loss: 0.6831 - val_acc: 0.4818 - val_precision: 0.0708 - val_recall: 0.6911\n",
      "Epoch 11/50\n",
      "679/679 [==============================] - 19s 28ms/step - loss: 1.3059 - acc: 0.3984 - precision: 0.0767 - recall: 0.7341 - val_loss: 0.7204 - val_acc: 0.4797 - val_precision: 0.0706 - val_recall: 0.6916\n",
      "Epoch 12/50\n",
      "679/679 [==============================] - 19s 28ms/step - loss: 1.3064 - acc: 0.3788 - precision: 0.0763 - recall: 0.7559 - val_loss: 0.7400 - val_acc: 0.2253 - val_precision: 0.0620 - val_recall: 0.9214\n",
      "Epoch 13/50\n",
      "679/679 [==============================] - 19s 28ms/step - loss: 1.3050 - acc: 0.3942 - precision: 0.0769 - recall: 0.7415 - val_loss: 0.6357 - val_acc: 0.7334 - val_precision: 0.0846 - val_recall: 0.3894\n",
      "Epoch 14/50\n",
      "679/679 [==============================] - 19s 28ms/step - loss: 1.3057 - acc: 0.3928 - precision: 0.0766 - recall: 0.7403 - val_loss: 0.6885 - val_acc: 0.4826 - val_precision: 0.0707 - val_recall: 0.6889\n",
      "Epoch 15/50\n",
      "679/679 [==============================] - 19s 28ms/step - loss: 1.3053 - acc: 0.3774 - precision: 0.0766 - recall: 0.7614 - val_loss: 0.6720 - val_acc: 0.4892 - val_precision: 0.0713 - val_recall: 0.6854\n",
      "Epoch 16/50\n",
      "679/679 [==============================] - 19s 28ms/step - loss: 1.3055 - acc: 0.3742 - precision: 0.0766 - recall: 0.7655 - val_loss: 0.7648 - val_acc: 0.2177 - val_precision: 0.0616 - val_recall: 0.9249\n",
      "Epoch 17/50\n",
      "679/679 [==============================] - 19s 28ms/step - loss: 1.3044 - acc: 0.3966 - precision: 0.0769 - recall: 0.7383 - val_loss: 0.6659 - val_acc: 0.4915 - val_precision: 0.0714 - val_recall: 0.6834\n",
      "Epoch 18/50\n",
      "679/679 [==============================] - 19s 28ms/step - loss: 1.3046 - acc: 0.3832 - precision: 0.0767 - recall: 0.7547 - val_loss: 0.6822 - val_acc: 0.4850 - val_precision: 0.0710 - val_recall: 0.6885\n",
      "Epoch 19/50\n",
      "679/679 [==============================] - 19s 28ms/step - loss: 1.3037 - acc: 0.3709 - precision: 0.0761 - recall: 0.7641 - val_loss: 0.6630 - val_acc: 0.4853 - val_precision: 0.0710 - val_recall: 0.6876\n",
      "Epoch 20/50\n",
      "679/679 [==============================] - 19s 28ms/step - loss: 1.3045 - acc: 0.3614 - precision: 0.0759 - recall: 0.7756 - val_loss: 0.7271 - val_acc: 0.4767 - val_precision: 0.0703 - val_recall: 0.6929\n",
      "Epoch 21/50\n",
      "679/679 [==============================] - 19s 28ms/step - loss: 1.3047 - acc: 0.3866 - precision: 0.0767 - recall: 0.7500 - val_loss: 0.6655 - val_acc: 0.4866 - val_precision: 0.0710 - val_recall: 0.6863.304\n",
      "Epoch 22/50\n",
      "679/679 [==============================] - 19s 28ms/step - loss: 1.3050 - acc: 0.3871 - precision: 0.0769 - recall: 0.7517 - val_loss: 0.6908 - val_acc: 0.4821 - val_precision: 0.0707 - val_recall: 0.6894\n",
      "Epoch 23/50\n",
      "679/679 [==============================] - 19s 28ms/step - loss: 1.3046 - acc: 0.3809 - precision: 0.0764 - recall: 0.7542 - val_loss: 0.6674 - val_acc: 0.4940 - val_precision: 0.0716 - val_recall: 0.6812\n",
      "Epoch 24/50\n",
      "679/679 [==============================] - 19s 28ms/step - loss: 1.3040 - acc: 0.3761 - precision: 0.0765 - recall: 0.7627 - val_loss: 0.6565 - val_acc: 0.4903 - val_precision: 0.0714 - val_recall: 0.6847\n",
      "Epoch 25/50\n",
      "679/679 [==============================] - 20s 29ms/step - loss: 1.3037 - acc: 0.3759 - precision: 0.0766 - recall: 0.7631 - val_loss: 0.6837 - val_acc: 0.4889 - val_precision: 0.0712 - val_recall: 0.6851\n",
      "Epoch 26/50\n",
      "679/679 [==============================] - 20s 29ms/step - loss: 1.3054 - acc: 0.3744 - precision: 0.0766 - recall: 0.7652 - val_loss: 0.6788 - val_acc: 0.4838 - val_precision: 0.0709 - val_recall: 0.6885\n",
      "Epoch 27/50\n",
      "679/679 [==============================] - 20s 29ms/step - loss: 1.3044 - acc: 0.3768 - precision: 0.0766 - recall: 0.7627 - val_loss: 0.7043 - val_acc: 0.4791 - val_precision: 0.0705 - val_recall: 0.6916\n",
      "Epoch 28/50\n",
      "679/679 [==============================] - 20s 29ms/step - loss: 1.3029 - acc: 0.3719 - precision: 0.0764 - recall: 0.7671 - val_loss: 0.6471 - val_acc: 0.4935 - val_precision: 0.0715 - val_recall: 0.6812\n",
      "Epoch 29/50\n",
      "679/679 [==============================] - 20s 29ms/step - loss: 1.3045 - acc: 0.3885 - precision: 0.0767 - recall: 0.7474 - val_loss: 0.6564 - val_acc: 0.4890 - val_precision: 0.0712 - val_recall: 0.6845\n",
      "Epoch 30/50\n",
      "679/679 [==============================] - 20s 29ms/step - loss: 1.3045 - acc: 0.3673 - precision: 0.0762 - recall: 0.7713 - val_loss: 0.6892 - val_acc: 0.4838 - val_precision: 0.0709 - val_recall: 0.6885\n",
      "Epoch 31/50\n",
      "679/679 [==============================] - 19s 29ms/step - loss: 1.3045 - acc: 0.3606 - precision: 0.0761 - recall: 0.7785 - val_loss: 0.6884 - val_acc: 0.4786 - val_precision: 0.0704 - val_recall: 0.6911\n",
      "Epoch 32/50\n",
      "679/679 [==============================] - 19s 28ms/step - loss: 1.3032 - acc: 0.3748 - precision: 0.0767 - recall: 0.7661 - val_loss: 0.6993 - val_acc: 0.4802 - val_precision: 0.0706 - val_recall: 0.6907\n",
      "Epoch 33/50\n",
      "679/679 [==============================] - 19s 28ms/step - loss: 1.3042 - acc: 0.3809 - precision: 0.0767 - recall: 0.7576 - val_loss: 0.7019 - val_acc: 0.4765 - val_precision: 0.0703 - val_recall: 0.6933\n",
      "Epoch 34/50\n",
      "679/679 [==============================] - 19s 29ms/step - loss: 1.3024 - acc: 0.3777 - precision: 0.0766 - recall: 0.7606 - val_loss: 0.7013 - val_acc: 0.4898 - val_precision: 0.0713 - val_recall: 0.6842\n",
      "Epoch 35/50\n",
      "679/679 [==============================] - 19s 28ms/step - loss: 1.3042 - acc: 0.3705 - precision: 0.0764 - recall: 0.7693 - val_loss: 0.6580 - val_acc: 0.4918 - val_precision: 0.0714 - val_recall: 0.6829\n",
      "Epoch 36/50\n",
      "679/679 [==============================] - 19s 28ms/step - loss: 1.3027 - acc: 0.3759 - precision: 0.0766 - recall: 0.7640 - val_loss: 0.6832 - val_acc: 0.4797 - val_precision: 0.0705 - val_recall: 0.6911\n",
      "Epoch 37/50\n",
      "679/679 [==============================] - 19s 28ms/step - loss: 1.3038 - acc: 0.3776 - precision: 0.0767 - recall: 0.7629 - val_loss: 0.7146 - val_acc: 0.4769 - val_precision: 0.0703 - val_recall: 0.6924\n",
      "Epoch 38/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "679/679 [==============================] - 19s 28ms/step - loss: 1.3040 - acc: 0.3727 - precision: 0.0765 - recall: 0.7669 - val_loss: 0.6832 - val_acc: 0.4856 - val_precision: 0.0710 - val_recall: 0.6876\n",
      "Epoch 39/50\n",
      "679/679 [==============================] - 20s 29ms/step - loss: 1.3038 - acc: 0.3594 - precision: 0.0760 - recall: 0.7793 - val_loss: 0.6564 - val_acc: 0.4879 - val_precision: 0.0711 - val_recall: 0.6854\n",
      "Epoch 40/50\n",
      "679/679 [==============================] - 19s 28ms/step - loss: 1.3032 - acc: 0.3740 - precision: 0.0766 - recall: 0.7656 - val_loss: 0.6904 - val_acc: 0.4884 - val_precision: 0.0712 - val_recall: 0.6853\n",
      "Epoch 41/50\n",
      "679/679 [==============================] - 19s 28ms/step - loss: 1.3038 - acc: 0.3838 - precision: 0.0768 - recall: 0.7547 - val_loss: 0.6890 - val_acc: 0.4791 - val_precision: 0.0705 - val_recall: 0.6911\n",
      "Epoch 42/50\n",
      "679/679 [==============================] - 19s 28ms/step - loss: 1.3039 - acc: 0.3709 - precision: 0.0764 - recall: 0.7686 - val_loss: 0.6970 - val_acc: 0.4839 - val_precision: 0.0709 - val_recall: 0.6885\n",
      "Epoch 43/50\n",
      "679/679 [==============================] - 19s 28ms/step - loss: 1.3048 - acc: 0.3733 - precision: 0.0764 - recall: 0.7644 - val_loss: 0.7189 - val_acc: 0.4773 - val_precision: 0.0704 - val_recall: 0.6929\n",
      "Epoch 44/50\n",
      "679/679 [==============================] - 19s 28ms/step - loss: 1.3020 - acc: 0.3894 - precision: 0.0770 - recall: 0.7494 - val_loss: 0.7004 - val_acc: 0.4804 - val_precision: 0.0706 - val_recall: 0.6907\n",
      "Epoch 45/50\n",
      "679/679 [==============================] - 19s 28ms/step - loss: 1.3034 - acc: 0.3793 - precision: 0.0767 - recall: 0.7599 - val_loss: 0.6805 - val_acc: 0.4844 - val_precision: 0.0709 - val_recall: 0.6883\n",
      "Epoch 46/50\n",
      "679/679 [==============================] - 19s 28ms/step - loss: 1.3035 - acc: 0.3797 - precision: 0.0769 - recall: 0.7615 - val_loss: 0.7237 - val_acc: 0.4756 - val_precision: 0.0702 - val_recall: 0.6937\n",
      "Epoch 47/50\n",
      "679/679 [==============================] - 19s 28ms/step - loss: 1.3031 - acc: 0.3920 - precision: 0.0770 - recall: 0.7465 - val_loss: 0.6325 - val_acc: 0.7361 - val_precision: 0.0852 - val_recall: 0.3876\n",
      "Epoch 48/50\n",
      "679/679 [==============================] - 19s 28ms/step - loss: 1.3036 - acc: 0.3815 - precision: 0.0766 - recall: 0.7554 - val_loss: 0.6728 - val_acc: 0.4865 - val_precision: 0.0711 - val_recall: 0.6872\n",
      "Epoch 49/50\n",
      "679/679 [==============================] - 19s 28ms/step - loss: 1.3029 - acc: 0.3705 - precision: 0.0764 - recall: 0.7682 - val_loss: 0.6987 - val_acc: 0.4829 - val_precision: 0.0708 - val_recall: 0.6889c: 0.3692 - precision: 0.075 - ETA: 1s - loss: 1.3019 - acc: 0.3695 \n",
      "Epoch 50/50\n",
      "679/679 [==============================] - 19s 29ms/step - loss: 1.3025 - acc: 0.3632 - precision: 0.0764 - recall: 0.7785 - val_loss: 0.6958 - val_acc: 0.4793 - val_precision: 0.0705 - val_recall: 0.6911\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Indices are: [22400, 44800, 67200, 89200, 111200, 133200, 157200, 174400, 191600, 208800, 226000, 248800, 271600]\n",
      "INFO:root:Indices are: [14400, 28800, 43200, 57600, 77600, 101200, 124800]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "679/679 [==============================] - 90s 133ms/step - loss: 1.3321 - acc: 0.2448 - precision: 0.0685 - recall: 0.8257 - val_loss: 0.7398 - val_acc: 0.1233 - val_precision: 0.0579 - val_recall: 0.9734\n",
      "Epoch 2/50\n",
      "679/679 [==============================] - 89s 131ms/step - loss: 1.3191 - acc: 0.3270 - precision: 0.0734 - recall: 0.7894 - val_loss: 0.6894 - val_acc: 0.4755 - val_precision: 0.0698 - val_recall: 0.6885\n",
      "Epoch 3/50\n",
      "679/679 [==============================] - 89s 131ms/step - loss: 1.3125 - acc: 0.3770 - precision: 0.0753 - recall: 0.7469 - val_loss: 0.7125 - val_acc: 0.4816 - val_precision: 0.0705 - val_recall: 0.6876\n",
      "Epoch 4/50\n",
      "679/679 [==============================] - 89s 131ms/step - loss: 1.3077 - acc: 0.3895 - precision: 0.0764 - recall: 0.7426 - val_loss: 0.7246 - val_acc: 0.4867 - val_precision: 0.0708 - val_recall: 0.6834\n",
      "Epoch 5/50\n",
      "679/679 [==============================] - 89s 131ms/step - loss: 1.3097 - acc: 0.3552 - precision: 0.0755 - recall: 0.7791 - val_loss: 0.6919 - val_acc: 0.4746 - val_precision: 0.0700 - val_recall: 0.6929\n",
      "Epoch 6/50\n",
      "679/679 [==============================] - 89s 131ms/step - loss: 1.3073 - acc: 0.3856 - precision: 0.0761 - recall: 0.7444 - val_loss: 0.7057 - val_acc: 0.4711 - val_precision: 0.0697 - val_recall: 0.6937\n",
      "Epoch 7/50\n",
      "679/679 [==============================] - 89s 131ms/step - loss: 1.3067 - acc: 0.3747 - precision: 0.0765 - recall: 0.7645 - val_loss: 0.6392 - val_acc: 0.4918 - val_precision: 0.0713 - val_recall: 0.6816\n",
      "Epoch 8/50\n",
      "679/679 [==============================] - 89s 131ms/step - loss: 1.3072 - acc: 0.4023 - precision: 0.0768 - recall: 0.7303 - val_loss: 0.7300 - val_acc: 0.4813 - val_precision: 0.0704 - val_recall: 0.6876\n",
      "Epoch 9/50\n",
      "679/679 [==============================] - 89s 131ms/step - loss: 1.3064 - acc: 0.3840 - precision: 0.0766 - recall: 0.7525 - val_loss: 0.6945 - val_acc: 0.4815 - val_precision: 0.0706 - val_recall: 0.6894\n",
      "Epoch 10/50\n",
      "679/679 [==============================] - 89s 131ms/step - loss: 1.3075 - acc: 0.3771 - precision: 0.0762 - recall: 0.7572 - val_loss: 0.6546 - val_acc: 0.4826 - val_precision: 0.0707 - val_recall: 0.6880\n",
      "Epoch 11/50\n",
      "679/679 [==============================] - 89s 131ms/step - loss: 1.3067 - acc: 0.3755 - precision: 0.0761 - recall: 0.7579 - val_loss: 0.6798 - val_acc: 0.4927 - val_precision: 0.0715 - val_recall: 0.6825\n",
      "Epoch 12/50\n",
      "679/679 [==============================] - 89s 131ms/step - loss: 1.3042 - acc: 0.3831 - precision: 0.0766 - recall: 0.7534 - val_loss: 0.7257 - val_acc: 0.4692 - val_precision: 0.0695 - val_recall: 0.6943\n",
      "Epoch 13/50\n",
      "679/679 [==============================] - 89s 131ms/step - loss: 1.3038 - acc: 0.3684 - precision: 0.0761 - recall: 0.7685 - val_loss: 0.6597 - val_acc: 0.4850 - val_precision: 0.0710 - val_recall: 0.6885\n",
      "Epoch 14/50\n",
      "679/679 [==============================] - 89s 131ms/step - loss: 1.3058 - acc: 0.3652 - precision: 0.0761 - recall: 0.7717 - val_loss: 0.6734 - val_acc: 0.4797 - val_precision: 0.0706 - val_recall: 0.6916\n",
      "Epoch 15/50\n",
      "679/679 [==============================] - 89s 131ms/step - loss: 1.3044 - acc: 0.3775 - precision: 0.0764 - recall: 0.7586 - val_loss: 0.6582 - val_acc: 0.4930 - val_precision: 0.0715 - val_recall: 0.6820\n",
      "Epoch 16/50\n",
      "679/679 [==============================] - 89s 131ms/step - loss: 1.3062 - acc: 0.3712 - precision: 0.0763 - recall: 0.7665 - val_loss: 0.6569 - val_acc: 0.4873 - val_precision: 0.0711 - val_recall: 0.6863\n",
      "Epoch 17/50\n",
      "679/679 [==============================] - 89s 131ms/step - loss: 1.3063 - acc: 0.3837 - precision: 0.0765 - recall: 0.7517 - val_loss: 0.6541 - val_acc: 0.4932 - val_precision: 0.0715 - val_recall: 0.6822\n",
      "Epoch 18/50\n",
      "679/679 [==============================] - 89s 131ms/step - loss: 1.3055 - acc: 0.3825 - precision: 0.0766 - recall: 0.7543 - val_loss: 0.6370 - val_acc: 0.4911 - val_precision: 0.0712 - val_recall: 0.6819\n",
      "Epoch 19/50\n",
      "679/679 [==============================] - 89s 131ms/step - loss: 1.3048 - acc: 0.4015 - precision: 0.0770 - recall: 0.7334 - val_loss: 0.6663 - val_acc: 0.4925 - val_precision: 0.0715 - val_recall: 0.6829\n",
      "Epoch 20/50\n",
      "679/679 [==============================] - 89s 131ms/step - loss: 1.3035 - acc: 0.3677 - precision: 0.0765 - recall: 0.7732 - val_loss: 0.6133 - val_acc: 0.4983 - val_precision: 0.0718 - val_recall: 0.6776\n",
      "Epoch 21/50\n",
      "679/679 [==============================] - 89s 132ms/step - loss: 1.3056 - acc: 0.3751 - precision: 0.0763 - recall: 0.7612 - val_loss: 0.7058 - val_acc: 0.4768 - val_precision: 0.0703 - val_recall: 0.6924\n",
      "Epoch 22/50\n",
      "679/679 [==============================] - 89s 131ms/step - loss: 1.3034 - acc: 0.3907 - precision: 0.0768 - recall: 0.7460 - val_loss: 0.6725 - val_acc: 0.4930 - val_precision: 0.0715 - val_recall: 0.6825\n",
      "Epoch 23/50\n",
      "679/679 [==============================] - 89s 131ms/step - loss: 1.3061 - acc: 0.3769 - precision: 0.0764 - recall: 0.7597 - val_loss: 0.7378 - val_acc: 0.4696 - val_precision: 0.0695 - val_recall: 0.6942\n",
      "Epoch 24/50\n",
      "679/679 [==============================] - 89s 131ms/step - loss: 1.3043 - acc: 0.3872 - precision: 0.0765 - recall: 0.7471 - val_loss: 0.7007 - val_acc: 0.4902 - val_precision: 0.0714 - val_recall: 0.6847\n",
      "Epoch 25/50\n",
      "679/679 [==============================] - 89s 131ms/step - loss: 1.3052 - acc: 0.3768 - precision: 0.0763 - recall: 0.7592 - val_loss: 0.6537 - val_acc: 0.4864 - val_precision: 0.0709 - val_recall: 0.6854\n",
      "Epoch 26/50\n",
      "679/679 [==============================] - 89s 131ms/step - loss: 1.3042 - acc: 0.3767 - precision: 0.0767 - recall: 0.7642 - val_loss: 0.6716 - val_acc: 0.4947 - val_precision: 0.0717 - val_recall: 0.6812\n",
      "Epoch 27/50\n",
      "679/679 [==============================] - 89s 131ms/step - loss: 1.3034 - acc: 0.3677 - precision: 0.0764 - recall: 0.7723 - val_loss: 0.6756 - val_acc: 0.4875 - val_precision: 0.0711 - val_recall: 0.6863\n",
      "Epoch 28/50\n",
      "679/679 [==============================] - 89s 131ms/step - loss: 1.3038 - acc: 0.3786 - precision: 0.0765 - recall: 0.7582 - val_loss: 0.6833 - val_acc: 0.4873 - val_precision: 0.0712 - val_recall: 0.6870\n",
      "Epoch 29/50\n",
      "679/679 [==============================] - 89s 131ms/step - loss: 1.3051 - acc: 0.3789 - precision: 0.0762 - recall: 0.7550 - val_loss: 0.6987 - val_acc: 0.4835 - val_precision: 0.0708 - val_recall: 0.6889\n",
      "Epoch 30/50\n",
      "679/679 [==============================] - 89s 131ms/step - loss: 1.3041 - acc: 0.3995 - precision: 0.0771 - recall: 0.7373 - val_loss: 0.6975 - val_acc: 0.4812 - val_precision: 0.0707 - val_recall: 0.6902\n",
      "Epoch 31/50\n",
      "679/679 [==============================] - 89s 131ms/step - loss: 1.3045 - acc: 0.3819 - precision: 0.0768 - recall: 0.7575 - val_loss: 0.6941 - val_acc: 0.4793 - val_precision: 0.0705 - val_recall: 0.6916\n",
      "Epoch 32/50\n",
      "679/679 [==============================] - 89s 131ms/step - loss: 1.3033 - acc: 0.3809 - precision: 0.0766 - recall: 0.7563 - val_loss: 0.7233 - val_acc: 0.4847 - val_precision: 0.0709 - val_recall: 0.6880\n",
      "Epoch 33/50\n",
      "679/679 [==============================] - 89s 131ms/step - loss: 1.3040 - acc: 0.3780 - precision: 0.0762 - recall: 0.7566 - val_loss: 0.7048 - val_acc: 0.4847 - val_precision: 0.0709 - val_recall: 0.6880\n",
      "Epoch 34/50\n",
      "679/679 [==============================] - 89s 132ms/step - loss: 1.3047 - acc: 0.3797 - precision: 0.0766 - recall: 0.7584 - val_loss: 0.7058 - val_acc: 0.4832 - val_precision: 0.0708 - val_recall: 0.6889\n",
      "Epoch 35/50\n",
      "679/679 [==============================] - 89s 131ms/step - loss: 1.3036 - acc: 0.3756 - precision: 0.0766 - recall: 0.7639 - val_loss: 0.6696 - val_acc: 0.4842 - val_precision: 0.0708 - val_recall: 0.6876\n",
      "Epoch 36/50\n",
      "679/679 [==============================] - 89s 132ms/step - loss: 1.3041 - acc: 0.3740 - precision: 0.0765 - recall: 0.7650 - val_loss: 0.7286 - val_acc: 0.2275 - val_precision: 0.0619 - val_recall: 0.9174\n",
      "Epoch 37/50\n",
      "679/679 [==============================] - 89s 131ms/step - loss: 1.3034 - acc: 0.3883 - precision: 0.0767 - recall: 0.7477 - val_loss: 0.6952 - val_acc: 0.4784 - val_precision: 0.0704 - val_recall: 0.6917\n",
      "Epoch 38/50\n",
      "679/679 [==============================] - 89s 131ms/step - loss: 1.3037 - acc: 0.3784 - precision: 0.0766 - recall: 0.7607 - val_loss: 0.7109 - val_acc: 0.4771 - val_precision: 0.0703 - val_recall: 0.6924\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39/50\n",
      "679/679 [==============================] - 89s 131ms/step - loss: 1.3035 - acc: 0.3636 - precision: 0.0759 - recall: 0.7724 - val_loss: 0.7446 - val_acc: 0.2192 - val_precision: 0.0617 - val_recall: 0.9237\n",
      "Epoch 40/50\n",
      "679/679 [==============================] - 89s 132ms/step - loss: 1.3042 - acc: 0.3798 - precision: 0.0767 - recall: 0.7597 - val_loss: 0.6864 - val_acc: 0.4839 - val_precision: 0.0709 - val_recall: 0.6889\n",
      "Epoch 41/50\n",
      "679/679 [==============================] - 89s 131ms/step - loss: 1.3039 - acc: 0.3794 - precision: 0.0767 - recall: 0.7605 - val_loss: 0.7054 - val_acc: 0.4799 - val_precision: 0.0706 - val_recall: 0.6911\n",
      "Epoch 42/50\n",
      "679/679 [==============================] - 89s 131ms/step - loss: 1.3031 - acc: 0.3887 - precision: 0.0768 - recall: 0.7483 - val_loss: 0.6776 - val_acc: 0.4889 - val_precision: 0.0713 - val_recall: 0.6857\n",
      "Epoch 43/50\n",
      "679/679 [==============================] - 89s 131ms/step - loss: 1.3030 - acc: 0.3796 - precision: 0.0766 - recall: 0.7589 - val_loss: 0.6577 - val_acc: 0.4885 - val_precision: 0.0712 - val_recall: 0.6850\n",
      "Epoch 44/50\n",
      "679/679 [==============================] - 89s 131ms/step - loss: 1.3038 - acc: 0.3792 - precision: 0.0765 - recall: 0.7586 - val_loss: 0.6769 - val_acc: 0.4870 - val_precision: 0.0712 - val_recall: 0.6875\n",
      "Epoch 45/50\n",
      "679/679 [==============================] - 89s 131ms/step - loss: 1.3038 - acc: 0.3764 - precision: 0.0766 - recall: 0.7626 - val_loss: 0.7068 - val_acc: 0.4832 - val_precision: 0.0709 - val_recall: 0.6898\n",
      "Epoch 46/50\n",
      "679/679 [==============================] - 89s 131ms/step - loss: 1.3036 - acc: 0.3777 - precision: 0.0766 - recall: 0.7611 - val_loss: 0.6830 - val_acc: 0.4838 - val_precision: 0.0709 - val_recall: 0.6889\n",
      "Epoch 47/50\n",
      "679/679 [==============================] - 89s 131ms/step - loss: 1.3030 - acc: 0.3630 - precision: 0.0763 - recall: 0.7771 - val_loss: 0.6900 - val_acc: 0.4935 - val_precision: 0.0716 - val_recall: 0.6820\n",
      "Epoch 48/50\n",
      "679/679 [==============================] - 89s 132ms/step - loss: 1.3040 - acc: 0.3907 - precision: 0.0767 - recall: 0.7441 - val_loss: 0.6243 - val_acc: 0.7388 - val_precision: 0.0857 - val_recall: 0.3854\n",
      "Epoch 49/50\n",
      "679/679 [==============================] - 89s 131ms/step - loss: 1.3031 - acc: 0.3973 - precision: 0.0772 - recall: 0.7411 - val_loss: 0.7230 - val_acc: 0.4783 - val_precision: 0.0703 - val_recall: 0.6907\n",
      "Epoch 50/50\n",
      "679/679 [==============================] - 89s 131ms/step - loss: 1.3030 - acc: 0.3764 - precision: 0.0765 - recall: 0.7618 - val_loss: 0.6897 - val_acc: 0.4852 - val_precision: 0.0711 - val_recall: 0.6889\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Experiment skipped due to error. Model: {'experiment_num': [305], 'valid': [True], 'num_mels': [80], 'win_len': [10], 'chunk_radius': [11], 'batch_size': [400], 'learning_rate': [0.001], 'note_class_weight': [{0: 1, 1: 15}], 'model_version': [1], 'spec_conv_layers': [[{'filters': 64, 'kernel_size': (3, 3), 'layers': 2, 'max_pool': True, 'activation': 'relu'}, {'filters': 128, 'kernel_size': (3, 3), 'layers': 2, 'max_pool': True, 'activation': 'relu'}, {'filters': 256, 'kernel_size': (3, 3), 'layers': 2, 'max_pool': True, 'activation': 'relu'}, {'filters': 512, 'kernel_size': (3, 3), 'layers': 2, 'max_pool': True, 'activation': 'relu'}]], 'spec_dense_layers': [[]], 'combined_layers': [[{'units': 23, 'activation': 'relu', 'num_layers': 3}]]}\n",
      "Error: Negative dimension size caused by subtracting 3 from 1 for 'conv2d_431/convolution' (op: 'Conv2D') with input shapes: [?,8,1,256], [3,3,256,512].\n",
      "INFO:root:Indices are: [22400, 44800, 67200, 89200, 111200, 133200, 157200, 174400, 191600, 208800, 226000, 248800, 271600]\n",
      "INFO:root:Indices are: [14400, 28800, 43200, 57600, 77600, 101200, 124800]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "679/679 [==============================] - 21s 32ms/step - loss: 1.5200 - acc: 0.1123 - precision: 0.0677 - recall: 0.9638 - val_loss: 0.8025 - val_acc: 0.0553 - val_precision: 0.0553 - val_recall: 1.0000\n",
      "Epoch 2/50\n",
      "679/679 [==============================] - 20s 30ms/step - loss: 1.5234 - acc: 0.0820 - precision: 0.0671 - recall: 0.9906 - val_loss: 0.9571 - val_acc: 0.0553 - val_precision: 0.0553 - val_recall: 1.0000\n",
      "Epoch 3/50\n",
      "679/679 [==============================] - 20s 30ms/step - loss: 1.5193 - acc: 0.1007 - precision: 0.0674 - recall: 0.9793 - val_loss: 0.8564 - val_acc: 0.0883 - val_precision: 0.0563 - val_recall: 0.9829\n",
      "Epoch 4/50\n",
      "679/679 [==============================] - 21s 30ms/step - loss: 1.4686 - acc: 0.3491 - precision: 0.0802 - recall: 0.8449 - val_loss: 0.8042 - val_acc: 0.3728 - val_precision: 0.0736 - val_recall: 0.8928\n",
      "Epoch 5/50\n",
      "679/679 [==============================] - 20s 30ms/step - loss: 1.3864 - acc: 0.5486 - precision: 0.1020 - recall: 0.7473 - val_loss: 0.6178 - val_acc: 0.7472 - val_precision: 0.1204 - val_recall: 0.5668\n",
      "Epoch 6/50\n",
      "679/679 [==============================] - 20s 30ms/step - loss: 1.2999 - acc: 0.6517 - precision: 0.1274 - recall: 0.7296 - val_loss: 0.6418 - val_acc: 0.7969 - val_precision: 0.1466 - val_recall: 0.5550\n",
      "Epoch 7/50\n",
      "679/679 [==============================] - 21s 30ms/step - loss: 1.3811 - acc: 0.5271 - precision: 0.1009 - recall: 0.7777 - val_loss: 0.7386 - val_acc: 0.6682 - val_precision: 0.1030 - val_recall: 0.6494\n",
      "Epoch 8/50\n",
      "679/679 [==============================] - 20s 30ms/step - loss: 1.2264 - acc: 0.6837 - precision: 0.1418 - recall: 0.7490 - val_loss: 0.8895 - val_acc: 0.5190 - val_precision: 0.0861 - val_recall: 0.8016\n",
      "Epoch 9/50\n",
      "679/679 [==============================] - 20s 30ms/step - loss: 1.4033 - acc: 0.4420 - precision: 0.0913 - recall: 0.8312 - val_loss: 0.8234 - val_acc: 0.4446 - val_precision: 0.0783 - val_recall: 0.8406\n",
      "Epoch 10/50\n",
      "679/679 [==============================] - 21s 30ms/step - loss: 1.2340 - acc: 0.6823 - precision: 0.1415 - recall: 0.7514 - val_loss: 0.6909 - val_acc: 0.6932 - val_precision: 0.1106 - val_recall: 0.6466\n",
      "Epoch 11/50\n",
      "679/679 [==============================] - 21s 30ms/step - loss: 1.2411 - acc: 0.6341 - precision: 0.1274 - recall: 0.7751 - val_loss: 0.4964 - val_acc: 0.8190 - val_precision: 0.1486 - val_recall: 0.4807\n",
      "Epoch 12/50\n",
      "679/679 [==============================] - 20s 30ms/step - loss: 1.2747 - acc: 0.6260 - precision: 0.1235 - recall: 0.7638 - val_loss: 0.6479 - val_acc: 0.7060 - val_precision: 0.1123 - val_recall: 0.6258\n",
      "Epoch 13/50\n",
      "679/679 [==============================] - 20s 30ms/step - loss: 1.2076 - acc: 0.6700 - precision: 0.1380 - recall: 0.7611 - val_loss: 0.5046 - val_acc: 0.8160 - val_precision: 0.1528 - val_recall: 0.5125\n",
      "Epoch 14/50\n",
      "679/679 [==============================] - 20s 30ms/step - loss: 1.2853 - acc: 0.5871 - precision: 0.1161 - recall: 0.7930 - val_loss: 0.5665 - val_acc: 0.7242 - val_precision: 0.1184 - val_recall: 0.6192\n",
      "Epoch 15/50\n",
      "679/679 [==============================] - 21s 30ms/step - loss: 1.1897 - acc: 0.6657 - precision: 0.1391 - recall: 0.7818 - val_loss: 0.5674 - val_acc: 0.7694 - val_precision: 0.1326 - val_recall: 0.5727\n",
      "Epoch 16/50\n",
      "679/679 [==============================] - 20s 30ms/step - loss: 1.3367 - acc: 0.5399 - precision: 0.1063 - recall: 0.8045 - val_loss: 0.6612 - val_acc: 0.6308 - val_precision: 0.0969 - val_recall: 0.6828\n",
      "Epoch 17/50\n",
      "679/679 [==============================] - 21s 31ms/step - loss: 1.1825 - acc: 0.6636 - precision: 0.1385 - recall: 0.7835 - val_loss: 0.7048 - val_acc: 0.6253 - val_precision: 0.1010 - val_recall: 0.7317\n",
      "Epoch 18/50\n",
      "679/679 [==============================] - 21s 30ms/step - loss: 1.3672 - acc: 0.4946 - precision: 0.0986 - recall: 0.8158 - val_loss: 0.7086 - val_acc: 0.5383 - val_precision: 0.0856 - val_recall: 0.7595\n",
      "Epoch 19/50\n",
      "679/679 [==============================] - 20s 30ms/step - loss: 1.1869 - acc: 0.6800 - precision: 0.1424 - recall: 0.7642 - val_loss: 0.7116 - val_acc: 0.6417 - val_precision: 0.1014 - val_recall: 0.6970\n",
      "Epoch 20/50\n",
      "679/679 [==============================] - 20s 30ms/step - loss: 1.1881 - acc: 0.6546 - precision: 0.1361 - recall: 0.7899 - val_loss: 0.8130 - val_acc: 0.5493 - val_precision: 0.0894 - val_recall: 0.7787\n",
      "Epoch 21/50\n",
      "679/679 [==============================] - 21s 31ms/step - loss: 1.2345 - acc: 0.6310 - precision: 0.1268 - recall: 0.7782 - val_loss: 0.7051 - val_acc: 0.5841 - val_precision: 0.0934 - val_recall: 0.7498\n",
      "Epoch 22/50\n",
      "679/679 [==============================] - 20s 30ms/step - loss: 1.1623 - acc: 0.6540 - precision: 0.1369 - recall: 0.7979 - val_loss: 0.5258 - val_acc: 0.7680 - val_precision: 0.1275 - val_recall: 0.5474\n",
      "Epoch 23/50\n",
      "679/679 [==============================] - 20s 30ms/step - loss: 1.2724 - acc: 0.5874 - precision: 0.1164 - recall: 0.7954 - val_loss: 0.7515 - val_acc: 0.5882 - val_precision: 0.0925 - val_recall: 0.7325\n",
      "Epoch 24/50\n",
      "679/679 [==============================] - 20s 30ms/step - loss: 1.1370 - acc: 0.6743 - precision: 0.1451 - recall: 0.8020 - val_loss: 0.5312 - val_acc: 0.7930 - val_precision: 0.1439 - val_recall: 0.5550\n",
      "Epoch 25/50\n",
      "679/679 [==============================] - 20s 30ms/step - loss: 1.3014 - acc: 0.5957 - precision: 0.1170 - recall: 0.7781 - val_loss: 0.5151 - val_acc: 0.8033 - val_precision: 0.1334 - val_recall: 0.4657\n",
      "Epoch 26/50\n",
      "679/679 [==============================] - 21s 30ms/step - loss: 1.1542 - acc: 0.6642 - precision: 0.1410 - recall: 0.8010 - val_loss: 0.7994 - val_acc: 0.4071 - val_precision: 0.0743 - val_recall: 0.8495\n",
      "Epoch 27/50\n",
      "679/679 [==============================] - 21s 31ms/step - loss: 1.4002 - acc: 0.4435 - precision: 0.0914 - recall: 0.8298 - val_loss: 0.5803 - val_acc: 0.7004 - val_precision: 0.1013 - val_recall: 0.5617\n",
      "Epoch 28/50\n",
      "679/679 [==============================] - 20s 30ms/step - loss: 1.1672 - acc: 0.6764 - precision: 0.1433 - recall: 0.7822 - val_loss: 0.5091 - val_acc: 0.7540 - val_precision: 0.1270 - val_recall: 0.5875\n",
      "Epoch 29/50\n",
      "679/679 [==============================] - 21s 30ms/step - loss: 1.1549 - acc: 0.6610 - precision: 0.1400 - recall: 0.8020 - val_loss: 0.6235 - val_acc: 0.7217 - val_precision: 0.1179 - val_recall: 0.6227\n",
      "Epoch 30/50\n",
      "679/679 [==============================] - 21s 30ms/step - loss: 1.2017 - acc: 0.6315 - precision: 0.1293 - recall: 0.7977 - val_loss: 0.6497 - val_acc: 0.6978 - val_precision: 0.1116 - val_recall: 0.6421\n",
      "Epoch 31/50\n",
      "679/679 [==============================] - 20s 30ms/step - loss: 1.1498 - acc: 0.6546 - precision: 0.1380 - recall: 0.8048 - val_loss: 0.4903 - val_acc: 0.7821 - val_precision: 0.1336 - val_recall: 0.5362\n",
      "Epoch 32/50\n",
      "679/679 [==============================] - 20s 30ms/step - loss: 1.2438 - acc: 0.6061 - precision: 0.1227 - recall: 0.8054 - val_loss: 0.5580 - val_acc: 0.7319 - val_precision: 0.1171 - val_recall: 0.5886\n",
      "Epoch 33/50\n",
      "679/679 [==============================] - 20s 30ms/step - loss: 1.1127 - acc: 0.6789 - precision: 0.1472 - recall: 0.8047 - val_loss: 0.4731 - val_acc: 0.7845 - val_precision: 0.1353 - val_recall: 0.5378\n",
      "Epoch 34/50\n",
      "679/679 [==============================] - 20s 30ms/step - loss: 1.2888 - acc: 0.5615 - precision: 0.1110 - recall: 0.8039 - val_loss: 0.6237 - val_acc: 0.6498 - val_precision: 0.0969 - val_recall: 0.6415\n",
      "Epoch 35/50\n",
      "679/679 [==============================] - 20s 30ms/step - loss: 1.1251 - acc: 0.6610 - precision: 0.1423 - recall: 0.8207 - val_loss: 0.5844 - val_acc: 0.6751 - val_precision: 0.1010 - val_recall: 0.6176\n",
      "Epoch 36/50\n",
      "679/679 [==============================] - 20s 30ms/step - loss: 1.3659 - acc: 0.4981 - precision: 0.0991 - recall: 0.8152 - val_loss: 0.5699 - val_acc: 0.7294 - val_precision: 0.1031 - val_recall: 0.5061\n",
      "Epoch 37/50\n",
      "679/679 [==============================] - 20s 29ms/step - loss: 1.1340 - acc: 0.6703 - precision: 0.1432 - recall: 0.7998 - val_loss: 0.6605 - val_acc: 0.6613 - val_precision: 0.1037 - val_recall: 0.6709\n",
      "Epoch 38/50\n",
      "679/679 [==============================] - 20s 30ms/step - loss: 1.1173 - acc: 0.6630 - precision: 0.1434 - recall: 0.8233 - val_loss: 0.5555 - val_acc: 0.7446 - val_precision: 0.1240 - val_recall: 0.5971\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39/50\n",
      "679/679 [==============================] - 20s 30ms/step - loss: 1.1806 - acc: 0.6297 - precision: 0.1315 - recall: 0.8206 - val_loss: 0.3933 - val_acc: 0.8345 - val_precision: 0.1508 - val_recall: 0.4304\n",
      "Epoch 40/50\n",
      "679/679 [==============================] - 20s 29ms/step - loss: 1.1337 - acc: 0.6561 - precision: 0.1402 - recall: 0.8180 - val_loss: 0.5005 - val_acc: 0.7610 - val_precision: 0.1336 - val_recall: 0.6062\n",
      "Epoch 41/50\n",
      "679/679 [==============================] - 20s 30ms/step - loss: 1.2190 - acc: 0.6191 - precision: 0.1270 - recall: 0.8109 - val_loss: 0.5637 - val_acc: 0.7227 - val_precision: 0.1158 - val_recall: 0.6052\n",
      "Epoch 42/50\n",
      "679/679 [==============================] - 20s 30ms/step - loss: 1.0683 - acc: 0.6750 - precision: 0.1489 - recall: 0.8300 - val_loss: 0.5846 - val_acc: 0.6688 - val_precision: 0.1058 - val_recall: 0.6702\n",
      "Epoch 43/50\n",
      "679/679 [==============================] - 20s 30ms/step - loss: 1.2602 - acc: 0.5718 - precision: 0.1149 - recall: 0.8174 - val_loss: 0.6340 - val_acc: 0.6529 - val_precision: 0.0982 - val_recall: 0.6451\n",
      "Epoch 44/50\n",
      "679/679 [==============================] - 20s 30ms/step - loss: 1.0933 - acc: 0.6501 - precision: 0.1405 - recall: 0.8383 - val_loss: 0.4989 - val_acc: 0.7411 - val_precision: 0.1146 - val_recall: 0.5478\n",
      "Epoch 45/50\n",
      "679/679 [==============================] - 20s 30ms/step - loss: 1.3379 - acc: 0.4980 - precision: 0.1005 - recall: 0.8294 - val_loss: 0.7424 - val_acc: 0.4910 - val_precision: 0.0813 - val_recall: 0.7968\n",
      "Epoch 46/50\n",
      "679/679 [==============================] - 20s 29ms/step - loss: 1.1433 - acc: 0.6465 - precision: 0.1367 - recall: 0.8176 - val_loss: 0.9205 - val_acc: 0.5140 - val_precision: 0.0835 - val_recall: 0.7809\n",
      "Epoch 47/50\n",
      "679/679 [==============================] - 20s 29ms/step - loss: 1.1213 - acc: 0.6398 - precision: 0.1366 - recall: 0.8362 - val_loss: 0.4853 - val_acc: 0.7585 - val_precision: 0.1239 - val_recall: 0.5551\n",
      "Epoch 48/50\n",
      "679/679 [==============================] - 20s 30ms/step - loss: 1.1580 - acc: 0.6359 - precision: 0.1336 - recall: 0.8218 - val_loss: 0.7397 - val_acc: 0.6271 - val_precision: 0.0991 - val_recall: 0.7107\n",
      "Epoch 49/50\n",
      "679/679 [==============================] - 20s 30ms/step - loss: 1.1087 - acc: 0.6642 - precision: 0.1435 - recall: 0.8210 - val_loss: 0.4121 - val_acc: 0.8349 - val_precision: 0.1514 - val_recall: 0.4313\n",
      "Epoch 50/50\n",
      "679/679 [==============================] - 20s 29ms/step - loss: 1.2079 - acc: 0.6169 - precision: 0.1270 - recall: 0.8159 - val_loss: 0.6449 - val_acc: 0.6714 - val_precision: 0.1040 - val_recall: 0.6495\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Indices are: [22400, 44800, 67200, 89200, 111200, 133200, 157200, 174400, 191600, 208800, 226000, 248800, 271600]\n",
      "INFO:root:Indices are: [14400, 28800, 43200, 57600, 77600, 101200, 124800]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "679/679 [==============================] - 91s 134ms/step - loss: 1.4707 - acc: 0.2264 - precision: 0.0743 - recall: 0.9226 - val_loss: 0.7272 - val_acc: 0.6170 - val_precision: 0.0970 - val_recall: 0.7134\n",
      "Epoch 2/50\n",
      "679/679 [==============================] - 90s 132ms/step - loss: 1.2869 - acc: 0.6583 - precision: 0.1311 - recall: 0.7407 - val_loss: 0.8185 - val_acc: 0.5852 - val_precision: 0.0928 - val_recall: 0.7412\n",
      "Epoch 3/50\n",
      "679/679 [==============================] - 90s 132ms/step - loss: 1.2858 - acc: 0.6472 - precision: 0.1272 - recall: 0.7398 - val_loss: 0.6703 - val_acc: 0.8155 - val_precision: 0.1463 - val_recall: 0.4837\n",
      "Epoch 4/50\n",
      "679/679 [==============================] - 89s 131ms/step - loss: 1.2425 - acc: 0.7024 - precision: 0.1453 - recall: 0.7173 - val_loss: 0.6495 - val_acc: 0.7879 - val_precision: 0.1472 - val_recall: 0.5922\n",
      "Epoch 5/50\n",
      "679/679 [==============================] - 90s 132ms/step - loss: 1.3096 - acc: 0.6305 - precision: 0.1231 - recall: 0.7497 - val_loss: 0.5216 - val_acc: 0.8183 - val_precision: 0.1418 - val_recall: 0.4525\n",
      "Epoch 6/50\n",
      "679/679 [==============================] - 90s 132ms/step - loss: 1.2064 - acc: 0.7066 - precision: 0.1512 - recall: 0.7449 - val_loss: 0.6472 - val_acc: 0.7478 - val_precision: 0.1309 - val_recall: 0.6320\n",
      "Epoch 7/50\n",
      "679/679 [==============================] - 90s 132ms/step - loss: 1.3156 - acc: 0.5777 - precision: 0.1127 - recall: 0.7838 - val_loss: 0.6318 - val_acc: 0.7148 - val_precision: 0.1140 - val_recall: 0.6144\n",
      "Epoch 8/50\n",
      "679/679 [==============================] - 90s 132ms/step - loss: 1.1759 - acc: 0.6991 - precision: 0.1494 - recall: 0.7566 - val_loss: 0.5417 - val_acc: 0.7717 - val_precision: 0.1346 - val_recall: 0.5768\n",
      "Epoch 9/50\n",
      "679/679 [==============================] - 90s 132ms/step - loss: 1.3639 - acc: 0.4891 - precision: 0.0978 - recall: 0.8182 - val_loss: 0.6664 - val_acc: 0.4803 - val_precision: 0.0810 - val_recall: 0.8120\n",
      "Epoch 10/50\n",
      "679/679 [==============================] - 90s 132ms/step - loss: 1.2000 - acc: 0.6816 - precision: 0.1425 - recall: 0.7603 - val_loss: 0.5582 - val_acc: 0.7825 - val_precision: 0.1378 - val_recall: 0.5582\n",
      "Epoch 11/50\n",
      "679/679 [==============================] - 89s 131ms/step - loss: 1.1640 - acc: 0.6860 - precision: 0.1457 - recall: 0.7707 - val_loss: 0.5766 - val_acc: 0.7918 - val_precision: 0.1409 - val_recall: 0.5431\n",
      "Epoch 12/50\n",
      "679/679 [==============================] - 90s 132ms/step - loss: 1.2030 - acc: 0.6574 - precision: 0.1356 - recall: 0.7769 - val_loss: 0.5765 - val_acc: 0.7455 - val_precision: 0.1219 - val_recall: 0.5810\n",
      "Epoch 13/50\n",
      "679/679 [==============================] - 90s 132ms/step - loss: 1.1476 - acc: 0.6886 - precision: 0.1487 - recall: 0.7854 - val_loss: 0.5885 - val_acc: 0.7113 - val_precision: 0.1146 - val_recall: 0.6283\n",
      "Epoch 14/50\n",
      "679/679 [==============================] - 90s 132ms/step - loss: 1.2358 - acc: 0.6273 - precision: 0.1262 - recall: 0.7829 - val_loss: 0.5794 - val_acc: 0.7203 - val_precision: 0.1105 - val_recall: 0.5763\n",
      "Epoch 15/50\n",
      "679/679 [==============================] - 90s 132ms/step - loss: 1.1524 - acc: 0.6727 - precision: 0.1427 - recall: 0.7858 - val_loss: 0.5901 - val_acc: 0.7485 - val_precision: 0.1250 - val_recall: 0.5922\n",
      "Epoch 16/50\n",
      "679/679 [==============================] - 89s 132ms/step - loss: 1.2739 - acc: 0.5844 - precision: 0.1163 - recall: 0.8006 - val_loss: 0.6799 - val_acc: 0.6447 - val_precision: 0.0964 - val_recall: 0.6487\n",
      "Epoch 17/50\n",
      "679/679 [==============================] - 89s 132ms/step - loss: 1.1062 - acc: 0.6937 - precision: 0.1525 - recall: 0.7974 - val_loss: 0.6482 - val_acc: 0.6717 - val_precision: 0.1052 - val_recall: 0.6582\n",
      "Epoch 18/50\n",
      "679/679 [==============================] - 89s 131ms/step - loss: 1.3373 - acc: 0.5183 - precision: 0.1026 - recall: 0.8110 - val_loss: 0.6836 - val_acc: 0.5828 - val_precision: 0.0841 - val_recall: 0.6624\n",
      "Epoch 19/50\n",
      "679/679 [==============================] - 90s 132ms/step - loss: 1.1125 - acc: 0.7083 - precision: 0.1568 - recall: 0.7788 - val_loss: 0.5901 - val_acc: 0.6417 - val_precision: 0.0982 - val_recall: 0.6705\n",
      "Epoch 20/50\n",
      "679/679 [==============================] - 89s 132ms/step - loss: 1.1219 - acc: 0.6782 - precision: 0.1469 - recall: 0.8050 - val_loss: 0.5036 - val_acc: 0.7802 - val_precision: 0.1302 - val_recall: 0.5241\n",
      "Epoch 21/50\n",
      "679/679 [==============================] - 89s 132ms/step - loss: 1.1534 - acc: 0.6451 - precision: 0.1352 - recall: 0.8083 - val_loss: 0.6390 - val_acc: 0.7094 - val_precision: 0.1122 - val_recall: 0.6156\n",
      "Epoch 22/50\n",
      "679/679 [==============================] - 90s 132ms/step - loss: 1.0933 - acc: 0.6921 - precision: 0.1539 - recall: 0.8124 - val_loss: 0.5741 - val_acc: 0.6977 - val_precision: 0.1039 - val_recall: 0.5863\n",
      "Epoch 23/50\n",
      "679/679 [==============================] - 89s 131ms/step - loss: 1.1845 - acc: 0.6302 - precision: 0.1298 - recall: 0.8043 - val_loss: 0.5574 - val_acc: 0.7262 - val_precision: 0.1090 - val_recall: 0.5509\n",
      "Epoch 24/50\n",
      "679/679 [==============================] - 89s 131ms/step - loss: 1.0868 - acc: 0.6781 - precision: 0.1501 - recall: 0.8278 - val_loss: 0.5211 - val_acc: 0.7458 - val_precision: 0.1228 - val_recall: 0.5856\n",
      "Epoch 25/50\n",
      "679/679 [==============================] - 90s 132ms/step - loss: 1.2712 - acc: 0.6012 - precision: 0.1203 - recall: 0.7977 - val_loss: 0.5645 - val_acc: 0.7336 - val_precision: 0.1104 - val_recall: 0.5412\n",
      "Epoch 26/50\n",
      "679/679 [==============================] - 90s 132ms/step - loss: 1.0739 - acc: 0.6944 - precision: 0.1564 - recall: 0.8182 - val_loss: 0.5731 - val_acc: 0.7438 - val_precision: 0.1161 - val_recall: 0.5500\n",
      "Epoch 27/50\n",
      "679/679 [==============================] - 90s 132ms/step - loss: 1.3147 - acc: 0.5119 - precision: 0.1031 - recall: 0.8283 - val_loss: 0.5782 - val_acc: 0.6961 - val_precision: 0.0943 - val_recall: 0.5225\n",
      "Epoch 28/50\n",
      "679/679 [==============================] - 90s 132ms/step - loss: 1.0773 - acc: 0.7042 - precision: 0.1582 - recall: 0.8014 - val_loss: 0.6746 - val_acc: 0.6053 - val_precision: 0.0916 - val_recall: 0.6886\n",
      "Epoch 29/50\n",
      "679/679 [==============================] - 89s 132ms/step - loss: 1.0752 - acc: 0.6747 - precision: 0.1489 - recall: 0.8312 - val_loss: 0.4794 - val_acc: 0.8326 - val_precision: 0.1610 - val_recall: 0.4818\n",
      "Epoch 30/50\n",
      "679/679 [==============================] - 90s 132ms/step - loss: 1.1232 - acc: 0.6564 - precision: 0.1405 - recall: 0.8196 - val_loss: 0.6170 - val_acc: 0.6983 - val_precision: 0.1069 - val_recall: 0.6065\n",
      "Epoch 31/50\n",
      "679/679 [==============================] - 90s 132ms/step - loss: 1.0586 - acc: 0.6891 - precision: 0.1538 - recall: 0.8197 - val_loss: 0.4959 - val_acc: 0.7918 - val_precision: 0.1309 - val_recall: 0.4906\n",
      "Epoch 32/50\n",
      "679/679 [==============================] - 90s 132ms/step - loss: 1.1649 - acc: 0.6328 - precision: 0.1320 - recall: 0.8165 - val_loss: 0.5041 - val_acc: 0.7457 - val_precision: 0.1151 - val_recall: 0.5386\n",
      "Epoch 33/50\n",
      "679/679 [==============================] - 89s 132ms/step - loss: 1.0510 - acc: 0.6829 - precision: 0.1533 - recall: 0.8390 - val_loss: 0.5336 - val_acc: 0.7507 - val_precision: 0.1266 - val_recall: 0.5949\n",
      "Epoch 34/50\n",
      "679/679 [==============================] - 89s 132ms/step - loss: 1.2146 - acc: 0.5997 - precision: 0.1219 - recall: 0.8148 - val_loss: 0.6785 - val_acc: 0.6598 - val_precision: 0.0932 - val_recall: 0.5903\n",
      "Epoch 35/50\n",
      "679/679 [==============================] - 89s 132ms/step - loss: 1.0404 - acc: 0.6972 - precision: 0.1586 - recall: 0.8300 - val_loss: 0.5471 - val_acc: 0.7035 - val_precision: 0.1073 - val_recall: 0.5960\n",
      "Epoch 36/50\n",
      "679/679 [==============================] - 90s 132ms/step - loss: 1.2986 - acc: 0.5266 - precision: 0.1058 - recall: 0.8264 - val_loss: 0.5750 - val_acc: 0.6982 - val_precision: 0.0963 - val_recall: 0.5321\n",
      "Epoch 37/50\n",
      "679/679 [==============================] - 89s 132ms/step - loss: 1.0650 - acc: 0.7067 - precision: 0.1617 - recall: 0.8207 - val_loss: 0.5518 - val_acc: 0.7119 - val_precision: 0.1091 - val_recall: 0.5879\n",
      "Epoch 38/50\n",
      "679/679 [==============================] - 90s 132ms/step - loss: 1.0332 - acc: 0.6902 - precision: 0.1564 - recall: 0.8386 - val_loss: 0.5280 - val_acc: 0.7657 - val_precision: 0.1043 - val_recall: 0.4272\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39/50\n",
      "679/679 [==============================] - 90s 132ms/step - loss: 1.0945 - acc: 0.6549 - precision: 0.1414 - recall: 0.8310 - val_loss: 0.5115 - val_acc: 0.7528 - val_precision: 0.1203 - val_recall: 0.5503\n",
      "Epoch 40/50\n",
      "679/679 [==============================] - 90s 132ms/step - loss: 1.0319 - acc: 0.6978 - precision: 0.1590 - recall: 0.8327 - val_loss: 0.5047 - val_acc: 0.7652 - val_precision: 0.1285 - val_recall: 0.5616\n",
      "Epoch 41/50\n",
      "679/679 [==============================] - 90s 132ms/step - loss: 1.1475 - acc: 0.6462 - precision: 0.1361 - recall: 0.8139 - val_loss: 0.5311 - val_acc: 0.6961 - val_precision: 0.0992 - val_recall: 0.5569\n",
      "Epoch 42/50\n",
      "679/679 [==============================] - 89s 132ms/step - loss: 1.0176 - acc: 0.6917 - precision: 0.1580 - recall: 0.8454 - val_loss: 0.5806 - val_acc: 0.6950 - val_precision: 0.1038 - val_recall: 0.5919\n",
      "Epoch 43/50\n",
      "679/679 [==============================] - 90s 132ms/step - loss: 1.1914 - acc: 0.6150 - precision: 0.1268 - recall: 0.8188 - val_loss: 0.5262 - val_acc: 0.7455 - val_precision: 0.1077 - val_recall: 0.4948\n",
      "Epoch 44/50\n",
      "679/679 [==============================] - 90s 132ms/step - loss: 0.9957 - acc: 0.7066 - precision: 0.1646 - recall: 0.8428 - val_loss: 0.6018 - val_acc: 0.6651 - val_precision: 0.0990 - val_recall: 0.6244\n",
      "Epoch 45/50\n",
      "679/679 [==============================] - 90s 132ms/step - loss: 1.2820 - acc: 0.5416 - precision: 0.1082 - recall: 0.8196 - val_loss: 0.5990 - val_acc: 0.6884 - val_precision: 0.0898 - val_recall: 0.5077\n",
      "Epoch 46/50\n",
      "679/679 [==============================] - 90s 132ms/step - loss: 1.0425 - acc: 0.6913 - precision: 0.1565 - recall: 0.8339 - val_loss: 0.5690 - val_acc: 0.7111 - val_precision: 0.1136 - val_recall: 0.6213\n",
      "Epoch 47/50\n",
      "679/679 [==============================] - 89s 132ms/step - loss: 1.0148 - acc: 0.6919 - precision: 0.1581 - recall: 0.8463 - val_loss: 0.4668 - val_acc: 0.7469 - val_precision: 0.1109 - val_recall: 0.5099\n",
      "Epoch 48/50\n",
      "679/679 [==============================] - 90s 132ms/step - loss: 1.0667 - acc: 0.6573 - precision: 0.1435 - recall: 0.8415 - val_loss: 0.4768 - val_acc: 0.8062 - val_precision: 0.1349 - val_recall: 0.4630\n",
      "Epoch 49/50\n",
      "679/679 [==============================] - 89s 131ms/step - loss: 0.9900 - acc: 0.7133 - precision: 0.1675 - recall: 0.8407 - val_loss: 0.4843 - val_acc: 0.7957 - val_precision: 0.1336 - val_recall: 0.4916\n",
      "Epoch 50/50\n",
      "679/679 [==============================] - 90s 132ms/step - loss: 1.1123 - acc: 0.6526 - precision: 0.1398 - recall: 0.8249 - val_loss: 0.5840 - val_acc: 0.6594 - val_precision: 0.0945 - val_recall: 0.6020\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Experiment skipped due to error. Model: {'experiment_num': [308], 'valid': [True], 'num_mels': [80], 'win_len': [10], 'chunk_radius': [11], 'batch_size': [400], 'learning_rate': [0.001], 'note_class_weight': [{0: 1, 1: 20}], 'model_version': [1], 'spec_conv_layers': [[{'filters': 64, 'kernel_size': (3, 3), 'layers': 2, 'max_pool': True, 'activation': 'relu'}, {'filters': 128, 'kernel_size': (3, 3), 'layers': 2, 'max_pool': True, 'activation': 'relu'}, {'filters': 256, 'kernel_size': (3, 3), 'layers': 2, 'max_pool': True, 'activation': 'relu'}, {'filters': 512, 'kernel_size': (3, 3), 'layers': 2, 'max_pool': True, 'activation': 'relu'}]], 'spec_dense_layers': [[]], 'combined_layers': [[{'units': 23, 'activation': 'relu', 'num_layers': 3}]]}\n",
      "Error: Negative dimension size caused by subtracting 3 from 1 for 'conv2d_452/convolution' (op: 'Conv2D') with input shapes: [?,8,1,256], [3,3,256,512].\n"
     ]
    }
   ],
   "source": [
    "#from tensorflow.python.client import device_lib\n",
    "#print(device_lib.list_local_devices())\n",
    "\n",
    "for chunk_radius in [3, 5, 7, 9, 11]:\n",
    "#for chunk_radius in [5]:\n",
    "    for learning_rate in [0.00001, 0.0001, 0.001]:\n",
    "    #for learning_rate in [0.001]:\n",
    "        for note_weight in [10, 15, 20]:\n",
    "        #for note_weight in [15]:\n",
    "            note_class_weight = {0: 1, 1: note_weight}\n",
    "            \n",
    "            for i in [1, 2, 3]:\n",
    "                spec_conv_layers = []\n",
    "                spec_dense_layers = []\n",
    "                combined_layers = []\n",
    "                \n",
    "                if i == 1:\n",
    "                    spec_conv_layers.append(create_conv2D_obj(64, (1, chunk_radius * 2 + 1), 4, False, \"relu\"))\n",
    "                    \n",
    "                    spec_dense_layers.append(create_dense_obj(10 * chunk_radius * 2 + 1, \"relu\", 3))\n",
    "                    \n",
    "                    combined_layers.append(create_dense_obj(chunk_radius * 2 + 1, \"relu\", 3))\n",
    "                elif i == 2:\n",
    "                    spec_conv_layers.append(create_conv2D_obj(64, (3, 3), 2, True, \"relu\"))\n",
    "                    spec_conv_layers.append(create_conv2D_obj(128, (3, 3), 2, True, \"relu\"))\n",
    "                    \n",
    "                    spec_dense_layers.append(create_dense_obj(10 * chunk_radius * 2 + 1, \"relu\", 2))\n",
    "                    \n",
    "                    combined_layers.append(create_dense_obj(chunk_radius * 2 + 1, \"relu\", 3))\n",
    "                elif i == 3:\n",
    "                    spec_conv_layers.append(create_conv2D_obj(64, (3, 3), 2, True, \"relu\"))\n",
    "                    spec_conv_layers.append(create_conv2D_obj(128, (3, 3), 2, True, \"relu\"))\n",
    "                    spec_conv_layers.append(create_conv2D_obj(256, (3, 3), 2, True, \"relu\"))\n",
    "                    spec_conv_layers.append(create_conv2D_obj(512, (3, 3), 2, True, \"relu\"))\n",
    "                    \n",
    "                    combined_layers.append(create_dense_obj(chunk_radius * 2 + 1, \"relu\", 3))\n",
    "                    \n",
    "                run_note_place_experiment(song_df, num_mels, win_len, chunk_radius, \n",
    "                              batch_size, num_epochs, learning_rate, \n",
    "                              note_class_weight, train_keys, validation_keys,\n",
    "                              model_folder, model_version, \n",
    "                              spec_conv_layers, spec_dense_layers, combined_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>experiment_num</th>\n",
       "      <th>valid</th>\n",
       "      <th>num_mels</th>\n",
       "      <th>win_len</th>\n",
       "      <th>chunk_radius</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>note_class_weight</th>\n",
       "      <th>model_version</th>\n",
       "      <th>spec_conv_layers</th>\n",
       "      <th>spec_dense_layers</th>\n",
       "      <th>combined_layers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>80</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>400</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>{0: 1, 1: 10}</td>\n",
       "      <td>1</td>\n",
       "      <td>[{'filters': 64, 'kernel_size': (1, 7), 'layer...</td>\n",
       "      <td>[{'units': 61, 'activation': 'relu', 'num_laye...</td>\n",
       "      <td>[{'units': 7, 'activation': 'relu', 'num_layer...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>False</td>\n",
       "      <td>80</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>400</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>{0: 1, 1: 10}</td>\n",
       "      <td>1</td>\n",
       "      <td>[{'filters': 64, 'kernel_size': (3, 3), 'layer...</td>\n",
       "      <td>[{'units': 61, 'activation': 'relu', 'num_laye...</td>\n",
       "      <td>[{'units': 7, 'activation': 'relu', 'num_layer...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>False</td>\n",
       "      <td>80</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>400</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>{0: 1, 1: 10}</td>\n",
       "      <td>1</td>\n",
       "      <td>[{'filters': 64, 'kernel_size': (3, 3), 'layer...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[{'units': 7, 'activation': 'relu', 'num_layer...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>True</td>\n",
       "      <td>80</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>400</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>{0: 1, 1: 15}</td>\n",
       "      <td>1</td>\n",
       "      <td>[{'filters': 64, 'kernel_size': (1, 7), 'layer...</td>\n",
       "      <td>[{'units': 61, 'activation': 'relu', 'num_laye...</td>\n",
       "      <td>[{'units': 7, 'activation': 'relu', 'num_layer...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>False</td>\n",
       "      <td>80</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>400</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>{0: 1, 1: 15}</td>\n",
       "      <td>1</td>\n",
       "      <td>[{'filters': 64, 'kernel_size': (3, 3), 'layer...</td>\n",
       "      <td>[{'units': 61, 'activation': 'relu', 'num_laye...</td>\n",
       "      <td>[{'units': 7, 'activation': 'relu', 'num_layer...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>False</td>\n",
       "      <td>80</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>400</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>{0: 1, 1: 15}</td>\n",
       "      <td>1</td>\n",
       "      <td>[{'filters': 64, 'kernel_size': (3, 3), 'layer...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[{'units': 7, 'activation': 'relu', 'num_layer...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>True</td>\n",
       "      <td>80</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>400</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>{0: 1, 1: 20}</td>\n",
       "      <td>1</td>\n",
       "      <td>[{'filters': 64, 'kernel_size': (1, 7), 'layer...</td>\n",
       "      <td>[{'units': 61, 'activation': 'relu', 'num_laye...</td>\n",
       "      <td>[{'units': 7, 'activation': 'relu', 'num_layer...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>False</td>\n",
       "      <td>80</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>400</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>{0: 1, 1: 20}</td>\n",
       "      <td>1</td>\n",
       "      <td>[{'filters': 64, 'kernel_size': (3, 3), 'layer...</td>\n",
       "      <td>[{'units': 61, 'activation': 'relu', 'num_laye...</td>\n",
       "      <td>[{'units': 7, 'activation': 'relu', 'num_layer...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>False</td>\n",
       "      <td>80</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>400</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>{0: 1, 1: 20}</td>\n",
       "      <td>1</td>\n",
       "      <td>[{'filters': 64, 'kernel_size': (3, 3), 'layer...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[{'units': 7, 'activation': 'relu', 'num_layer...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>True</td>\n",
       "      <td>80</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>400</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>{0: 1, 1: 10}</td>\n",
       "      <td>1</td>\n",
       "      <td>[{'filters': 64, 'kernel_size': (1, 7), 'layer...</td>\n",
       "      <td>[{'units': 61, 'activation': 'relu', 'num_laye...</td>\n",
       "      <td>[{'units': 7, 'activation': 'relu', 'num_layer...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>False</td>\n",
       "      <td>80</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>400</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>{0: 1, 1: 10}</td>\n",
       "      <td>1</td>\n",
       "      <td>[{'filters': 64, 'kernel_size': (3, 3), 'layer...</td>\n",
       "      <td>[{'units': 61, 'activation': 'relu', 'num_laye...</td>\n",
       "      <td>[{'units': 7, 'activation': 'relu', 'num_layer...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>False</td>\n",
       "      <td>80</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>400</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>{0: 1, 1: 10}</td>\n",
       "      <td>1</td>\n",
       "      <td>[{'filters': 64, 'kernel_size': (3, 3), 'layer...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[{'units': 7, 'activation': 'relu', 'num_layer...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>True</td>\n",
       "      <td>80</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>400</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>{0: 1, 1: 15}</td>\n",
       "      <td>1</td>\n",
       "      <td>[{'filters': 64, 'kernel_size': (1, 7), 'layer...</td>\n",
       "      <td>[{'units': 61, 'activation': 'relu', 'num_laye...</td>\n",
       "      <td>[{'units': 7, 'activation': 'relu', 'num_layer...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>False</td>\n",
       "      <td>80</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>400</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>{0: 1, 1: 15}</td>\n",
       "      <td>1</td>\n",
       "      <td>[{'filters': 64, 'kernel_size': (3, 3), 'layer...</td>\n",
       "      <td>[{'units': 61, 'activation': 'relu', 'num_laye...</td>\n",
       "      <td>[{'units': 7, 'activation': 'relu', 'num_layer...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>False</td>\n",
       "      <td>80</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>400</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>{0: 1, 1: 15}</td>\n",
       "      <td>1</td>\n",
       "      <td>[{'filters': 64, 'kernel_size': (3, 3), 'layer...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[{'units': 7, 'activation': 'relu', 'num_layer...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16</td>\n",
       "      <td>True</td>\n",
       "      <td>80</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>400</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>{0: 1, 1: 20}</td>\n",
       "      <td>1</td>\n",
       "      <td>[{'filters': 64, 'kernel_size': (1, 7), 'layer...</td>\n",
       "      <td>[{'units': 61, 'activation': 'relu', 'num_laye...</td>\n",
       "      <td>[{'units': 7, 'activation': 'relu', 'num_layer...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17</td>\n",
       "      <td>False</td>\n",
       "      <td>80</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>400</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>{0: 1, 1: 20}</td>\n",
       "      <td>1</td>\n",
       "      <td>[{'filters': 64, 'kernel_size': (3, 3), 'layer...</td>\n",
       "      <td>[{'units': 61, 'activation': 'relu', 'num_laye...</td>\n",
       "      <td>[{'units': 7, 'activation': 'relu', 'num_layer...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18</td>\n",
       "      <td>False</td>\n",
       "      <td>80</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>400</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>{0: 1, 1: 20}</td>\n",
       "      <td>1</td>\n",
       "      <td>[{'filters': 64, 'kernel_size': (3, 3), 'layer...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[{'units': 7, 'activation': 'relu', 'num_layer...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19</td>\n",
       "      <td>True</td>\n",
       "      <td>80</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>400</td>\n",
       "      <td>0.00100</td>\n",
       "      <td>{0: 1, 1: 10}</td>\n",
       "      <td>1</td>\n",
       "      <td>[{'filters': 64, 'kernel_size': (1, 7), 'layer...</td>\n",
       "      <td>[{'units': 61, 'activation': 'relu', 'num_laye...</td>\n",
       "      <td>[{'units': 7, 'activation': 'relu', 'num_layer...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20</td>\n",
       "      <td>False</td>\n",
       "      <td>80</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>400</td>\n",
       "      <td>0.00100</td>\n",
       "      <td>{0: 1, 1: 10}</td>\n",
       "      <td>1</td>\n",
       "      <td>[{'filters': 64, 'kernel_size': (3, 3), 'layer...</td>\n",
       "      <td>[{'units': 61, 'activation': 'relu', 'num_laye...</td>\n",
       "      <td>[{'units': 7, 'activation': 'relu', 'num_layer...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>21</td>\n",
       "      <td>False</td>\n",
       "      <td>80</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>400</td>\n",
       "      <td>0.00100</td>\n",
       "      <td>{0: 1, 1: 10}</td>\n",
       "      <td>1</td>\n",
       "      <td>[{'filters': 64, 'kernel_size': (3, 3), 'layer...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[{'units': 7, 'activation': 'relu', 'num_layer...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>22</td>\n",
       "      <td>True</td>\n",
       "      <td>80</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>400</td>\n",
       "      <td>0.00100</td>\n",
       "      <td>{0: 1, 1: 15}</td>\n",
       "      <td>1</td>\n",
       "      <td>[{'filters': 64, 'kernel_size': (1, 7), 'layer...</td>\n",
       "      <td>[{'units': 61, 'activation': 'relu', 'num_laye...</td>\n",
       "      <td>[{'units': 7, 'activation': 'relu', 'num_layer...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>23</td>\n",
       "      <td>False</td>\n",
       "      <td>80</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>400</td>\n",
       "      <td>0.00100</td>\n",
       "      <td>{0: 1, 1: 15}</td>\n",
       "      <td>1</td>\n",
       "      <td>[{'filters': 64, 'kernel_size': (3, 3), 'layer...</td>\n",
       "      <td>[{'units': 61, 'activation': 'relu', 'num_laye...</td>\n",
       "      <td>[{'units': 7, 'activation': 'relu', 'num_layer...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>24</td>\n",
       "      <td>False</td>\n",
       "      <td>80</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>400</td>\n",
       "      <td>0.00100</td>\n",
       "      <td>{0: 1, 1: 15}</td>\n",
       "      <td>1</td>\n",
       "      <td>[{'filters': 64, 'kernel_size': (3, 3), 'layer...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[{'units': 7, 'activation': 'relu', 'num_layer...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>25</td>\n",
       "      <td>True</td>\n",
       "      <td>80</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>400</td>\n",
       "      <td>0.00100</td>\n",
       "      <td>{0: 1, 1: 20}</td>\n",
       "      <td>1</td>\n",
       "      <td>[{'filters': 64, 'kernel_size': (1, 7), 'layer...</td>\n",
       "      <td>[{'units': 61, 'activation': 'relu', 'num_laye...</td>\n",
       "      <td>[{'units': 7, 'activation': 'relu', 'num_layer...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>26</td>\n",
       "      <td>False</td>\n",
       "      <td>80</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>400</td>\n",
       "      <td>0.00100</td>\n",
       "      <td>{0: 1, 1: 20}</td>\n",
       "      <td>1</td>\n",
       "      <td>[{'filters': 64, 'kernel_size': (3, 3), 'layer...</td>\n",
       "      <td>[{'units': 61, 'activation': 'relu', 'num_laye...</td>\n",
       "      <td>[{'units': 7, 'activation': 'relu', 'num_layer...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>27</td>\n",
       "      <td>False</td>\n",
       "      <td>80</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>400</td>\n",
       "      <td>0.00100</td>\n",
       "      <td>{0: 1, 1: 20}</td>\n",
       "      <td>1</td>\n",
       "      <td>[{'filters': 64, 'kernel_size': (3, 3), 'layer...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[{'units': 7, 'activation': 'relu', 'num_layer...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>28</td>\n",
       "      <td>True</td>\n",
       "      <td>80</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>400</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>{0: 1, 1: 10}</td>\n",
       "      <td>1</td>\n",
       "      <td>[{'filters': 64, 'kernel_size': (1, 11), 'laye...</td>\n",
       "      <td>[{'units': 101, 'activation': 'relu', 'num_lay...</td>\n",
       "      <td>[{'units': 11, 'activation': 'relu', 'num_laye...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>29</td>\n",
       "      <td>True</td>\n",
       "      <td>80</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>400</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>{0: 1, 1: 10}</td>\n",
       "      <td>1</td>\n",
       "      <td>[{'filters': 64, 'kernel_size': (3, 3), 'layer...</td>\n",
       "      <td>[{'units': 101, 'activation': 'relu', 'num_lay...</td>\n",
       "      <td>[{'units': 11, 'activation': 'relu', 'num_laye...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>30</td>\n",
       "      <td>False</td>\n",
       "      <td>80</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>400</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>{0: 1, 1: 10}</td>\n",
       "      <td>1</td>\n",
       "      <td>[{'filters': 64, 'kernel_size': (3, 3), 'layer...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[{'units': 11, 'activation': 'relu', 'num_laye...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>278</th>\n",
       "      <td>279</td>\n",
       "      <td>False</td>\n",
       "      <td>80</td>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "      <td>400</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>{0: 1, 1: 10}</td>\n",
       "      <td>1</td>\n",
       "      <td>[{'filters': 64, 'kernel_size': (3, 3), 'layer...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[{'units': 19, 'activation': 'relu', 'num_laye...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>279</th>\n",
       "      <td>280</td>\n",
       "      <td>False</td>\n",
       "      <td>80</td>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "      <td>400</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>{0: 1, 1: 15}</td>\n",
       "      <td>1</td>\n",
       "      <td>[{'filters': 64, 'kernel_size': (3, 3), 'layer...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[{'units': 19, 'activation': 'relu', 'num_laye...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>280</th>\n",
       "      <td>281</td>\n",
       "      <td>False</td>\n",
       "      <td>80</td>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "      <td>400</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>{0: 1, 1: 20}</td>\n",
       "      <td>1</td>\n",
       "      <td>[{'filters': 64, 'kernel_size': (3, 3), 'layer...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[{'units': 19, 'activation': 'relu', 'num_laye...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>281</th>\n",
       "      <td>282</td>\n",
       "      <td>False</td>\n",
       "      <td>80</td>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "      <td>400</td>\n",
       "      <td>0.00100</td>\n",
       "      <td>{0: 1, 1: 10}</td>\n",
       "      <td>1</td>\n",
       "      <td>[{'filters': 64, 'kernel_size': (3, 3), 'layer...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[{'units': 19, 'activation': 'relu', 'num_laye...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>282</th>\n",
       "      <td>283</td>\n",
       "      <td>False</td>\n",
       "      <td>80</td>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "      <td>400</td>\n",
       "      <td>0.00100</td>\n",
       "      <td>{0: 1, 1: 15}</td>\n",
       "      <td>1</td>\n",
       "      <td>[{'filters': 64, 'kernel_size': (3, 3), 'layer...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[{'units': 19, 'activation': 'relu', 'num_laye...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>283</th>\n",
       "      <td>284</td>\n",
       "      <td>False</td>\n",
       "      <td>80</td>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "      <td>400</td>\n",
       "      <td>0.00100</td>\n",
       "      <td>{0: 1, 1: 20}</td>\n",
       "      <td>1</td>\n",
       "      <td>[{'filters': 64, 'kernel_size': (3, 3), 'layer...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[{'units': 19, 'activation': 'relu', 'num_laye...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284</th>\n",
       "      <td>285</td>\n",
       "      <td>False</td>\n",
       "      <td>80</td>\n",
       "      <td>10</td>\n",
       "      <td>11</td>\n",
       "      <td>400</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>{0: 1, 1: 10}</td>\n",
       "      <td>1</td>\n",
       "      <td>[{'filters': 64, 'kernel_size': (3, 3), 'layer...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[{'units': 23, 'activation': 'relu', 'num_laye...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>285</th>\n",
       "      <td>286</td>\n",
       "      <td>True</td>\n",
       "      <td>80</td>\n",
       "      <td>10</td>\n",
       "      <td>11</td>\n",
       "      <td>400</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>{0: 1, 1: 15}</td>\n",
       "      <td>1</td>\n",
       "      <td>[{'filters': 64, 'kernel_size': (3, 3), 'layer...</td>\n",
       "      <td>[{'units': 221, 'activation': 'relu', 'num_lay...</td>\n",
       "      <td>[{'units': 23, 'activation': 'relu', 'num_laye...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>286</th>\n",
       "      <td>287</td>\n",
       "      <td>False</td>\n",
       "      <td>80</td>\n",
       "      <td>10</td>\n",
       "      <td>11</td>\n",
       "      <td>400</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>{0: 1, 1: 15}</td>\n",
       "      <td>1</td>\n",
       "      <td>[{'filters': 64, 'kernel_size': (3, 3), 'layer...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[{'units': 23, 'activation': 'relu', 'num_laye...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>287</th>\n",
       "      <td>288</td>\n",
       "      <td>True</td>\n",
       "      <td>80</td>\n",
       "      <td>10</td>\n",
       "      <td>11</td>\n",
       "      <td>400</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>{0: 1, 1: 20}</td>\n",
       "      <td>1</td>\n",
       "      <td>[{'filters': 64, 'kernel_size': (1, 23), 'laye...</td>\n",
       "      <td>[{'units': 221, 'activation': 'relu', 'num_lay...</td>\n",
       "      <td>[{'units': 23, 'activation': 'relu', 'num_laye...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>288</th>\n",
       "      <td>289</td>\n",
       "      <td>True</td>\n",
       "      <td>80</td>\n",
       "      <td>10</td>\n",
       "      <td>11</td>\n",
       "      <td>400</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>{0: 1, 1: 20}</td>\n",
       "      <td>1</td>\n",
       "      <td>[{'filters': 64, 'kernel_size': (3, 3), 'layer...</td>\n",
       "      <td>[{'units': 221, 'activation': 'relu', 'num_lay...</td>\n",
       "      <td>[{'units': 23, 'activation': 'relu', 'num_laye...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>289</th>\n",
       "      <td>290</td>\n",
       "      <td>False</td>\n",
       "      <td>80</td>\n",
       "      <td>10</td>\n",
       "      <td>11</td>\n",
       "      <td>400</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>{0: 1, 1: 20}</td>\n",
       "      <td>1</td>\n",
       "      <td>[{'filters': 64, 'kernel_size': (3, 3), 'layer...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[{'units': 23, 'activation': 'relu', 'num_laye...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>290</th>\n",
       "      <td>291</td>\n",
       "      <td>True</td>\n",
       "      <td>80</td>\n",
       "      <td>10</td>\n",
       "      <td>11</td>\n",
       "      <td>400</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>{0: 1, 1: 10}</td>\n",
       "      <td>1</td>\n",
       "      <td>[{'filters': 64, 'kernel_size': (1, 23), 'laye...</td>\n",
       "      <td>[{'units': 221, 'activation': 'relu', 'num_lay...</td>\n",
       "      <td>[{'units': 23, 'activation': 'relu', 'num_laye...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291</th>\n",
       "      <td>292</td>\n",
       "      <td>True</td>\n",
       "      <td>80</td>\n",
       "      <td>10</td>\n",
       "      <td>11</td>\n",
       "      <td>400</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>{0: 1, 1: 10}</td>\n",
       "      <td>1</td>\n",
       "      <td>[{'filters': 64, 'kernel_size': (3, 3), 'layer...</td>\n",
       "      <td>[{'units': 221, 'activation': 'relu', 'num_lay...</td>\n",
       "      <td>[{'units': 23, 'activation': 'relu', 'num_laye...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>292</th>\n",
       "      <td>293</td>\n",
       "      <td>False</td>\n",
       "      <td>80</td>\n",
       "      <td>10</td>\n",
       "      <td>11</td>\n",
       "      <td>400</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>{0: 1, 1: 10}</td>\n",
       "      <td>1</td>\n",
       "      <td>[{'filters': 64, 'kernel_size': (3, 3), 'layer...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[{'units': 23, 'activation': 'relu', 'num_laye...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>293</th>\n",
       "      <td>294</td>\n",
       "      <td>True</td>\n",
       "      <td>80</td>\n",
       "      <td>10</td>\n",
       "      <td>11</td>\n",
       "      <td>400</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>{0: 1, 1: 15}</td>\n",
       "      <td>1</td>\n",
       "      <td>[{'filters': 64, 'kernel_size': (1, 23), 'laye...</td>\n",
       "      <td>[{'units': 221, 'activation': 'relu', 'num_lay...</td>\n",
       "      <td>[{'units': 23, 'activation': 'relu', 'num_laye...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>294</th>\n",
       "      <td>295</td>\n",
       "      <td>True</td>\n",
       "      <td>80</td>\n",
       "      <td>10</td>\n",
       "      <td>11</td>\n",
       "      <td>400</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>{0: 1, 1: 15}</td>\n",
       "      <td>1</td>\n",
       "      <td>[{'filters': 64, 'kernel_size': (3, 3), 'layer...</td>\n",
       "      <td>[{'units': 221, 'activation': 'relu', 'num_lay...</td>\n",
       "      <td>[{'units': 23, 'activation': 'relu', 'num_laye...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>295</th>\n",
       "      <td>296</td>\n",
       "      <td>False</td>\n",
       "      <td>80</td>\n",
       "      <td>10</td>\n",
       "      <td>11</td>\n",
       "      <td>400</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>{0: 1, 1: 15}</td>\n",
       "      <td>1</td>\n",
       "      <td>[{'filters': 64, 'kernel_size': (3, 3), 'layer...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[{'units': 23, 'activation': 'relu', 'num_laye...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>296</th>\n",
       "      <td>297</td>\n",
       "      <td>True</td>\n",
       "      <td>80</td>\n",
       "      <td>10</td>\n",
       "      <td>11</td>\n",
       "      <td>400</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>{0: 1, 1: 20}</td>\n",
       "      <td>1</td>\n",
       "      <td>[{'filters': 64, 'kernel_size': (1, 23), 'laye...</td>\n",
       "      <td>[{'units': 221, 'activation': 'relu', 'num_lay...</td>\n",
       "      <td>[{'units': 23, 'activation': 'relu', 'num_laye...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297</th>\n",
       "      <td>298</td>\n",
       "      <td>True</td>\n",
       "      <td>80</td>\n",
       "      <td>10</td>\n",
       "      <td>11</td>\n",
       "      <td>400</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>{0: 1, 1: 20}</td>\n",
       "      <td>1</td>\n",
       "      <td>[{'filters': 64, 'kernel_size': (3, 3), 'layer...</td>\n",
       "      <td>[{'units': 221, 'activation': 'relu', 'num_lay...</td>\n",
       "      <td>[{'units': 23, 'activation': 'relu', 'num_laye...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>299</td>\n",
       "      <td>False</td>\n",
       "      <td>80</td>\n",
       "      <td>10</td>\n",
       "      <td>11</td>\n",
       "      <td>400</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>{0: 1, 1: 20}</td>\n",
       "      <td>1</td>\n",
       "      <td>[{'filters': 64, 'kernel_size': (3, 3), 'layer...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[{'units': 23, 'activation': 'relu', 'num_laye...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>300</td>\n",
       "      <td>True</td>\n",
       "      <td>80</td>\n",
       "      <td>10</td>\n",
       "      <td>11</td>\n",
       "      <td>400</td>\n",
       "      <td>0.00100</td>\n",
       "      <td>{0: 1, 1: 10}</td>\n",
       "      <td>1</td>\n",
       "      <td>[{'filters': 64, 'kernel_size': (1, 23), 'laye...</td>\n",
       "      <td>[{'units': 221, 'activation': 'relu', 'num_lay...</td>\n",
       "      <td>[{'units': 23, 'activation': 'relu', 'num_laye...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300</th>\n",
       "      <td>301</td>\n",
       "      <td>True</td>\n",
       "      <td>80</td>\n",
       "      <td>10</td>\n",
       "      <td>11</td>\n",
       "      <td>400</td>\n",
       "      <td>0.00100</td>\n",
       "      <td>{0: 1, 1: 10}</td>\n",
       "      <td>1</td>\n",
       "      <td>[{'filters': 64, 'kernel_size': (3, 3), 'layer...</td>\n",
       "      <td>[{'units': 221, 'activation': 'relu', 'num_lay...</td>\n",
       "      <td>[{'units': 23, 'activation': 'relu', 'num_laye...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>301</th>\n",
       "      <td>302</td>\n",
       "      <td>False</td>\n",
       "      <td>80</td>\n",
       "      <td>10</td>\n",
       "      <td>11</td>\n",
       "      <td>400</td>\n",
       "      <td>0.00100</td>\n",
       "      <td>{0: 1, 1: 10}</td>\n",
       "      <td>1</td>\n",
       "      <td>[{'filters': 64, 'kernel_size': (3, 3), 'layer...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[{'units': 23, 'activation': 'relu', 'num_laye...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>302</th>\n",
       "      <td>303</td>\n",
       "      <td>True</td>\n",
       "      <td>80</td>\n",
       "      <td>10</td>\n",
       "      <td>11</td>\n",
       "      <td>400</td>\n",
       "      <td>0.00100</td>\n",
       "      <td>{0: 1, 1: 15}</td>\n",
       "      <td>1</td>\n",
       "      <td>[{'filters': 64, 'kernel_size': (1, 23), 'laye...</td>\n",
       "      <td>[{'units': 221, 'activation': 'relu', 'num_lay...</td>\n",
       "      <td>[{'units': 23, 'activation': 'relu', 'num_laye...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>303</th>\n",
       "      <td>304</td>\n",
       "      <td>True</td>\n",
       "      <td>80</td>\n",
       "      <td>10</td>\n",
       "      <td>11</td>\n",
       "      <td>400</td>\n",
       "      <td>0.00100</td>\n",
       "      <td>{0: 1, 1: 15}</td>\n",
       "      <td>1</td>\n",
       "      <td>[{'filters': 64, 'kernel_size': (3, 3), 'layer...</td>\n",
       "      <td>[{'units': 221, 'activation': 'relu', 'num_lay...</td>\n",
       "      <td>[{'units': 23, 'activation': 'relu', 'num_laye...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>304</th>\n",
       "      <td>305</td>\n",
       "      <td>False</td>\n",
       "      <td>80</td>\n",
       "      <td>10</td>\n",
       "      <td>11</td>\n",
       "      <td>400</td>\n",
       "      <td>0.00100</td>\n",
       "      <td>{0: 1, 1: 15}</td>\n",
       "      <td>1</td>\n",
       "      <td>[{'filters': 64, 'kernel_size': (3, 3), 'layer...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[{'units': 23, 'activation': 'relu', 'num_laye...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>305</th>\n",
       "      <td>306</td>\n",
       "      <td>True</td>\n",
       "      <td>80</td>\n",
       "      <td>10</td>\n",
       "      <td>11</td>\n",
       "      <td>400</td>\n",
       "      <td>0.00100</td>\n",
       "      <td>{0: 1, 1: 20}</td>\n",
       "      <td>1</td>\n",
       "      <td>[{'filters': 64, 'kernel_size': (1, 23), 'laye...</td>\n",
       "      <td>[{'units': 221, 'activation': 'relu', 'num_lay...</td>\n",
       "      <td>[{'units': 23, 'activation': 'relu', 'num_laye...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>306</th>\n",
       "      <td>307</td>\n",
       "      <td>True</td>\n",
       "      <td>80</td>\n",
       "      <td>10</td>\n",
       "      <td>11</td>\n",
       "      <td>400</td>\n",
       "      <td>0.00100</td>\n",
       "      <td>{0: 1, 1: 20}</td>\n",
       "      <td>1</td>\n",
       "      <td>[{'filters': 64, 'kernel_size': (3, 3), 'layer...</td>\n",
       "      <td>[{'units': 221, 'activation': 'relu', 'num_lay...</td>\n",
       "      <td>[{'units': 23, 'activation': 'relu', 'num_laye...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>307</th>\n",
       "      <td>308</td>\n",
       "      <td>False</td>\n",
       "      <td>80</td>\n",
       "      <td>10</td>\n",
       "      <td>11</td>\n",
       "      <td>400</td>\n",
       "      <td>0.00100</td>\n",
       "      <td>{0: 1, 1: 20}</td>\n",
       "      <td>1</td>\n",
       "      <td>[{'filters': 64, 'kernel_size': (3, 3), 'layer...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[{'units': 23, 'activation': 'relu', 'num_laye...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>308 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     experiment_num  valid  num_mels  win_len  chunk_radius  batch_size  \\\n",
       "0                 1   True        80       10             3         400   \n",
       "1                 2  False        80       10             3         400   \n",
       "2                 3  False        80       10             3         400   \n",
       "3                 4   True        80       10             3         400   \n",
       "4                 5  False        80       10             3         400   \n",
       "5                 6  False        80       10             3         400   \n",
       "6                 7   True        80       10             3         400   \n",
       "7                 8  False        80       10             3         400   \n",
       "8                 9  False        80       10             3         400   \n",
       "9                10   True        80       10             3         400   \n",
       "10               11  False        80       10             3         400   \n",
       "11               12  False        80       10             3         400   \n",
       "12               13   True        80       10             3         400   \n",
       "13               14  False        80       10             3         400   \n",
       "14               15  False        80       10             3         400   \n",
       "15               16   True        80       10             3         400   \n",
       "16               17  False        80       10             3         400   \n",
       "17               18  False        80       10             3         400   \n",
       "18               19   True        80       10             3         400   \n",
       "19               20  False        80       10             3         400   \n",
       "20               21  False        80       10             3         400   \n",
       "21               22   True        80       10             3         400   \n",
       "22               23  False        80       10             3         400   \n",
       "23               24  False        80       10             3         400   \n",
       "24               25   True        80       10             3         400   \n",
       "25               26  False        80       10             3         400   \n",
       "26               27  False        80       10             3         400   \n",
       "27               28   True        80       10             5         400   \n",
       "28               29   True        80       10             5         400   \n",
       "29               30  False        80       10             5         400   \n",
       "..              ...    ...       ...      ...           ...         ...   \n",
       "278             279  False        80       10             9         400   \n",
       "279             280  False        80       10             9         400   \n",
       "280             281  False        80       10             9         400   \n",
       "281             282  False        80       10             9         400   \n",
       "282             283  False        80       10             9         400   \n",
       "283             284  False        80       10             9         400   \n",
       "284             285  False        80       10            11         400   \n",
       "285             286   True        80       10            11         400   \n",
       "286             287  False        80       10            11         400   \n",
       "287             288   True        80       10            11         400   \n",
       "288             289   True        80       10            11         400   \n",
       "289             290  False        80       10            11         400   \n",
       "290             291   True        80       10            11         400   \n",
       "291             292   True        80       10            11         400   \n",
       "292             293  False        80       10            11         400   \n",
       "293             294   True        80       10            11         400   \n",
       "294             295   True        80       10            11         400   \n",
       "295             296  False        80       10            11         400   \n",
       "296             297   True        80       10            11         400   \n",
       "297             298   True        80       10            11         400   \n",
       "298             299  False        80       10            11         400   \n",
       "299             300   True        80       10            11         400   \n",
       "300             301   True        80       10            11         400   \n",
       "301             302  False        80       10            11         400   \n",
       "302             303   True        80       10            11         400   \n",
       "303             304   True        80       10            11         400   \n",
       "304             305  False        80       10            11         400   \n",
       "305             306   True        80       10            11         400   \n",
       "306             307   True        80       10            11         400   \n",
       "307             308  False        80       10            11         400   \n",
       "\n",
       "     learning_rate note_class_weight  model_version  \\\n",
       "0          0.00001     {0: 1, 1: 10}              1   \n",
       "1          0.00001     {0: 1, 1: 10}              1   \n",
       "2          0.00001     {0: 1, 1: 10}              1   \n",
       "3          0.00001     {0: 1, 1: 15}              1   \n",
       "4          0.00001     {0: 1, 1: 15}              1   \n",
       "5          0.00001     {0: 1, 1: 15}              1   \n",
       "6          0.00001     {0: 1, 1: 20}              1   \n",
       "7          0.00001     {0: 1, 1: 20}              1   \n",
       "8          0.00001     {0: 1, 1: 20}              1   \n",
       "9          0.00010     {0: 1, 1: 10}              1   \n",
       "10         0.00010     {0: 1, 1: 10}              1   \n",
       "11         0.00010     {0: 1, 1: 10}              1   \n",
       "12         0.00010     {0: 1, 1: 15}              1   \n",
       "13         0.00010     {0: 1, 1: 15}              1   \n",
       "14         0.00010     {0: 1, 1: 15}              1   \n",
       "15         0.00010     {0: 1, 1: 20}              1   \n",
       "16         0.00010     {0: 1, 1: 20}              1   \n",
       "17         0.00010     {0: 1, 1: 20}              1   \n",
       "18         0.00100     {0: 1, 1: 10}              1   \n",
       "19         0.00100     {0: 1, 1: 10}              1   \n",
       "20         0.00100     {0: 1, 1: 10}              1   \n",
       "21         0.00100     {0: 1, 1: 15}              1   \n",
       "22         0.00100     {0: 1, 1: 15}              1   \n",
       "23         0.00100     {0: 1, 1: 15}              1   \n",
       "24         0.00100     {0: 1, 1: 20}              1   \n",
       "25         0.00100     {0: 1, 1: 20}              1   \n",
       "26         0.00100     {0: 1, 1: 20}              1   \n",
       "27         0.00001     {0: 1, 1: 10}              1   \n",
       "28         0.00001     {0: 1, 1: 10}              1   \n",
       "29         0.00001     {0: 1, 1: 10}              1   \n",
       "..             ...               ...            ...   \n",
       "278        0.00010     {0: 1, 1: 10}              1   \n",
       "279        0.00010     {0: 1, 1: 15}              1   \n",
       "280        0.00010     {0: 1, 1: 20}              1   \n",
       "281        0.00100     {0: 1, 1: 10}              1   \n",
       "282        0.00100     {0: 1, 1: 15}              1   \n",
       "283        0.00100     {0: 1, 1: 20}              1   \n",
       "284        0.00001     {0: 1, 1: 10}              1   \n",
       "285        0.00001     {0: 1, 1: 15}              1   \n",
       "286        0.00001     {0: 1, 1: 15}              1   \n",
       "287        0.00001     {0: 1, 1: 20}              1   \n",
       "288        0.00001     {0: 1, 1: 20}              1   \n",
       "289        0.00001     {0: 1, 1: 20}              1   \n",
       "290        0.00010     {0: 1, 1: 10}              1   \n",
       "291        0.00010     {0: 1, 1: 10}              1   \n",
       "292        0.00010     {0: 1, 1: 10}              1   \n",
       "293        0.00010     {0: 1, 1: 15}              1   \n",
       "294        0.00010     {0: 1, 1: 15}              1   \n",
       "295        0.00010     {0: 1, 1: 15}              1   \n",
       "296        0.00010     {0: 1, 1: 20}              1   \n",
       "297        0.00010     {0: 1, 1: 20}              1   \n",
       "298        0.00010     {0: 1, 1: 20}              1   \n",
       "299        0.00100     {0: 1, 1: 10}              1   \n",
       "300        0.00100     {0: 1, 1: 10}              1   \n",
       "301        0.00100     {0: 1, 1: 10}              1   \n",
       "302        0.00100     {0: 1, 1: 15}              1   \n",
       "303        0.00100     {0: 1, 1: 15}              1   \n",
       "304        0.00100     {0: 1, 1: 15}              1   \n",
       "305        0.00100     {0: 1, 1: 20}              1   \n",
       "306        0.00100     {0: 1, 1: 20}              1   \n",
       "307        0.00100     {0: 1, 1: 20}              1   \n",
       "\n",
       "                                      spec_conv_layers  \\\n",
       "0    [{'filters': 64, 'kernel_size': (1, 7), 'layer...   \n",
       "1    [{'filters': 64, 'kernel_size': (3, 3), 'layer...   \n",
       "2    [{'filters': 64, 'kernel_size': (3, 3), 'layer...   \n",
       "3    [{'filters': 64, 'kernel_size': (1, 7), 'layer...   \n",
       "4    [{'filters': 64, 'kernel_size': (3, 3), 'layer...   \n",
       "5    [{'filters': 64, 'kernel_size': (3, 3), 'layer...   \n",
       "6    [{'filters': 64, 'kernel_size': (1, 7), 'layer...   \n",
       "7    [{'filters': 64, 'kernel_size': (3, 3), 'layer...   \n",
       "8    [{'filters': 64, 'kernel_size': (3, 3), 'layer...   \n",
       "9    [{'filters': 64, 'kernel_size': (1, 7), 'layer...   \n",
       "10   [{'filters': 64, 'kernel_size': (3, 3), 'layer...   \n",
       "11   [{'filters': 64, 'kernel_size': (3, 3), 'layer...   \n",
       "12   [{'filters': 64, 'kernel_size': (1, 7), 'layer...   \n",
       "13   [{'filters': 64, 'kernel_size': (3, 3), 'layer...   \n",
       "14   [{'filters': 64, 'kernel_size': (3, 3), 'layer...   \n",
       "15   [{'filters': 64, 'kernel_size': (1, 7), 'layer...   \n",
       "16   [{'filters': 64, 'kernel_size': (3, 3), 'layer...   \n",
       "17   [{'filters': 64, 'kernel_size': (3, 3), 'layer...   \n",
       "18   [{'filters': 64, 'kernel_size': (1, 7), 'layer...   \n",
       "19   [{'filters': 64, 'kernel_size': (3, 3), 'layer...   \n",
       "20   [{'filters': 64, 'kernel_size': (3, 3), 'layer...   \n",
       "21   [{'filters': 64, 'kernel_size': (1, 7), 'layer...   \n",
       "22   [{'filters': 64, 'kernel_size': (3, 3), 'layer...   \n",
       "23   [{'filters': 64, 'kernel_size': (3, 3), 'layer...   \n",
       "24   [{'filters': 64, 'kernel_size': (1, 7), 'layer...   \n",
       "25   [{'filters': 64, 'kernel_size': (3, 3), 'layer...   \n",
       "26   [{'filters': 64, 'kernel_size': (3, 3), 'layer...   \n",
       "27   [{'filters': 64, 'kernel_size': (1, 11), 'laye...   \n",
       "28   [{'filters': 64, 'kernel_size': (3, 3), 'layer...   \n",
       "29   [{'filters': 64, 'kernel_size': (3, 3), 'layer...   \n",
       "..                                                 ...   \n",
       "278  [{'filters': 64, 'kernel_size': (3, 3), 'layer...   \n",
       "279  [{'filters': 64, 'kernel_size': (3, 3), 'layer...   \n",
       "280  [{'filters': 64, 'kernel_size': (3, 3), 'layer...   \n",
       "281  [{'filters': 64, 'kernel_size': (3, 3), 'layer...   \n",
       "282  [{'filters': 64, 'kernel_size': (3, 3), 'layer...   \n",
       "283  [{'filters': 64, 'kernel_size': (3, 3), 'layer...   \n",
       "284  [{'filters': 64, 'kernel_size': (3, 3), 'layer...   \n",
       "285  [{'filters': 64, 'kernel_size': (3, 3), 'layer...   \n",
       "286  [{'filters': 64, 'kernel_size': (3, 3), 'layer...   \n",
       "287  [{'filters': 64, 'kernel_size': (1, 23), 'laye...   \n",
       "288  [{'filters': 64, 'kernel_size': (3, 3), 'layer...   \n",
       "289  [{'filters': 64, 'kernel_size': (3, 3), 'layer...   \n",
       "290  [{'filters': 64, 'kernel_size': (1, 23), 'laye...   \n",
       "291  [{'filters': 64, 'kernel_size': (3, 3), 'layer...   \n",
       "292  [{'filters': 64, 'kernel_size': (3, 3), 'layer...   \n",
       "293  [{'filters': 64, 'kernel_size': (1, 23), 'laye...   \n",
       "294  [{'filters': 64, 'kernel_size': (3, 3), 'layer...   \n",
       "295  [{'filters': 64, 'kernel_size': (3, 3), 'layer...   \n",
       "296  [{'filters': 64, 'kernel_size': (1, 23), 'laye...   \n",
       "297  [{'filters': 64, 'kernel_size': (3, 3), 'layer...   \n",
       "298  [{'filters': 64, 'kernel_size': (3, 3), 'layer...   \n",
       "299  [{'filters': 64, 'kernel_size': (1, 23), 'laye...   \n",
       "300  [{'filters': 64, 'kernel_size': (3, 3), 'layer...   \n",
       "301  [{'filters': 64, 'kernel_size': (3, 3), 'layer...   \n",
       "302  [{'filters': 64, 'kernel_size': (1, 23), 'laye...   \n",
       "303  [{'filters': 64, 'kernel_size': (3, 3), 'layer...   \n",
       "304  [{'filters': 64, 'kernel_size': (3, 3), 'layer...   \n",
       "305  [{'filters': 64, 'kernel_size': (1, 23), 'laye...   \n",
       "306  [{'filters': 64, 'kernel_size': (3, 3), 'layer...   \n",
       "307  [{'filters': 64, 'kernel_size': (3, 3), 'layer...   \n",
       "\n",
       "                                     spec_dense_layers  \\\n",
       "0    [{'units': 61, 'activation': 'relu', 'num_laye...   \n",
       "1    [{'units': 61, 'activation': 'relu', 'num_laye...   \n",
       "2                                                   []   \n",
       "3    [{'units': 61, 'activation': 'relu', 'num_laye...   \n",
       "4    [{'units': 61, 'activation': 'relu', 'num_laye...   \n",
       "5                                                   []   \n",
       "6    [{'units': 61, 'activation': 'relu', 'num_laye...   \n",
       "7    [{'units': 61, 'activation': 'relu', 'num_laye...   \n",
       "8                                                   []   \n",
       "9    [{'units': 61, 'activation': 'relu', 'num_laye...   \n",
       "10   [{'units': 61, 'activation': 'relu', 'num_laye...   \n",
       "11                                                  []   \n",
       "12   [{'units': 61, 'activation': 'relu', 'num_laye...   \n",
       "13   [{'units': 61, 'activation': 'relu', 'num_laye...   \n",
       "14                                                  []   \n",
       "15   [{'units': 61, 'activation': 'relu', 'num_laye...   \n",
       "16   [{'units': 61, 'activation': 'relu', 'num_laye...   \n",
       "17                                                  []   \n",
       "18   [{'units': 61, 'activation': 'relu', 'num_laye...   \n",
       "19   [{'units': 61, 'activation': 'relu', 'num_laye...   \n",
       "20                                                  []   \n",
       "21   [{'units': 61, 'activation': 'relu', 'num_laye...   \n",
       "22   [{'units': 61, 'activation': 'relu', 'num_laye...   \n",
       "23                                                  []   \n",
       "24   [{'units': 61, 'activation': 'relu', 'num_laye...   \n",
       "25   [{'units': 61, 'activation': 'relu', 'num_laye...   \n",
       "26                                                  []   \n",
       "27   [{'units': 101, 'activation': 'relu', 'num_lay...   \n",
       "28   [{'units': 101, 'activation': 'relu', 'num_lay...   \n",
       "29                                                  []   \n",
       "..                                                 ...   \n",
       "278                                                 []   \n",
       "279                                                 []   \n",
       "280                                                 []   \n",
       "281                                                 []   \n",
       "282                                                 []   \n",
       "283                                                 []   \n",
       "284                                                 []   \n",
       "285  [{'units': 221, 'activation': 'relu', 'num_lay...   \n",
       "286                                                 []   \n",
       "287  [{'units': 221, 'activation': 'relu', 'num_lay...   \n",
       "288  [{'units': 221, 'activation': 'relu', 'num_lay...   \n",
       "289                                                 []   \n",
       "290  [{'units': 221, 'activation': 'relu', 'num_lay...   \n",
       "291  [{'units': 221, 'activation': 'relu', 'num_lay...   \n",
       "292                                                 []   \n",
       "293  [{'units': 221, 'activation': 'relu', 'num_lay...   \n",
       "294  [{'units': 221, 'activation': 'relu', 'num_lay...   \n",
       "295                                                 []   \n",
       "296  [{'units': 221, 'activation': 'relu', 'num_lay...   \n",
       "297  [{'units': 221, 'activation': 'relu', 'num_lay...   \n",
       "298                                                 []   \n",
       "299  [{'units': 221, 'activation': 'relu', 'num_lay...   \n",
       "300  [{'units': 221, 'activation': 'relu', 'num_lay...   \n",
       "301                                                 []   \n",
       "302  [{'units': 221, 'activation': 'relu', 'num_lay...   \n",
       "303  [{'units': 221, 'activation': 'relu', 'num_lay...   \n",
       "304                                                 []   \n",
       "305  [{'units': 221, 'activation': 'relu', 'num_lay...   \n",
       "306  [{'units': 221, 'activation': 'relu', 'num_lay...   \n",
       "307                                                 []   \n",
       "\n",
       "                                       combined_layers  \n",
       "0    [{'units': 7, 'activation': 'relu', 'num_layer...  \n",
       "1    [{'units': 7, 'activation': 'relu', 'num_layer...  \n",
       "2    [{'units': 7, 'activation': 'relu', 'num_layer...  \n",
       "3    [{'units': 7, 'activation': 'relu', 'num_layer...  \n",
       "4    [{'units': 7, 'activation': 'relu', 'num_layer...  \n",
       "5    [{'units': 7, 'activation': 'relu', 'num_layer...  \n",
       "6    [{'units': 7, 'activation': 'relu', 'num_layer...  \n",
       "7    [{'units': 7, 'activation': 'relu', 'num_layer...  \n",
       "8    [{'units': 7, 'activation': 'relu', 'num_layer...  \n",
       "9    [{'units': 7, 'activation': 'relu', 'num_layer...  \n",
       "10   [{'units': 7, 'activation': 'relu', 'num_layer...  \n",
       "11   [{'units': 7, 'activation': 'relu', 'num_layer...  \n",
       "12   [{'units': 7, 'activation': 'relu', 'num_layer...  \n",
       "13   [{'units': 7, 'activation': 'relu', 'num_layer...  \n",
       "14   [{'units': 7, 'activation': 'relu', 'num_layer...  \n",
       "15   [{'units': 7, 'activation': 'relu', 'num_layer...  \n",
       "16   [{'units': 7, 'activation': 'relu', 'num_layer...  \n",
       "17   [{'units': 7, 'activation': 'relu', 'num_layer...  \n",
       "18   [{'units': 7, 'activation': 'relu', 'num_layer...  \n",
       "19   [{'units': 7, 'activation': 'relu', 'num_layer...  \n",
       "20   [{'units': 7, 'activation': 'relu', 'num_layer...  \n",
       "21   [{'units': 7, 'activation': 'relu', 'num_layer...  \n",
       "22   [{'units': 7, 'activation': 'relu', 'num_layer...  \n",
       "23   [{'units': 7, 'activation': 'relu', 'num_layer...  \n",
       "24   [{'units': 7, 'activation': 'relu', 'num_layer...  \n",
       "25   [{'units': 7, 'activation': 'relu', 'num_layer...  \n",
       "26   [{'units': 7, 'activation': 'relu', 'num_layer...  \n",
       "27   [{'units': 11, 'activation': 'relu', 'num_laye...  \n",
       "28   [{'units': 11, 'activation': 'relu', 'num_laye...  \n",
       "29   [{'units': 11, 'activation': 'relu', 'num_laye...  \n",
       "..                                                 ...  \n",
       "278  [{'units': 19, 'activation': 'relu', 'num_laye...  \n",
       "279  [{'units': 19, 'activation': 'relu', 'num_laye...  \n",
       "280  [{'units': 19, 'activation': 'relu', 'num_laye...  \n",
       "281  [{'units': 19, 'activation': 'relu', 'num_laye...  \n",
       "282  [{'units': 19, 'activation': 'relu', 'num_laye...  \n",
       "283  [{'units': 19, 'activation': 'relu', 'num_laye...  \n",
       "284  [{'units': 23, 'activation': 'relu', 'num_laye...  \n",
       "285  [{'units': 23, 'activation': 'relu', 'num_laye...  \n",
       "286  [{'units': 23, 'activation': 'relu', 'num_laye...  \n",
       "287  [{'units': 23, 'activation': 'relu', 'num_laye...  \n",
       "288  [{'units': 23, 'activation': 'relu', 'num_laye...  \n",
       "289  [{'units': 23, 'activation': 'relu', 'num_laye...  \n",
       "290  [{'units': 23, 'activation': 'relu', 'num_laye...  \n",
       "291  [{'units': 23, 'activation': 'relu', 'num_laye...  \n",
       "292  [{'units': 23, 'activation': 'relu', 'num_laye...  \n",
       "293  [{'units': 23, 'activation': 'relu', 'num_laye...  \n",
       "294  [{'units': 23, 'activation': 'relu', 'num_laye...  \n",
       "295  [{'units': 23, 'activation': 'relu', 'num_laye...  \n",
       "296  [{'units': 23, 'activation': 'relu', 'num_laye...  \n",
       "297  [{'units': 23, 'activation': 'relu', 'num_laye...  \n",
       "298  [{'units': 23, 'activation': 'relu', 'num_laye...  \n",
       "299  [{'units': 23, 'activation': 'relu', 'num_laye...  \n",
       "300  [{'units': 23, 'activation': 'relu', 'num_laye...  \n",
       "301  [{'units': 23, 'activation': 'relu', 'num_laye...  \n",
       "302  [{'units': 23, 'activation': 'relu', 'num_laye...  \n",
       "303  [{'units': 23, 'activation': 'relu', 'num_laye...  \n",
       "304  [{'units': 23, 'activation': 'relu', 'num_laye...  \n",
       "305  [{'units': 23, 'activation': 'relu', 'num_laye...  \n",
       "306  [{'units': 23, 'activation': 'relu', 'num_laye...  \n",
       "307  [{'units': 23, 'activation': 'relu', 'num_laye...  \n",
       "\n",
       "[308 rows x 12 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary_df, metric_df = get_experiment_dfs(model_folder)\n",
    "summary_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>val_loss</th>\n",
       "      <th>val_acc</th>\n",
       "      <th>val_precision</th>\n",
       "      <th>val_recall</th>\n",
       "      <th>loss</th>\n",
       "      <th>acc</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>epoch_num</th>\n",
       "      <th>experiment_num</th>\n",
       "      <th>val_acc_val_prec</th>\n",
       "      <th>val_acc_val_prec_val_recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>0.274599</td>\n",
       "      <td>0.922604</td>\n",
       "      <td>0.160080</td>\n",
       "      <td>0.094276</td>\n",
       "      <td>0.906488</td>\n",
       "      <td>0.830758</td>\n",
       "      <td>0.213530</td>\n",
       "      <td>0.572336</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>1.082683</td>\n",
       "      <td>1.176960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1664</th>\n",
       "      <td>0.278654</td>\n",
       "      <td>0.912960</td>\n",
       "      <td>0.252427</td>\n",
       "      <td>0.293076</td>\n",
       "      <td>0.858906</td>\n",
       "      <td>0.844002</td>\n",
       "      <td>0.236435</td>\n",
       "      <td>0.599799</td>\n",
       "      <td>15</td>\n",
       "      <td>158</td>\n",
       "      <td>1.165387</td>\n",
       "      <td>1.458463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>328</th>\n",
       "      <td>0.281642</td>\n",
       "      <td>0.917652</td>\n",
       "      <td>0.227487</td>\n",
       "      <td>0.204509</td>\n",
       "      <td>0.840107</td>\n",
       "      <td>0.840533</td>\n",
       "      <td>0.228663</td>\n",
       "      <td>0.585371</td>\n",
       "      <td>29</td>\n",
       "      <td>19</td>\n",
       "      <td>1.145140</td>\n",
       "      <td>1.349649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3510</th>\n",
       "      <td>0.287094</td>\n",
       "      <td>0.897248</td>\n",
       "      <td>0.207340</td>\n",
       "      <td>0.304348</td>\n",
       "      <td>0.827327</td>\n",
       "      <td>0.854692</td>\n",
       "      <td>0.252228</td>\n",
       "      <td>0.599131</td>\n",
       "      <td>11</td>\n",
       "      <td>292</td>\n",
       "      <td>1.104588</td>\n",
       "      <td>1.408936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3519</th>\n",
       "      <td>0.287145</td>\n",
       "      <td>0.902304</td>\n",
       "      <td>0.205658</td>\n",
       "      <td>0.268189</td>\n",
       "      <td>0.790889</td>\n",
       "      <td>0.864024</td>\n",
       "      <td>0.269589</td>\n",
       "      <td>0.606819</td>\n",
       "      <td>20</td>\n",
       "      <td>292</td>\n",
       "      <td>1.107962</td>\n",
       "      <td>1.376151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3470</th>\n",
       "      <td>0.288091</td>\n",
       "      <td>0.905274</td>\n",
       "      <td>0.168186</td>\n",
       "      <td>0.180940</td>\n",
       "      <td>0.846915</td>\n",
       "      <td>0.847672</td>\n",
       "      <td>0.236334</td>\n",
       "      <td>0.574676</td>\n",
       "      <td>21</td>\n",
       "      <td>291</td>\n",
       "      <td>1.073460</td>\n",
       "      <td>1.254400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3486</th>\n",
       "      <td>0.294346</td>\n",
       "      <td>0.890209</td>\n",
       "      <td>0.190446</td>\n",
       "      <td>0.303469</td>\n",
       "      <td>0.745943</td>\n",
       "      <td>0.859094</td>\n",
       "      <td>0.267247</td>\n",
       "      <td>0.637903</td>\n",
       "      <td>37</td>\n",
       "      <td>291</td>\n",
       "      <td>1.080654</td>\n",
       "      <td>1.384124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1141</th>\n",
       "      <td>0.306713</td>\n",
       "      <td>0.890516</td>\n",
       "      <td>0.160365</td>\n",
       "      <td>0.231591</td>\n",
       "      <td>0.728936</td>\n",
       "      <td>0.844221</td>\n",
       "      <td>0.249309</td>\n",
       "      <td>0.663306</td>\n",
       "      <td>42</td>\n",
       "      <td>87</td>\n",
       "      <td>1.050881</td>\n",
       "      <td>1.282472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3596</th>\n",
       "      <td>0.306795</td>\n",
       "      <td>0.882984</td>\n",
       "      <td>0.170352</td>\n",
       "      <td>0.288684</td>\n",
       "      <td>0.887864</td>\n",
       "      <td>0.762668</td>\n",
       "      <td>0.189690</td>\n",
       "      <td>0.781182</td>\n",
       "      <td>47</td>\n",
       "      <td>294</td>\n",
       "      <td>1.053336</td>\n",
       "      <td>1.342020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187</th>\n",
       "      <td>0.307306</td>\n",
       "      <td>0.893534</td>\n",
       "      <td>0.153678</td>\n",
       "      <td>0.205534</td>\n",
       "      <td>0.822088</td>\n",
       "      <td>0.852862</td>\n",
       "      <td>0.248007</td>\n",
       "      <td>0.592725</td>\n",
       "      <td>38</td>\n",
       "      <td>10</td>\n",
       "      <td>1.047212</td>\n",
       "      <td>1.252745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>310</th>\n",
       "      <td>0.307459</td>\n",
       "      <td>0.909173</td>\n",
       "      <td>0.183220</td>\n",
       "      <td>0.186064</td>\n",
       "      <td>0.895736</td>\n",
       "      <td>0.838480</td>\n",
       "      <td>0.212484</td>\n",
       "      <td>0.524706</td>\n",
       "      <td>11</td>\n",
       "      <td>19</td>\n",
       "      <td>1.092394</td>\n",
       "      <td>1.278457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>780</th>\n",
       "      <td>0.308464</td>\n",
       "      <td>0.897078</td>\n",
       "      <td>0.177931</td>\n",
       "      <td>0.238179</td>\n",
       "      <td>0.822276</td>\n",
       "      <td>0.865879</td>\n",
       "      <td>0.268658</td>\n",
       "      <td>0.586151</td>\n",
       "      <td>31</td>\n",
       "      <td>77</td>\n",
       "      <td>1.075009</td>\n",
       "      <td>1.313187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3595</th>\n",
       "      <td>0.309137</td>\n",
       "      <td>0.879286</td>\n",
       "      <td>0.178574</td>\n",
       "      <td>0.328942</td>\n",
       "      <td>0.878430</td>\n",
       "      <td>0.770494</td>\n",
       "      <td>0.195555</td>\n",
       "      <td>0.783188</td>\n",
       "      <td>46</td>\n",
       "      <td>294</td>\n",
       "      <td>1.057860</td>\n",
       "      <td>1.386802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2636</th>\n",
       "      <td>0.310689</td>\n",
       "      <td>0.898769</td>\n",
       "      <td>0.202908</td>\n",
       "      <td>0.283999</td>\n",
       "      <td>0.735695</td>\n",
       "      <td>0.858205</td>\n",
       "      <td>0.269562</td>\n",
       "      <td>0.657679</td>\n",
       "      <td>37</td>\n",
       "      <td>219</td>\n",
       "      <td>1.101676</td>\n",
       "      <td>1.385676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>787</th>\n",
       "      <td>0.314505</td>\n",
       "      <td>0.890120</td>\n",
       "      <td>0.163644</td>\n",
       "      <td>0.240375</td>\n",
       "      <td>0.813574</td>\n",
       "      <td>0.860581</td>\n",
       "      <td>0.262318</td>\n",
       "      <td>0.600858</td>\n",
       "      <td>38</td>\n",
       "      <td>77</td>\n",
       "      <td>1.053763</td>\n",
       "      <td>1.294138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2646</th>\n",
       "      <td>0.316521</td>\n",
       "      <td>0.886268</td>\n",
       "      <td>0.188320</td>\n",
       "      <td>0.319573</td>\n",
       "      <td>0.693312</td>\n",
       "      <td>0.859198</td>\n",
       "      <td>0.276794</td>\n",
       "      <td>0.687984</td>\n",
       "      <td>47</td>\n",
       "      <td>219</td>\n",
       "      <td>1.074588</td>\n",
       "      <td>1.394160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2628</th>\n",
       "      <td>0.316556</td>\n",
       "      <td>0.900435</td>\n",
       "      <td>0.213321</td>\n",
       "      <td>0.298199</td>\n",
       "      <td>0.753931</td>\n",
       "      <td>0.865920</td>\n",
       "      <td>0.276110</td>\n",
       "      <td>0.622082</td>\n",
       "      <td>29</td>\n",
       "      <td>219</td>\n",
       "      <td>1.113756</td>\n",
       "      <td>1.411955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1119</th>\n",
       "      <td>0.319088</td>\n",
       "      <td>0.898688</td>\n",
       "      <td>0.158854</td>\n",
       "      <td>0.193969</td>\n",
       "      <td>0.819587</td>\n",
       "      <td>0.843842</td>\n",
       "      <td>0.237410</td>\n",
       "      <td>0.605593</td>\n",
       "      <td>20</td>\n",
       "      <td>87</td>\n",
       "      <td>1.057542</td>\n",
       "      <td>1.251510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3525</th>\n",
       "      <td>0.323106</td>\n",
       "      <td>0.886309</td>\n",
       "      <td>0.213793</td>\n",
       "      <td>0.394818</td>\n",
       "      <td>0.773398</td>\n",
       "      <td>0.867132</td>\n",
       "      <td>0.278850</td>\n",
       "      <td>0.624589</td>\n",
       "      <td>26</td>\n",
       "      <td>292</td>\n",
       "      <td>1.100102</td>\n",
       "      <td>1.494920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>819</th>\n",
       "      <td>0.325427</td>\n",
       "      <td>0.904019</td>\n",
       "      <td>0.217113</td>\n",
       "      <td>0.282682</td>\n",
       "      <td>0.833104</td>\n",
       "      <td>0.864693</td>\n",
       "      <td>0.265347</td>\n",
       "      <td>0.580803</td>\n",
       "      <td>20</td>\n",
       "      <td>78</td>\n",
       "      <td>1.121132</td>\n",
       "      <td>1.403814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3520</th>\n",
       "      <td>0.325748</td>\n",
       "      <td>0.889836</td>\n",
       "      <td>0.178710</td>\n",
       "      <td>0.276241</td>\n",
       "      <td>0.813793</td>\n",
       "      <td>0.855171</td>\n",
       "      <td>0.253075</td>\n",
       "      <td>0.599521</td>\n",
       "      <td>21</td>\n",
       "      <td>292</td>\n",
       "      <td>1.068547</td>\n",
       "      <td>1.344787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>0.325763</td>\n",
       "      <td>0.901762</td>\n",
       "      <td>0.196757</td>\n",
       "      <td>0.252232</td>\n",
       "      <td>0.783817</td>\n",
       "      <td>0.863050</td>\n",
       "      <td>0.268205</td>\n",
       "      <td>0.608545</td>\n",
       "      <td>46</td>\n",
       "      <td>10</td>\n",
       "      <td>1.098519</td>\n",
       "      <td>1.350752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>838</th>\n",
       "      <td>0.326221</td>\n",
       "      <td>0.881098</td>\n",
       "      <td>0.144085</td>\n",
       "      <td>0.233055</td>\n",
       "      <td>0.815915</td>\n",
       "      <td>0.846162</td>\n",
       "      <td>0.239339</td>\n",
       "      <td>0.599075</td>\n",
       "      <td>39</td>\n",
       "      <td>78</td>\n",
       "      <td>1.025184</td>\n",
       "      <td>1.258239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2631</th>\n",
       "      <td>0.326272</td>\n",
       "      <td>0.882992</td>\n",
       "      <td>0.165101</td>\n",
       "      <td>0.275362</td>\n",
       "      <td>0.811624</td>\n",
       "      <td>0.838559</td>\n",
       "      <td>0.232317</td>\n",
       "      <td>0.615676</td>\n",
       "      <td>32</td>\n",
       "      <td>219</td>\n",
       "      <td>1.048093</td>\n",
       "      <td>1.323455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3469</th>\n",
       "      <td>0.327180</td>\n",
       "      <td>0.899076</td>\n",
       "      <td>0.197361</td>\n",
       "      <td>0.269360</td>\n",
       "      <td>0.824534</td>\n",
       "      <td>0.854558</td>\n",
       "      <td>0.250930</td>\n",
       "      <td>0.593950</td>\n",
       "      <td>20</td>\n",
       "      <td>291</td>\n",
       "      <td>1.096437</td>\n",
       "      <td>1.365798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1076</th>\n",
       "      <td>0.327433</td>\n",
       "      <td>0.885322</td>\n",
       "      <td>0.110616</td>\n",
       "      <td>0.152686</td>\n",
       "      <td>0.988253</td>\n",
       "      <td>0.767921</td>\n",
       "      <td>0.139961</td>\n",
       "      <td>0.481477</td>\n",
       "      <td>27</td>\n",
       "      <td>86</td>\n",
       "      <td>0.995938</td>\n",
       "      <td>1.148624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3462</th>\n",
       "      <td>0.329166</td>\n",
       "      <td>0.902078</td>\n",
       "      <td>0.191840</td>\n",
       "      <td>0.240228</td>\n",
       "      <td>0.848172</td>\n",
       "      <td>0.851955</td>\n",
       "      <td>0.244973</td>\n",
       "      <td>0.585037</td>\n",
       "      <td>13</td>\n",
       "      <td>291</td>\n",
       "      <td>1.093918</td>\n",
       "      <td>1.334146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1149</th>\n",
       "      <td>0.329283</td>\n",
       "      <td>0.892547</td>\n",
       "      <td>0.100074</td>\n",
       "      <td>0.118138</td>\n",
       "      <td>0.812741</td>\n",
       "      <td>0.818897</td>\n",
       "      <td>0.210053</td>\n",
       "      <td>0.620634</td>\n",
       "      <td>50</td>\n",
       "      <td>87</td>\n",
       "      <td>0.992621</td>\n",
       "      <td>1.110759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2312</th>\n",
       "      <td>0.329811</td>\n",
       "      <td>0.908219</td>\n",
       "      <td>0.120054</td>\n",
       "      <td>0.104377</td>\n",
       "      <td>0.929935</td>\n",
       "      <td>0.799247</td>\n",
       "      <td>0.182113</td>\n",
       "      <td>0.575121</td>\n",
       "      <td>13</td>\n",
       "      <td>210</td>\n",
       "      <td>1.028272</td>\n",
       "      <td>1.132650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1243</th>\n",
       "      <td>0.330586</td>\n",
       "      <td>0.856850</td>\n",
       "      <td>0.163089</td>\n",
       "      <td>0.384863</td>\n",
       "      <td>0.910626</td>\n",
       "      <td>0.763902</td>\n",
       "      <td>0.187865</td>\n",
       "      <td>0.763579</td>\n",
       "      <td>44</td>\n",
       "      <td>90</td>\n",
       "      <td>1.019940</td>\n",
       "      <td>1.404803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>422</th>\n",
       "      <td>0.881718</td>\n",
       "      <td>0.055268</td>\n",
       "      <td>0.055268</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.536624</td>\n",
       "      <td>0.066744</td>\n",
       "      <td>0.066744</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>23</td>\n",
       "      <td>25</td>\n",
       "      <td>0.110536</td>\n",
       "      <td>1.110536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3350</th>\n",
       "      <td>0.882934</td>\n",
       "      <td>0.055818</td>\n",
       "      <td>0.055298</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.544174</td>\n",
       "      <td>0.127963</td>\n",
       "      <td>0.066735</td>\n",
       "      <td>0.929196</td>\n",
       "      <td>1</td>\n",
       "      <td>288</td>\n",
       "      <td>0.111116</td>\n",
       "      <td>1.111116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2503</th>\n",
       "      <td>0.884872</td>\n",
       "      <td>0.263799</td>\n",
       "      <td>0.068347</td>\n",
       "      <td>0.975406</td>\n",
       "      <td>1.477424</td>\n",
       "      <td>0.123583</td>\n",
       "      <td>0.069616</td>\n",
       "      <td>0.981115</td>\n",
       "      <td>4</td>\n",
       "      <td>216</td>\n",
       "      <td>0.332146</td>\n",
       "      <td>1.307552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3367</th>\n",
       "      <td>0.885759</td>\n",
       "      <td>0.434158</td>\n",
       "      <td>0.076379</td>\n",
       "      <td>0.832821</td>\n",
       "      <td>1.392064</td>\n",
       "      <td>0.510541</td>\n",
       "      <td>0.098892</td>\n",
       "      <td>0.780736</td>\n",
       "      <td>18</td>\n",
       "      <td>288</td>\n",
       "      <td>0.510536</td>\n",
       "      <td>1.343357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3136</th>\n",
       "      <td>0.886526</td>\n",
       "      <td>0.220788</td>\n",
       "      <td>0.061821</td>\n",
       "      <td>0.924023</td>\n",
       "      <td>1.506139</td>\n",
       "      <td>0.176764</td>\n",
       "      <td>0.071537</td>\n",
       "      <td>0.946187</td>\n",
       "      <td>37</td>\n",
       "      <td>234</td>\n",
       "      <td>0.282609</td>\n",
       "      <td>1.206632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2708</th>\n",
       "      <td>0.886768</td>\n",
       "      <td>0.377741</td>\n",
       "      <td>0.072204</td>\n",
       "      <td>0.865759</td>\n",
       "      <td>1.212556</td>\n",
       "      <td>0.640274</td>\n",
       "      <td>0.115979</td>\n",
       "      <td>0.662860</td>\n",
       "      <td>9</td>\n",
       "      <td>222</td>\n",
       "      <td>0.449945</td>\n",
       "      <td>1.315704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2454</th>\n",
       "      <td>0.887895</td>\n",
       "      <td>0.063933</td>\n",
       "      <td>0.055751</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.518696</td>\n",
       "      <td>0.070540</td>\n",
       "      <td>0.066934</td>\n",
       "      <td>0.998886</td>\n",
       "      <td>5</td>\n",
       "      <td>215</td>\n",
       "      <td>0.119684</td>\n",
       "      <td>1.119684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3138</th>\n",
       "      <td>0.888698</td>\n",
       "      <td>0.121240</td>\n",
       "      <td>0.058552</td>\n",
       "      <td>0.988142</td>\n",
       "      <td>1.507178</td>\n",
       "      <td>0.175786</td>\n",
       "      <td>0.071476</td>\n",
       "      <td>0.946465</td>\n",
       "      <td>39</td>\n",
       "      <td>234</td>\n",
       "      <td>0.179792</td>\n",
       "      <td>1.167934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3957</th>\n",
       "      <td>0.889455</td>\n",
       "      <td>0.518997</td>\n",
       "      <td>0.086139</td>\n",
       "      <td>0.801640</td>\n",
       "      <td>1.234450</td>\n",
       "      <td>0.680642</td>\n",
       "      <td>0.141779</td>\n",
       "      <td>0.748983</td>\n",
       "      <td>8</td>\n",
       "      <td>306</td>\n",
       "      <td>0.605136</td>\n",
       "      <td>1.406775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1600</th>\n",
       "      <td>0.889553</td>\n",
       "      <td>0.055268</td>\n",
       "      <td>0.055268</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.541428</td>\n",
       "      <td>0.071593</td>\n",
       "      <td>0.066878</td>\n",
       "      <td>0.996713</td>\n",
       "      <td>1</td>\n",
       "      <td>156</td>\n",
       "      <td>0.110536</td>\n",
       "      <td>1.110536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>702</th>\n",
       "      <td>0.891146</td>\n",
       "      <td>0.055268</td>\n",
       "      <td>0.055268</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.527211</td>\n",
       "      <td>0.066744</td>\n",
       "      <td>0.066744</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3</td>\n",
       "      <td>75</td>\n",
       "      <td>0.110536</td>\n",
       "      <td>1.110536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2483</th>\n",
       "      <td>0.898747</td>\n",
       "      <td>0.461998</td>\n",
       "      <td>0.079083</td>\n",
       "      <td>0.820524</td>\n",
       "      <td>1.353188</td>\n",
       "      <td>0.581267</td>\n",
       "      <td>0.111073</td>\n",
       "      <td>0.753050</td>\n",
       "      <td>34</td>\n",
       "      <td>215</td>\n",
       "      <td>0.541081</td>\n",
       "      <td>1.361605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3116</th>\n",
       "      <td>0.899480</td>\n",
       "      <td>0.111798</td>\n",
       "      <td>0.057943</td>\n",
       "      <td>0.987703</td>\n",
       "      <td>1.508829</td>\n",
       "      <td>0.177333</td>\n",
       "      <td>0.071543</td>\n",
       "      <td>0.945574</td>\n",
       "      <td>17</td>\n",
       "      <td>234</td>\n",
       "      <td>0.169740</td>\n",
       "      <td>1.157444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1017</th>\n",
       "      <td>0.899541</td>\n",
       "      <td>0.449020</td>\n",
       "      <td>0.076269</td>\n",
       "      <td>0.807202</td>\n",
       "      <td>1.404596</td>\n",
       "      <td>0.500881</td>\n",
       "      <td>0.096586</td>\n",
       "      <td>0.775500</td>\n",
       "      <td>18</td>\n",
       "      <td>84</td>\n",
       "      <td>0.525289</td>\n",
       "      <td>1.332491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2206</th>\n",
       "      <td>0.900295</td>\n",
       "      <td>0.188555</td>\n",
       "      <td>0.062722</td>\n",
       "      <td>0.981262</td>\n",
       "      <td>1.506379</td>\n",
       "      <td>0.237604</td>\n",
       "      <td>0.073828</td>\n",
       "      <td>0.902791</td>\n",
       "      <td>7</td>\n",
       "      <td>207</td>\n",
       "      <td>0.251277</td>\n",
       "      <td>1.232539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>655</th>\n",
       "      <td>0.901022</td>\n",
       "      <td>0.446625</td>\n",
       "      <td>0.076716</td>\n",
       "      <td>0.816718</td>\n",
       "      <td>1.354769</td>\n",
       "      <td>0.476568</td>\n",
       "      <td>0.100227</td>\n",
       "      <td>0.857724</td>\n",
       "      <td>6</td>\n",
       "      <td>34</td>\n",
       "      <td>0.523341</td>\n",
       "      <td>1.340059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1270</th>\n",
       "      <td>0.903290</td>\n",
       "      <td>0.548771</td>\n",
       "      <td>0.088380</td>\n",
       "      <td>0.769141</td>\n",
       "      <td>1.317304</td>\n",
       "      <td>0.637723</td>\n",
       "      <td>0.123377</td>\n",
       "      <td>0.725252</td>\n",
       "      <td>21</td>\n",
       "      <td>92</td>\n",
       "      <td>0.637151</td>\n",
       "      <td>1.406291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1626</th>\n",
       "      <td>0.904164</td>\n",
       "      <td>0.406236</td>\n",
       "      <td>0.074732</td>\n",
       "      <td>0.856097</td>\n",
       "      <td>1.438064</td>\n",
       "      <td>0.451274</td>\n",
       "      <td>0.089773</td>\n",
       "      <td>0.790151</td>\n",
       "      <td>27</td>\n",
       "      <td>156</td>\n",
       "      <td>0.480968</td>\n",
       "      <td>1.337066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2652</th>\n",
       "      <td>0.907290</td>\n",
       "      <td>0.520526</td>\n",
       "      <td>0.086382</td>\n",
       "      <td>0.801493</td>\n",
       "      <td>1.148861</td>\n",
       "      <td>0.727792</td>\n",
       "      <td>0.148688</td>\n",
       "      <td>0.651440</td>\n",
       "      <td>3</td>\n",
       "      <td>221</td>\n",
       "      <td>0.606909</td>\n",
       "      <td>1.408402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3995</th>\n",
       "      <td>0.920515</td>\n",
       "      <td>0.513981</td>\n",
       "      <td>0.083464</td>\n",
       "      <td>0.780852</td>\n",
       "      <td>1.150958</td>\n",
       "      <td>0.643104</td>\n",
       "      <td>0.136662</td>\n",
       "      <td>0.817559</td>\n",
       "      <td>46</td>\n",
       "      <td>306</td>\n",
       "      <td>0.597445</td>\n",
       "      <td>1.378297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3123</th>\n",
       "      <td>0.937926</td>\n",
       "      <td>0.104411</td>\n",
       "      <td>0.057650</td>\n",
       "      <td>0.990777</td>\n",
       "      <td>1.507189</td>\n",
       "      <td>0.171588</td>\n",
       "      <td>0.071249</td>\n",
       "      <td>0.948192</td>\n",
       "      <td>24</td>\n",
       "      <td>234</td>\n",
       "      <td>0.162061</td>\n",
       "      <td>1.152838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3551</th>\n",
       "      <td>0.940258</td>\n",
       "      <td>0.524928</td>\n",
       "      <td>0.085719</td>\n",
       "      <td>0.785829</td>\n",
       "      <td>1.115859</td>\n",
       "      <td>0.766047</td>\n",
       "      <td>0.169715</td>\n",
       "      <td>0.643641</td>\n",
       "      <td>2</td>\n",
       "      <td>294</td>\n",
       "      <td>0.610647</td>\n",
       "      <td>1.396476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3103</th>\n",
       "      <td>0.951587</td>\n",
       "      <td>0.116369</td>\n",
       "      <td>0.058111</td>\n",
       "      <td>0.985507</td>\n",
       "      <td>1.510998</td>\n",
       "      <td>0.186900</td>\n",
       "      <td>0.071656</td>\n",
       "      <td>0.935324</td>\n",
       "      <td>4</td>\n",
       "      <td>234</td>\n",
       "      <td>0.174480</td>\n",
       "      <td>1.159987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3951</th>\n",
       "      <td>0.957150</td>\n",
       "      <td>0.055268</td>\n",
       "      <td>0.055268</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.530666</td>\n",
       "      <td>0.079776</td>\n",
       "      <td>0.067074</td>\n",
       "      <td>0.990585</td>\n",
       "      <td>2</td>\n",
       "      <td>306</td>\n",
       "      <td>0.110536</td>\n",
       "      <td>1.110536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000</th>\n",
       "      <td>0.965840</td>\n",
       "      <td>0.084751</td>\n",
       "      <td>0.056555</td>\n",
       "      <td>0.992241</td>\n",
       "      <td>1.527532</td>\n",
       "      <td>0.087867</td>\n",
       "      <td>0.067350</td>\n",
       "      <td>0.985850</td>\n",
       "      <td>1</td>\n",
       "      <td>84</td>\n",
       "      <td>0.141306</td>\n",
       "      <td>1.133547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2202</th>\n",
       "      <td>0.974019</td>\n",
       "      <td>0.091199</td>\n",
       "      <td>0.056792</td>\n",
       "      <td>0.989460</td>\n",
       "      <td>1.517464</td>\n",
       "      <td>0.152600</td>\n",
       "      <td>0.069857</td>\n",
       "      <td>0.949752</td>\n",
       "      <td>3</td>\n",
       "      <td>207</td>\n",
       "      <td>0.147991</td>\n",
       "      <td>1.137451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2166</th>\n",
       "      <td>0.984650</td>\n",
       "      <td>0.464563</td>\n",
       "      <td>0.079116</td>\n",
       "      <td>0.816572</td>\n",
       "      <td>1.164080</td>\n",
       "      <td>0.671097</td>\n",
       "      <td>0.143625</td>\n",
       "      <td>0.791488</td>\n",
       "      <td>17</td>\n",
       "      <td>206</td>\n",
       "      <td>0.543679</td>\n",
       "      <td>1.360250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1633</th>\n",
       "      <td>1.058241</td>\n",
       "      <td>0.376980</td>\n",
       "      <td>0.073940</td>\n",
       "      <td>0.891378</td>\n",
       "      <td>1.395982</td>\n",
       "      <td>0.541848</td>\n",
       "      <td>0.101642</td>\n",
       "      <td>0.748148</td>\n",
       "      <td>34</td>\n",
       "      <td>156</td>\n",
       "      <td>0.450920</td>\n",
       "      <td>1.342297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2055</th>\n",
       "      <td>1.065086</td>\n",
       "      <td>0.443988</td>\n",
       "      <td>0.076802</td>\n",
       "      <td>0.822134</td>\n",
       "      <td>1.086487</td>\n",
       "      <td>0.751896</td>\n",
       "      <td>0.166281</td>\n",
       "      <td>0.676954</td>\n",
       "      <td>6</td>\n",
       "      <td>203</td>\n",
       "      <td>0.520790</td>\n",
       "      <td>1.342924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1253</th>\n",
       "      <td>1.527274</td>\n",
       "      <td>0.130698</td>\n",
       "      <td>0.058997</td>\n",
       "      <td>0.985214</td>\n",
       "      <td>1.496154</td>\n",
       "      <td>0.162694</td>\n",
       "      <td>0.069751</td>\n",
       "      <td>0.935825</td>\n",
       "      <td>4</td>\n",
       "      <td>92</td>\n",
       "      <td>0.189695</td>\n",
       "      <td>1.174910</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4050 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      val_loss   val_acc  val_precision  val_recall      loss       acc  \\\n",
       "153   0.274599  0.922604       0.160080    0.094276  0.906488  0.830758   \n",
       "1664  0.278654  0.912960       0.252427    0.293076  0.858906  0.844002   \n",
       "328   0.281642  0.917652       0.227487    0.204509  0.840107  0.840533   \n",
       "3510  0.287094  0.897248       0.207340    0.304348  0.827327  0.854692   \n",
       "3519  0.287145  0.902304       0.205658    0.268189  0.790889  0.864024   \n",
       "3470  0.288091  0.905274       0.168186    0.180940  0.846915  0.847672   \n",
       "3486  0.294346  0.890209       0.190446    0.303469  0.745943  0.859094   \n",
       "1141  0.306713  0.890516       0.160365    0.231591  0.728936  0.844221   \n",
       "3596  0.306795  0.882984       0.170352    0.288684  0.887864  0.762668   \n",
       "187   0.307306  0.893534       0.153678    0.205534  0.822088  0.852862   \n",
       "310   0.307459  0.909173       0.183220    0.186064  0.895736  0.838480   \n",
       "780   0.308464  0.897078       0.177931    0.238179  0.822276  0.865879   \n",
       "3595  0.309137  0.879286       0.178574    0.328942  0.878430  0.770494   \n",
       "2636  0.310689  0.898769       0.202908    0.283999  0.735695  0.858205   \n",
       "787   0.314505  0.890120       0.163644    0.240375  0.813574  0.860581   \n",
       "2646  0.316521  0.886268       0.188320    0.319573  0.693312  0.859198   \n",
       "2628  0.316556  0.900435       0.213321    0.298199  0.753931  0.865920   \n",
       "1119  0.319088  0.898688       0.158854    0.193969  0.819587  0.843842   \n",
       "3525  0.323106  0.886309       0.213793    0.394818  0.773398  0.867132   \n",
       "819   0.325427  0.904019       0.217113    0.282682  0.833104  0.864693   \n",
       "3520  0.325748  0.889836       0.178710    0.276241  0.813793  0.855171   \n",
       "195   0.325763  0.901762       0.196757    0.252232  0.783817  0.863050   \n",
       "838   0.326221  0.881098       0.144085    0.233055  0.815915  0.846162   \n",
       "2631  0.326272  0.882992       0.165101    0.275362  0.811624  0.838559   \n",
       "3469  0.327180  0.899076       0.197361    0.269360  0.824534  0.854558   \n",
       "1076  0.327433  0.885322       0.110616    0.152686  0.988253  0.767921   \n",
       "3462  0.329166  0.902078       0.191840    0.240228  0.848172  0.851955   \n",
       "1149  0.329283  0.892547       0.100074    0.118138  0.812741  0.818897   \n",
       "2312  0.329811  0.908219       0.120054    0.104377  0.929935  0.799247   \n",
       "1243  0.330586  0.856850       0.163089    0.384863  0.910626  0.763902   \n",
       "...        ...       ...            ...         ...       ...       ...   \n",
       "422   0.881718  0.055268       0.055268    1.000000  1.536624  0.066744   \n",
       "3350  0.882934  0.055818       0.055298    1.000000  1.544174  0.127963   \n",
       "2503  0.884872  0.263799       0.068347    0.975406  1.477424  0.123583   \n",
       "3367  0.885759  0.434158       0.076379    0.832821  1.392064  0.510541   \n",
       "3136  0.886526  0.220788       0.061821    0.924023  1.506139  0.176764   \n",
       "2708  0.886768  0.377741       0.072204    0.865759  1.212556  0.640274   \n",
       "2454  0.887895  0.063933       0.055751    1.000000  1.518696  0.070540   \n",
       "3138  0.888698  0.121240       0.058552    0.988142  1.507178  0.175786   \n",
       "3957  0.889455  0.518997       0.086139    0.801640  1.234450  0.680642   \n",
       "1600  0.889553  0.055268       0.055268    1.000000  1.541428  0.071593   \n",
       "702   0.891146  0.055268       0.055268    1.000000  1.527211  0.066744   \n",
       "2483  0.898747  0.461998       0.079083    0.820524  1.353188  0.581267   \n",
       "3116  0.899480  0.111798       0.057943    0.987703  1.508829  0.177333   \n",
       "1017  0.899541  0.449020       0.076269    0.807202  1.404596  0.500881   \n",
       "2206  0.900295  0.188555       0.062722    0.981262  1.506379  0.237604   \n",
       "655   0.901022  0.446625       0.076716    0.816718  1.354769  0.476568   \n",
       "1270  0.903290  0.548771       0.088380    0.769141  1.317304  0.637723   \n",
       "1626  0.904164  0.406236       0.074732    0.856097  1.438064  0.451274   \n",
       "2652  0.907290  0.520526       0.086382    0.801493  1.148861  0.727792   \n",
       "3995  0.920515  0.513981       0.083464    0.780852  1.150958  0.643104   \n",
       "3123  0.937926  0.104411       0.057650    0.990777  1.507189  0.171588   \n",
       "3551  0.940258  0.524928       0.085719    0.785829  1.115859  0.766047   \n",
       "3103  0.951587  0.116369       0.058111    0.985507  1.510998  0.186900   \n",
       "3951  0.957150  0.055268       0.055268    1.000000  1.530666  0.079776   \n",
       "1000  0.965840  0.084751       0.056555    0.992241  1.527532  0.087867   \n",
       "2202  0.974019  0.091199       0.056792    0.989460  1.517464  0.152600   \n",
       "2166  0.984650  0.464563       0.079116    0.816572  1.164080  0.671097   \n",
       "1633  1.058241  0.376980       0.073940    0.891378  1.395982  0.541848   \n",
       "2055  1.065086  0.443988       0.076802    0.822134  1.086487  0.751896   \n",
       "1253  1.527274  0.130698       0.058997    0.985214  1.496154  0.162694   \n",
       "\n",
       "      precision    recall  epoch_num  experiment_num  val_acc_val_prec  \\\n",
       "153    0.213530  0.572336          4              10          1.082683   \n",
       "1664   0.236435  0.599799         15             158          1.165387   \n",
       "328    0.228663  0.585371         29              19          1.145140   \n",
       "3510   0.252228  0.599131         11             292          1.104588   \n",
       "3519   0.269589  0.606819         20             292          1.107962   \n",
       "3470   0.236334  0.574676         21             291          1.073460   \n",
       "3486   0.267247  0.637903         37             291          1.080654   \n",
       "1141   0.249309  0.663306         42              87          1.050881   \n",
       "3596   0.189690  0.781182         47             294          1.053336   \n",
       "187    0.248007  0.592725         38              10          1.047212   \n",
       "310    0.212484  0.524706         11              19          1.092394   \n",
       "780    0.268658  0.586151         31              77          1.075009   \n",
       "3595   0.195555  0.783188         46             294          1.057860   \n",
       "2636   0.269562  0.657679         37             219          1.101676   \n",
       "787    0.262318  0.600858         38              77          1.053763   \n",
       "2646   0.276794  0.687984         47             219          1.074588   \n",
       "2628   0.276110  0.622082         29             219          1.113756   \n",
       "1119   0.237410  0.605593         20              87          1.057542   \n",
       "3525   0.278850  0.624589         26             292          1.100102   \n",
       "819    0.265347  0.580803         20              78          1.121132   \n",
       "3520   0.253075  0.599521         21             292          1.068547   \n",
       "195    0.268205  0.608545         46              10          1.098519   \n",
       "838    0.239339  0.599075         39              78          1.025184   \n",
       "2631   0.232317  0.615676         32             219          1.048093   \n",
       "3469   0.250930  0.593950         20             291          1.096437   \n",
       "1076   0.139961  0.481477         27              86          0.995938   \n",
       "3462   0.244973  0.585037         13             291          1.093918   \n",
       "1149   0.210053  0.620634         50              87          0.992621   \n",
       "2312   0.182113  0.575121         13             210          1.028272   \n",
       "1243   0.187865  0.763579         44              90          1.019940   \n",
       "...         ...       ...        ...             ...               ...   \n",
       "422    0.066744  1.000000         23              25          0.110536   \n",
       "3350   0.066735  0.929196          1             288          0.111116   \n",
       "2503   0.069616  0.981115          4             216          0.332146   \n",
       "3367   0.098892  0.780736         18             288          0.510536   \n",
       "3136   0.071537  0.946187         37             234          0.282609   \n",
       "2708   0.115979  0.662860          9             222          0.449945   \n",
       "2454   0.066934  0.998886          5             215          0.119684   \n",
       "3138   0.071476  0.946465         39             234          0.179792   \n",
       "3957   0.141779  0.748983          8             306          0.605136   \n",
       "1600   0.066878  0.996713          1             156          0.110536   \n",
       "702    0.066744  1.000000          3              75          0.110536   \n",
       "2483   0.111073  0.753050         34             215          0.541081   \n",
       "3116   0.071543  0.945574         17             234          0.169740   \n",
       "1017   0.096586  0.775500         18              84          0.525289   \n",
       "2206   0.073828  0.902791          7             207          0.251277   \n",
       "655    0.100227  0.857724          6              34          0.523341   \n",
       "1270   0.123377  0.725252         21              92          0.637151   \n",
       "1626   0.089773  0.790151         27             156          0.480968   \n",
       "2652   0.148688  0.651440          3             221          0.606909   \n",
       "3995   0.136662  0.817559         46             306          0.597445   \n",
       "3123   0.071249  0.948192         24             234          0.162061   \n",
       "3551   0.169715  0.643641          2             294          0.610647   \n",
       "3103   0.071656  0.935324          4             234          0.174480   \n",
       "3951   0.067074  0.990585          2             306          0.110536   \n",
       "1000   0.067350  0.985850          1              84          0.141306   \n",
       "2202   0.069857  0.949752          3             207          0.147991   \n",
       "2166   0.143625  0.791488         17             206          0.543679   \n",
       "1633   0.101642  0.748148         34             156          0.450920   \n",
       "2055   0.166281  0.676954          6             203          0.520790   \n",
       "1253   0.069751  0.935825          4              92          0.189695   \n",
       "\n",
       "      val_acc_val_prec_val_recall  \n",
       "153                      1.176960  \n",
       "1664                     1.458463  \n",
       "328                      1.349649  \n",
       "3510                     1.408936  \n",
       "3519                     1.376151  \n",
       "3470                     1.254400  \n",
       "3486                     1.384124  \n",
       "1141                     1.282472  \n",
       "3596                     1.342020  \n",
       "187                      1.252745  \n",
       "310                      1.278457  \n",
       "780                      1.313187  \n",
       "3595                     1.386802  \n",
       "2636                     1.385676  \n",
       "787                      1.294138  \n",
       "2646                     1.394160  \n",
       "2628                     1.411955  \n",
       "1119                     1.251510  \n",
       "3525                     1.494920  \n",
       "819                      1.403814  \n",
       "3520                     1.344787  \n",
       "195                      1.350752  \n",
       "838                      1.258239  \n",
       "2631                     1.323455  \n",
       "3469                     1.365798  \n",
       "1076                     1.148624  \n",
       "3462                     1.334146  \n",
       "1149                     1.110759  \n",
       "2312                     1.132650  \n",
       "1243                     1.404803  \n",
       "...                           ...  \n",
       "422                      1.110536  \n",
       "3350                     1.111116  \n",
       "2503                     1.307552  \n",
       "3367                     1.343357  \n",
       "3136                     1.206632  \n",
       "2708                     1.315704  \n",
       "2454                     1.119684  \n",
       "3138                     1.167934  \n",
       "3957                     1.406775  \n",
       "1600                     1.110536  \n",
       "702                      1.110536  \n",
       "2483                     1.361605  \n",
       "3116                     1.157444  \n",
       "1017                     1.332491  \n",
       "2206                     1.232539  \n",
       "655                      1.340059  \n",
       "1270                     1.406291  \n",
       "1626                     1.337066  \n",
       "2652                     1.408402  \n",
       "3995                     1.378297  \n",
       "3123                     1.152838  \n",
       "3551                     1.396476  \n",
       "3103                     1.159987  \n",
       "3951                     1.110536  \n",
       "1000                     1.133547  \n",
       "2202                     1.137451  \n",
       "2166                     1.360250  \n",
       "1633                     1.342297  \n",
       "2055                     1.342924  \n",
       "1253                     1.174910  \n",
       "\n",
       "[4050 rows x 12 columns]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric_df[\"val_acc_val_prec_val_recall\"] = metric_df[\"val_acc\"] + metric_df[\"val_precision\"] + metric_df[\"val_recall\"]\n",
    "metric_df.sort_values(\"val_loss\", ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>experiment_num</th>\n",
       "      <th>valid</th>\n",
       "      <th>num_mels</th>\n",
       "      <th>win_len</th>\n",
       "      <th>chunk_radius</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>note_class_weight</th>\n",
       "      <th>model_version</th>\n",
       "      <th>spec_conv_layers</th>\n",
       "      <th>spec_dense_layers</th>\n",
       "      <th>combined_layers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>292</th>\n",
       "      <td>293</td>\n",
       "      <td>False</td>\n",
       "      <td>80</td>\n",
       "      <td>10</td>\n",
       "      <td>11</td>\n",
       "      <td>400</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>{0: 1, 1: 10}</td>\n",
       "      <td>1</td>\n",
       "      <td>[{'filters': 64, 'kernel_size': (3, 3), 'layer...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[{'units': 23, 'activation': 'relu', 'num_laye...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     experiment_num  valid  num_mels  win_len  chunk_radius  batch_size  \\\n",
       "292             293  False        80       10            11         400   \n",
       "\n",
       "     learning_rate note_class_weight  model_version  \\\n",
       "292         0.0001     {0: 1, 1: 10}              1   \n",
       "\n",
       "                                      spec_conv_layers spec_dense_layers  \\\n",
       "292  [{'filters': 64, 'kernel_size': (3, 3), 'layer...                []   \n",
       "\n",
       "                                       combined_layers  \n",
       "292  [{'units': 23, 'activation': 'relu', 'num_laye...  "
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary_df[summary_df.experiment_num == 293]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "292    [{'filters': 64, 'kernel_size': (3, 3), 'layer...\n",
      "Name: spec_conv_layers, dtype: object\n"
     ]
    }
   ],
   "source": [
    "summary_df[summary_df.experiment_num == 293][\"spec_conv_layers\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>val_loss</th>\n",
       "      <th>val_acc</th>\n",
       "      <th>val_precision</th>\n",
       "      <th>val_recall</th>\n",
       "      <th>loss</th>\n",
       "      <th>acc</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>epoch_num</th>\n",
       "      <th>experiment_num</th>\n",
       "      <th>val_acc_val_prec</th>\n",
       "      <th>val_acc_val_prec_val_recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3599</th>\n",
       "      <td>0.591798</td>\n",
       "      <td>0.714890</td>\n",
       "      <td>0.097825</td>\n",
       "      <td>0.505782</td>\n",
       "      <td>0.968682</td>\n",
       "      <td>0.732792</td>\n",
       "      <td>0.168847</td>\n",
       "      <td>0.765696</td>\n",
       "      <td>50</td>\n",
       "      <td>294</td>\n",
       "      <td>0.812716</td>\n",
       "      <td>1.318498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3598</th>\n",
       "      <td>0.432596</td>\n",
       "      <td>0.807659</td>\n",
       "      <td>0.125144</td>\n",
       "      <td>0.413995</td>\n",
       "      <td>0.857624</td>\n",
       "      <td>0.771223</td>\n",
       "      <td>0.199077</td>\n",
       "      <td>0.803019</td>\n",
       "      <td>49</td>\n",
       "      <td>294</td>\n",
       "      <td>0.932803</td>\n",
       "      <td>1.346798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3597</th>\n",
       "      <td>0.447922</td>\n",
       "      <td>0.803516</td>\n",
       "      <td>0.111823</td>\n",
       "      <td>0.368028</td>\n",
       "      <td>0.919202</td>\n",
       "      <td>0.749067</td>\n",
       "      <td>0.180927</td>\n",
       "      <td>0.782408</td>\n",
       "      <td>48</td>\n",
       "      <td>294</td>\n",
       "      <td>0.915339</td>\n",
       "      <td>1.283367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3596</th>\n",
       "      <td>0.306795</td>\n",
       "      <td>0.882984</td>\n",
       "      <td>0.170352</td>\n",
       "      <td>0.288684</td>\n",
       "      <td>0.887864</td>\n",
       "      <td>0.762668</td>\n",
       "      <td>0.189690</td>\n",
       "      <td>0.781182</td>\n",
       "      <td>47</td>\n",
       "      <td>294</td>\n",
       "      <td>1.053336</td>\n",
       "      <td>1.342020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3595</th>\n",
       "      <td>0.309137</td>\n",
       "      <td>0.879286</td>\n",
       "      <td>0.178574</td>\n",
       "      <td>0.328942</td>\n",
       "      <td>0.878430</td>\n",
       "      <td>0.770494</td>\n",
       "      <td>0.195555</td>\n",
       "      <td>0.783188</td>\n",
       "      <td>46</td>\n",
       "      <td>294</td>\n",
       "      <td>1.057860</td>\n",
       "      <td>1.386802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3594</th>\n",
       "      <td>0.701081</td>\n",
       "      <td>0.577930</td>\n",
       "      <td>0.082572</td>\n",
       "      <td>0.656419</td>\n",
       "      <td>1.126397</td>\n",
       "      <td>0.645033</td>\n",
       "      <td>0.126066</td>\n",
       "      <td>0.727926</td>\n",
       "      <td>45</td>\n",
       "      <td>294</td>\n",
       "      <td>0.660502</td>\n",
       "      <td>1.316922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3593</th>\n",
       "      <td>0.457902</td>\n",
       "      <td>0.807869</td>\n",
       "      <td>0.132804</td>\n",
       "      <td>0.447811</td>\n",
       "      <td>0.872642</td>\n",
       "      <td>0.774216</td>\n",
       "      <td>0.198205</td>\n",
       "      <td>0.782463</td>\n",
       "      <td>44</td>\n",
       "      <td>294</td>\n",
       "      <td>0.940673</td>\n",
       "      <td>1.388484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3592</th>\n",
       "      <td>0.466899</td>\n",
       "      <td>0.799107</td>\n",
       "      <td>0.097644</td>\n",
       "      <td>0.319719</td>\n",
       "      <td>1.033254</td>\n",
       "      <td>0.701571</td>\n",
       "      <td>0.151631</td>\n",
       "      <td>0.755445</td>\n",
       "      <td>43</td>\n",
       "      <td>294</td>\n",
       "      <td>0.896751</td>\n",
       "      <td>1.216470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3591</th>\n",
       "      <td>0.507086</td>\n",
       "      <td>0.767108</td>\n",
       "      <td>0.125026</td>\n",
       "      <td>0.535793</td>\n",
       "      <td>0.877324</td>\n",
       "      <td>0.773655</td>\n",
       "      <td>0.197306</td>\n",
       "      <td>0.779344</td>\n",
       "      <td>42</td>\n",
       "      <td>294</td>\n",
       "      <td>0.892134</td>\n",
       "      <td>1.427926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3590</th>\n",
       "      <td>0.496189</td>\n",
       "      <td>0.765296</td>\n",
       "      <td>0.095587</td>\n",
       "      <td>0.383692</td>\n",
       "      <td>1.010813</td>\n",
       "      <td>0.727613</td>\n",
       "      <td>0.162904</td>\n",
       "      <td>0.744471</td>\n",
       "      <td>41</td>\n",
       "      <td>294</td>\n",
       "      <td>0.860883</td>\n",
       "      <td>1.244575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3589</th>\n",
       "      <td>0.377937</td>\n",
       "      <td>0.863024</td>\n",
       "      <td>0.160766</td>\n",
       "      <td>0.350315</td>\n",
       "      <td>0.905314</td>\n",
       "      <td>0.777867</td>\n",
       "      <td>0.197493</td>\n",
       "      <td>0.759958</td>\n",
       "      <td>40</td>\n",
       "      <td>294</td>\n",
       "      <td>1.023790</td>\n",
       "      <td>1.374104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3588</th>\n",
       "      <td>0.442676</td>\n",
       "      <td>0.825216</td>\n",
       "      <td>0.127872</td>\n",
       "      <td>0.371542</td>\n",
       "      <td>0.959043</td>\n",
       "      <td>0.758403</td>\n",
       "      <td>0.181781</td>\n",
       "      <td>0.748259</td>\n",
       "      <td>39</td>\n",
       "      <td>294</td>\n",
       "      <td>0.953087</td>\n",
       "      <td>1.324629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3587</th>\n",
       "      <td>0.449888</td>\n",
       "      <td>0.831777</td>\n",
       "      <td>0.141161</td>\n",
       "      <td>0.401991</td>\n",
       "      <td>0.920719</td>\n",
       "      <td>0.771736</td>\n",
       "      <td>0.192104</td>\n",
       "      <td>0.754944</td>\n",
       "      <td>38</td>\n",
       "      <td>294</td>\n",
       "      <td>0.972938</td>\n",
       "      <td>1.374929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3586</th>\n",
       "      <td>0.459475</td>\n",
       "      <td>0.822796</td>\n",
       "      <td>0.157555</td>\n",
       "      <td>0.507539</td>\n",
       "      <td>0.928706</td>\n",
       "      <td>0.779864</td>\n",
       "      <td>0.196239</td>\n",
       "      <td>0.742354</td>\n",
       "      <td>37</td>\n",
       "      <td>294</td>\n",
       "      <td>0.980352</td>\n",
       "      <td>1.487891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3585</th>\n",
       "      <td>0.696727</td>\n",
       "      <td>0.583181</td>\n",
       "      <td>0.084206</td>\n",
       "      <td>0.662421</td>\n",
       "      <td>1.155303</td>\n",
       "      <td>0.638173</td>\n",
       "      <td>0.122645</td>\n",
       "      <td>0.718456</td>\n",
       "      <td>36</td>\n",
       "      <td>294</td>\n",
       "      <td>0.667387</td>\n",
       "      <td>1.329809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3584</th>\n",
       "      <td>0.489447</td>\n",
       "      <td>0.807465</td>\n",
       "      <td>0.134920</td>\n",
       "      <td>0.458937</td>\n",
       "      <td>0.920045</td>\n",
       "      <td>0.774986</td>\n",
       "      <td>0.194137</td>\n",
       "      <td>0.752549</td>\n",
       "      <td>35</td>\n",
       "      <td>294</td>\n",
       "      <td>0.942384</td>\n",
       "      <td>1.401322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3583</th>\n",
       "      <td>0.570540</td>\n",
       "      <td>0.729324</td>\n",
       "      <td>0.105430</td>\n",
       "      <td>0.520714</td>\n",
       "      <td>1.063246</td>\n",
       "      <td>0.710603</td>\n",
       "      <td>0.150824</td>\n",
       "      <td>0.720461</td>\n",
       "      <td>34</td>\n",
       "      <td>294</td>\n",
       "      <td>0.834754</td>\n",
       "      <td>1.355469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3582</th>\n",
       "      <td>0.561592</td>\n",
       "      <td>0.758240</td>\n",
       "      <td>0.130277</td>\n",
       "      <td>0.594496</td>\n",
       "      <td>0.927766</td>\n",
       "      <td>0.786382</td>\n",
       "      <td>0.199860</td>\n",
       "      <td>0.732661</td>\n",
       "      <td>33</td>\n",
       "      <td>294</td>\n",
       "      <td>0.888518</td>\n",
       "      <td>1.483013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3581</th>\n",
       "      <td>0.503849</td>\n",
       "      <td>0.798654</td>\n",
       "      <td>0.122328</td>\n",
       "      <td>0.428049</td>\n",
       "      <td>1.040677</td>\n",
       "      <td>0.735161</td>\n",
       "      <td>0.162233</td>\n",
       "      <td>0.712774</td>\n",
       "      <td>32</td>\n",
       "      <td>294</td>\n",
       "      <td>0.920981</td>\n",
       "      <td>1.349030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3580</th>\n",
       "      <td>0.502778</td>\n",
       "      <td>0.786954</td>\n",
       "      <td>0.130817</td>\n",
       "      <td>0.505782</td>\n",
       "      <td>0.955265</td>\n",
       "      <td>0.789018</td>\n",
       "      <td>0.199396</td>\n",
       "      <td>0.716729</td>\n",
       "      <td>31</td>\n",
       "      <td>294</td>\n",
       "      <td>0.917771</td>\n",
       "      <td>1.423554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3579</th>\n",
       "      <td>0.595032</td>\n",
       "      <td>0.752795</td>\n",
       "      <td>0.119592</td>\n",
       "      <td>0.545894</td>\n",
       "      <td>1.003349</td>\n",
       "      <td>0.765665</td>\n",
       "      <td>0.179708</td>\n",
       "      <td>0.704418</td>\n",
       "      <td>30</td>\n",
       "      <td>294</td>\n",
       "      <td>0.872387</td>\n",
       "      <td>1.418281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3578</th>\n",
       "      <td>0.402382</td>\n",
       "      <td>0.850637</td>\n",
       "      <td>0.163795</td>\n",
       "      <td>0.414727</td>\n",
       "      <td>0.968918</td>\n",
       "      <td>0.790167</td>\n",
       "      <td>0.198496</td>\n",
       "      <td>0.705699</td>\n",
       "      <td>29</td>\n",
       "      <td>294</td>\n",
       "      <td>1.014432</td>\n",
       "      <td>1.429159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3577</th>\n",
       "      <td>0.517885</td>\n",
       "      <td>0.816737</td>\n",
       "      <td>0.156057</td>\n",
       "      <td>0.525399</td>\n",
       "      <td>0.975795</td>\n",
       "      <td>0.788639</td>\n",
       "      <td>0.197367</td>\n",
       "      <td>0.706534</td>\n",
       "      <td>28</td>\n",
       "      <td>294</td>\n",
       "      <td>0.972794</td>\n",
       "      <td>1.498192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3576</th>\n",
       "      <td>0.606203</td>\n",
       "      <td>0.669404</td>\n",
       "      <td>0.088930</td>\n",
       "      <td>0.538867</td>\n",
       "      <td>1.173523</td>\n",
       "      <td>0.638720</td>\n",
       "      <td>0.120555</td>\n",
       "      <td>0.701019</td>\n",
       "      <td>27</td>\n",
       "      <td>294</td>\n",
       "      <td>0.758334</td>\n",
       "      <td>1.297201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3575</th>\n",
       "      <td>0.502915</td>\n",
       "      <td>0.812238</td>\n",
       "      <td>0.152845</td>\n",
       "      <td>0.527741</td>\n",
       "      <td>0.982900</td>\n",
       "      <td>0.796257</td>\n",
       "      <td>0.201187</td>\n",
       "      <td>0.690992</td>\n",
       "      <td>26</td>\n",
       "      <td>294</td>\n",
       "      <td>0.965083</td>\n",
       "      <td>1.492824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3574</th>\n",
       "      <td>0.481810</td>\n",
       "      <td>0.814471</td>\n",
       "      <td>0.123761</td>\n",
       "      <td>0.387645</td>\n",
       "      <td>1.111032</td>\n",
       "      <td>0.724155</td>\n",
       "      <td>0.150652</td>\n",
       "      <td>0.675506</td>\n",
       "      <td>25</td>\n",
       "      <td>294</td>\n",
       "      <td>0.938233</td>\n",
       "      <td>1.325877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3573</th>\n",
       "      <td>0.377269</td>\n",
       "      <td>0.870605</td>\n",
       "      <td>0.185328</td>\n",
       "      <td>0.394964</td>\n",
       "      <td>0.984011</td>\n",
       "      <td>0.811346</td>\n",
       "      <td>0.213181</td>\n",
       "      <td>0.678792</td>\n",
       "      <td>24</td>\n",
       "      <td>294</td>\n",
       "      <td>1.055932</td>\n",
       "      <td>1.450896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3572</th>\n",
       "      <td>0.555192</td>\n",
       "      <td>0.769778</td>\n",
       "      <td>0.126657</td>\n",
       "      <td>0.536964</td>\n",
       "      <td>1.090549</td>\n",
       "      <td>0.733079</td>\n",
       "      <td>0.156602</td>\n",
       "      <td>0.683862</td>\n",
       "      <td>23</td>\n",
       "      <td>294</td>\n",
       "      <td>0.896435</td>\n",
       "      <td>1.433399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3571</th>\n",
       "      <td>0.539272</td>\n",
       "      <td>0.828339</td>\n",
       "      <td>0.159938</td>\n",
       "      <td>0.495242</td>\n",
       "      <td>1.004478</td>\n",
       "      <td>0.810037</td>\n",
       "      <td>0.209186</td>\n",
       "      <td>0.663974</td>\n",
       "      <td>22</td>\n",
       "      <td>294</td>\n",
       "      <td>0.988276</td>\n",
       "      <td>1.483519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3570</th>\n",
       "      <td>0.457239</td>\n",
       "      <td>0.837643</td>\n",
       "      <td>0.142154</td>\n",
       "      <td>0.384863</td>\n",
       "      <td>1.051630</td>\n",
       "      <td>0.772900</td>\n",
       "      <td>0.178571</td>\n",
       "      <td>0.667372</td>\n",
       "      <td>21</td>\n",
       "      <td>294</td>\n",
       "      <td>0.979797</td>\n",
       "      <td>1.364660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3569</th>\n",
       "      <td>0.647300</td>\n",
       "      <td>0.756258</td>\n",
       "      <td>0.132025</td>\n",
       "      <td>0.611770</td>\n",
       "      <td>1.011390</td>\n",
       "      <td>0.807839</td>\n",
       "      <td>0.207871</td>\n",
       "      <td>0.668542</td>\n",
       "      <td>20</td>\n",
       "      <td>294</td>\n",
       "      <td>0.888284</td>\n",
       "      <td>1.500053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3568</th>\n",
       "      <td>0.503799</td>\n",
       "      <td>0.846041</td>\n",
       "      <td>0.168677</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>1.028593</td>\n",
       "      <td>0.794655</td>\n",
       "      <td>0.197137</td>\n",
       "      <td>0.675840</td>\n",
       "      <td>19</td>\n",
       "      <td>294</td>\n",
       "      <td>1.014718</td>\n",
       "      <td>1.469263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3567</th>\n",
       "      <td>0.563002</td>\n",
       "      <td>0.713304</td>\n",
       "      <td>0.099137</td>\n",
       "      <td>0.517787</td>\n",
       "      <td>1.201506</td>\n",
       "      <td>0.652964</td>\n",
       "      <td>0.118887</td>\n",
       "      <td>0.655005</td>\n",
       "      <td>18</td>\n",
       "      <td>294</td>\n",
       "      <td>0.812441</td>\n",
       "      <td>1.330228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3566</th>\n",
       "      <td>0.458945</td>\n",
       "      <td>0.857975</td>\n",
       "      <td>0.177387</td>\n",
       "      <td>0.431562</td>\n",
       "      <td>1.029958</td>\n",
       "      <td>0.814257</td>\n",
       "      <td>0.211339</td>\n",
       "      <td>0.652666</td>\n",
       "      <td>17</td>\n",
       "      <td>294</td>\n",
       "      <td>1.035362</td>\n",
       "      <td>1.466924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3565</th>\n",
       "      <td>0.543759</td>\n",
       "      <td>0.767885</td>\n",
       "      <td>0.124459</td>\n",
       "      <td>0.530230</td>\n",
       "      <td>1.148684</td>\n",
       "      <td>0.699604</td>\n",
       "      <td>0.137950</td>\n",
       "      <td>0.666927</td>\n",
       "      <td>16</td>\n",
       "      <td>294</td>\n",
       "      <td>0.892343</td>\n",
       "      <td>1.422573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3564</th>\n",
       "      <td>0.535084</td>\n",
       "      <td>0.804738</td>\n",
       "      <td>0.157705</td>\n",
       "      <td>0.583516</td>\n",
       "      <td>1.024506</td>\n",
       "      <td>0.812900</td>\n",
       "      <td>0.212021</td>\n",
       "      <td>0.663807</td>\n",
       "      <td>15</td>\n",
       "      <td>294</td>\n",
       "      <td>0.962443</td>\n",
       "      <td>1.545960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3563</th>\n",
       "      <td>0.588532</td>\n",
       "      <td>0.773645</td>\n",
       "      <td>0.128287</td>\n",
       "      <td>0.534182</td>\n",
       "      <td>1.125865</td>\n",
       "      <td>0.734953</td>\n",
       "      <td>0.153261</td>\n",
       "      <td>0.656621</td>\n",
       "      <td>14</td>\n",
       "      <td>294</td>\n",
       "      <td>0.901932</td>\n",
       "      <td>1.436115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3562</th>\n",
       "      <td>0.536040</td>\n",
       "      <td>0.829641</td>\n",
       "      <td>0.163026</td>\n",
       "      <td>0.503733</td>\n",
       "      <td>1.050629</td>\n",
       "      <td>0.813290</td>\n",
       "      <td>0.206751</td>\n",
       "      <td>0.633614</td>\n",
       "      <td>13</td>\n",
       "      <td>294</td>\n",
       "      <td>0.992668</td>\n",
       "      <td>1.496401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3561</th>\n",
       "      <td>0.618220</td>\n",
       "      <td>0.786089</td>\n",
       "      <td>0.139585</td>\n",
       "      <td>0.555848</td>\n",
       "      <td>1.089045</td>\n",
       "      <td>0.767434</td>\n",
       "      <td>0.173648</td>\n",
       "      <td>0.660966</td>\n",
       "      <td>12</td>\n",
       "      <td>294</td>\n",
       "      <td>0.925674</td>\n",
       "      <td>1.481522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3560</th>\n",
       "      <td>0.499878</td>\n",
       "      <td>0.843485</td>\n",
       "      <td>0.165294</td>\n",
       "      <td>0.452350</td>\n",
       "      <td>1.065089</td>\n",
       "      <td>0.798819</td>\n",
       "      <td>0.196440</td>\n",
       "      <td>0.651719</td>\n",
       "      <td>11</td>\n",
       "      <td>294</td>\n",
       "      <td>1.008778</td>\n",
       "      <td>1.461128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3559</th>\n",
       "      <td>0.450104</td>\n",
       "      <td>0.863347</td>\n",
       "      <td>0.182661</td>\n",
       "      <td>0.423803</td>\n",
       "      <td>1.078948</td>\n",
       "      <td>0.775439</td>\n",
       "      <td>0.180543</td>\n",
       "      <td>0.668152</td>\n",
       "      <td>10</td>\n",
       "      <td>294</td>\n",
       "      <td>1.046009</td>\n",
       "      <td>1.469812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3558</th>\n",
       "      <td>0.504643</td>\n",
       "      <td>0.805652</td>\n",
       "      <td>0.120865</td>\n",
       "      <td>0.401113</td>\n",
       "      <td>1.227900</td>\n",
       "      <td>0.622509</td>\n",
       "      <td>0.110056</td>\n",
       "      <td>0.657011</td>\n",
       "      <td>9</td>\n",
       "      <td>294</td>\n",
       "      <td>0.926517</td>\n",
       "      <td>1.327629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3557</th>\n",
       "      <td>0.467912</td>\n",
       "      <td>0.845321</td>\n",
       "      <td>0.176129</td>\n",
       "      <td>0.489094</td>\n",
       "      <td>1.061160</td>\n",
       "      <td>0.802006</td>\n",
       "      <td>0.197681</td>\n",
       "      <td>0.642917</td>\n",
       "      <td>8</td>\n",
       "      <td>294</td>\n",
       "      <td>1.021451</td>\n",
       "      <td>1.510544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3556</th>\n",
       "      <td>0.598079</td>\n",
       "      <td>0.735473</td>\n",
       "      <td>0.120206</td>\n",
       "      <td>0.599180</td>\n",
       "      <td>1.189151</td>\n",
       "      <td>0.681255</td>\n",
       "      <td>0.129291</td>\n",
       "      <td>0.658403</td>\n",
       "      <td>7</td>\n",
       "      <td>294</td>\n",
       "      <td>0.855679</td>\n",
       "      <td>1.454859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3555</th>\n",
       "      <td>0.453200</td>\n",
       "      <td>0.846786</td>\n",
       "      <td>0.178579</td>\n",
       "      <td>0.492314</td>\n",
       "      <td>1.079884</td>\n",
       "      <td>0.789851</td>\n",
       "      <td>0.189925</td>\n",
       "      <td>0.658013</td>\n",
       "      <td>6</td>\n",
       "      <td>294</td>\n",
       "      <td>1.025365</td>\n",
       "      <td>1.517679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3554</th>\n",
       "      <td>0.482305</td>\n",
       "      <td>0.859893</td>\n",
       "      <td>0.159369</td>\n",
       "      <td>0.359098</td>\n",
       "      <td>1.157049</td>\n",
       "      <td>0.718794</td>\n",
       "      <td>0.144504</td>\n",
       "      <td>0.653056</td>\n",
       "      <td>5</td>\n",
       "      <td>294</td>\n",
       "      <td>1.019261</td>\n",
       "      <td>1.378359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3553</th>\n",
       "      <td>0.459817</td>\n",
       "      <td>0.859925</td>\n",
       "      <td>0.178703</td>\n",
       "      <td>0.426731</td>\n",
       "      <td>1.094059</td>\n",
       "      <td>0.799481</td>\n",
       "      <td>0.191236</td>\n",
       "      <td>0.620690</td>\n",
       "      <td>4</td>\n",
       "      <td>294</td>\n",
       "      <td>1.038628</td>\n",
       "      <td>1.465359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3552</th>\n",
       "      <td>0.515498</td>\n",
       "      <td>0.838646</td>\n",
       "      <td>0.162688</td>\n",
       "      <td>0.462890</td>\n",
       "      <td>1.145199</td>\n",
       "      <td>0.747081</td>\n",
       "      <td>0.156684</td>\n",
       "      <td>0.636511</td>\n",
       "      <td>3</td>\n",
       "      <td>294</td>\n",
       "      <td>1.001334</td>\n",
       "      <td>1.464224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3551</th>\n",
       "      <td>0.940258</td>\n",
       "      <td>0.524928</td>\n",
       "      <td>0.085719</td>\n",
       "      <td>0.785829</td>\n",
       "      <td>1.115859</td>\n",
       "      <td>0.766047</td>\n",
       "      <td>0.169715</td>\n",
       "      <td>0.643641</td>\n",
       "      <td>2</td>\n",
       "      <td>294</td>\n",
       "      <td>0.610647</td>\n",
       "      <td>1.396476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3550</th>\n",
       "      <td>0.651902</td>\n",
       "      <td>0.746921</td>\n",
       "      <td>0.130110</td>\n",
       "      <td>0.629483</td>\n",
       "      <td>1.243561</td>\n",
       "      <td>0.513575</td>\n",
       "      <td>0.094672</td>\n",
       "      <td>0.734332</td>\n",
       "      <td>1</td>\n",
       "      <td>294</td>\n",
       "      <td>0.877031</td>\n",
       "      <td>1.506515</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      val_loss   val_acc  val_precision  val_recall      loss       acc  \\\n",
       "3599  0.591798  0.714890       0.097825    0.505782  0.968682  0.732792   \n",
       "3598  0.432596  0.807659       0.125144    0.413995  0.857624  0.771223   \n",
       "3597  0.447922  0.803516       0.111823    0.368028  0.919202  0.749067   \n",
       "3596  0.306795  0.882984       0.170352    0.288684  0.887864  0.762668   \n",
       "3595  0.309137  0.879286       0.178574    0.328942  0.878430  0.770494   \n",
       "3594  0.701081  0.577930       0.082572    0.656419  1.126397  0.645033   \n",
       "3593  0.457902  0.807869       0.132804    0.447811  0.872642  0.774216   \n",
       "3592  0.466899  0.799107       0.097644    0.319719  1.033254  0.701571   \n",
       "3591  0.507086  0.767108       0.125026    0.535793  0.877324  0.773655   \n",
       "3590  0.496189  0.765296       0.095587    0.383692  1.010813  0.727613   \n",
       "3589  0.377937  0.863024       0.160766    0.350315  0.905314  0.777867   \n",
       "3588  0.442676  0.825216       0.127872    0.371542  0.959043  0.758403   \n",
       "3587  0.449888  0.831777       0.141161    0.401991  0.920719  0.771736   \n",
       "3586  0.459475  0.822796       0.157555    0.507539  0.928706  0.779864   \n",
       "3585  0.696727  0.583181       0.084206    0.662421  1.155303  0.638173   \n",
       "3584  0.489447  0.807465       0.134920    0.458937  0.920045  0.774986   \n",
       "3583  0.570540  0.729324       0.105430    0.520714  1.063246  0.710603   \n",
       "3582  0.561592  0.758240       0.130277    0.594496  0.927766  0.786382   \n",
       "3581  0.503849  0.798654       0.122328    0.428049  1.040677  0.735161   \n",
       "3580  0.502778  0.786954       0.130817    0.505782  0.955265  0.789018   \n",
       "3579  0.595032  0.752795       0.119592    0.545894  1.003349  0.765665   \n",
       "3578  0.402382  0.850637       0.163795    0.414727  0.968918  0.790167   \n",
       "3577  0.517885  0.816737       0.156057    0.525399  0.975795  0.788639   \n",
       "3576  0.606203  0.669404       0.088930    0.538867  1.173523  0.638720   \n",
       "3575  0.502915  0.812238       0.152845    0.527741  0.982900  0.796257   \n",
       "3574  0.481810  0.814471       0.123761    0.387645  1.111032  0.724155   \n",
       "3573  0.377269  0.870605       0.185328    0.394964  0.984011  0.811346   \n",
       "3572  0.555192  0.769778       0.126657    0.536964  1.090549  0.733079   \n",
       "3571  0.539272  0.828339       0.159938    0.495242  1.004478  0.810037   \n",
       "3570  0.457239  0.837643       0.142154    0.384863  1.051630  0.772900   \n",
       "3569  0.647300  0.756258       0.132025    0.611770  1.011390  0.807839   \n",
       "3568  0.503799  0.846041       0.168677    0.454545  1.028593  0.794655   \n",
       "3567  0.563002  0.713304       0.099137    0.517787  1.201506  0.652964   \n",
       "3566  0.458945  0.857975       0.177387    0.431562  1.029958  0.814257   \n",
       "3565  0.543759  0.767885       0.124459    0.530230  1.148684  0.699604   \n",
       "3564  0.535084  0.804738       0.157705    0.583516  1.024506  0.812900   \n",
       "3563  0.588532  0.773645       0.128287    0.534182  1.125865  0.734953   \n",
       "3562  0.536040  0.829641       0.163026    0.503733  1.050629  0.813290   \n",
       "3561  0.618220  0.786089       0.139585    0.555848  1.089045  0.767434   \n",
       "3560  0.499878  0.843485       0.165294    0.452350  1.065089  0.798819   \n",
       "3559  0.450104  0.863347       0.182661    0.423803  1.078948  0.775439   \n",
       "3558  0.504643  0.805652       0.120865    0.401113  1.227900  0.622509   \n",
       "3557  0.467912  0.845321       0.176129    0.489094  1.061160  0.802006   \n",
       "3556  0.598079  0.735473       0.120206    0.599180  1.189151  0.681255   \n",
       "3555  0.453200  0.846786       0.178579    0.492314  1.079884  0.789851   \n",
       "3554  0.482305  0.859893       0.159369    0.359098  1.157049  0.718794   \n",
       "3553  0.459817  0.859925       0.178703    0.426731  1.094059  0.799481   \n",
       "3552  0.515498  0.838646       0.162688    0.462890  1.145199  0.747081   \n",
       "3551  0.940258  0.524928       0.085719    0.785829  1.115859  0.766047   \n",
       "3550  0.651902  0.746921       0.130110    0.629483  1.243561  0.513575   \n",
       "\n",
       "      precision    recall  epoch_num  experiment_num  val_acc_val_prec  \\\n",
       "3599   0.168847  0.765696         50             294          0.812716   \n",
       "3598   0.199077  0.803019         49             294          0.932803   \n",
       "3597   0.180927  0.782408         48             294          0.915339   \n",
       "3596   0.189690  0.781182         47             294          1.053336   \n",
       "3595   0.195555  0.783188         46             294          1.057860   \n",
       "3594   0.126066  0.727926         45             294          0.660502   \n",
       "3593   0.198205  0.782463         44             294          0.940673   \n",
       "3592   0.151631  0.755445         43             294          0.896751   \n",
       "3591   0.197306  0.779344         42             294          0.892134   \n",
       "3590   0.162904  0.744471         41             294          0.860883   \n",
       "3589   0.197493  0.759958         40             294          1.023790   \n",
       "3588   0.181781  0.748259         39             294          0.953087   \n",
       "3587   0.192104  0.754944         38             294          0.972938   \n",
       "3586   0.196239  0.742354         37             294          0.980352   \n",
       "3585   0.122645  0.718456         36             294          0.667387   \n",
       "3584   0.194137  0.752549         35             294          0.942384   \n",
       "3583   0.150824  0.720461         34             294          0.834754   \n",
       "3582   0.199860  0.732661         33             294          0.888518   \n",
       "3581   0.162233  0.712774         32             294          0.920981   \n",
       "3580   0.199396  0.716729         31             294          0.917771   \n",
       "3579   0.179708  0.704418         30             294          0.872387   \n",
       "3578   0.198496  0.705699         29             294          1.014432   \n",
       "3577   0.197367  0.706534         28             294          0.972794   \n",
       "3576   0.120555  0.701019         27             294          0.758334   \n",
       "3575   0.201187  0.690992         26             294          0.965083   \n",
       "3574   0.150652  0.675506         25             294          0.938233   \n",
       "3573   0.213181  0.678792         24             294          1.055932   \n",
       "3572   0.156602  0.683862         23             294          0.896435   \n",
       "3571   0.209186  0.663974         22             294          0.988276   \n",
       "3570   0.178571  0.667372         21             294          0.979797   \n",
       "3569   0.207871  0.668542         20             294          0.888284   \n",
       "3568   0.197137  0.675840         19             294          1.014718   \n",
       "3567   0.118887  0.655005         18             294          0.812441   \n",
       "3566   0.211339  0.652666         17             294          1.035362   \n",
       "3565   0.137950  0.666927         16             294          0.892343   \n",
       "3564   0.212021  0.663807         15             294          0.962443   \n",
       "3563   0.153261  0.656621         14             294          0.901932   \n",
       "3562   0.206751  0.633614         13             294          0.992668   \n",
       "3561   0.173648  0.660966         12             294          0.925674   \n",
       "3560   0.196440  0.651719         11             294          1.008778   \n",
       "3559   0.180543  0.668152         10             294          1.046009   \n",
       "3558   0.110056  0.657011          9             294          0.926517   \n",
       "3557   0.197681  0.642917          8             294          1.021451   \n",
       "3556   0.129291  0.658403          7             294          0.855679   \n",
       "3555   0.189925  0.658013          6             294          1.025365   \n",
       "3554   0.144504  0.653056          5             294          1.019261   \n",
       "3553   0.191236  0.620690          4             294          1.038628   \n",
       "3552   0.156684  0.636511          3             294          1.001334   \n",
       "3551   0.169715  0.643641          2             294          0.610647   \n",
       "3550   0.094672  0.734332          1             294          0.877031   \n",
       "\n",
       "      val_acc_val_prec_val_recall  \n",
       "3599                     1.318498  \n",
       "3598                     1.346798  \n",
       "3597                     1.283367  \n",
       "3596                     1.342020  \n",
       "3595                     1.386802  \n",
       "3594                     1.316922  \n",
       "3593                     1.388484  \n",
       "3592                     1.216470  \n",
       "3591                     1.427926  \n",
       "3590                     1.244575  \n",
       "3589                     1.374104  \n",
       "3588                     1.324629  \n",
       "3587                     1.374929  \n",
       "3586                     1.487891  \n",
       "3585                     1.329809  \n",
       "3584                     1.401322  \n",
       "3583                     1.355469  \n",
       "3582                     1.483013  \n",
       "3581                     1.349030  \n",
       "3580                     1.423554  \n",
       "3579                     1.418281  \n",
       "3578                     1.429159  \n",
       "3577                     1.498192  \n",
       "3576                     1.297201  \n",
       "3575                     1.492824  \n",
       "3574                     1.325877  \n",
       "3573                     1.450896  \n",
       "3572                     1.433399  \n",
       "3571                     1.483519  \n",
       "3570                     1.364660  \n",
       "3569                     1.500053  \n",
       "3568                     1.469263  \n",
       "3567                     1.330228  \n",
       "3566                     1.466924  \n",
       "3565                     1.422573  \n",
       "3564                     1.545960  \n",
       "3563                     1.436115  \n",
       "3562                     1.496401  \n",
       "3561                     1.481522  \n",
       "3560                     1.461128  \n",
       "3559                     1.469812  \n",
       "3558                     1.327629  \n",
       "3557                     1.510544  \n",
       "3556                     1.454859  \n",
       "3555                     1.517679  \n",
       "3554                     1.378359  \n",
       "3553                     1.465359  \n",
       "3552                     1.464224  \n",
       "3551                     1.396476  \n",
       "3550                     1.506515  "
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric_df[metric_df.experiment_num == 294].sort_values(\"epoch_num\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CountDataGenerator(keras.utils.Sequence):\n",
    "    'Generates data for Keras'\n",
    "    def __init__(self, key_difs, song_df, num_mels, win_len, chunk_radius, \n",
    "                 object_type=\"Note\", max_placements=13, batch_size=200, \n",
    "                 count_weights={1: 1, 2: 1, 3: 100, 4: 50, 5: 500, 6: 400}):\n",
    "        'Initialization'\n",
    "        self.key_difs = key_difs\n",
    "        self.song_df = song_df\n",
    "        self.num_mels = num_mels\n",
    "        self.win_len = win_len\n",
    "        self.chunk_radius = chunk_radius\n",
    "        self.object_type = object_type\n",
    "        self.max_placements = max_placements\n",
    "        self.batch_size = batch_size\n",
    "        self.count_weights = count_weights\n",
    "        \n",
    "        self.pitch_shift = 0\n",
    "        self.pitch_index = 0\n",
    "        self.pitch_shifts = [0, 4, -4, 8, -8, 12, -12, 16, -16]\n",
    "\n",
    "        self.val_counts = {}\n",
    "        \n",
    "        for i in range(self.max_placements):\n",
    "            self.val_counts[i] = 0\n",
    "        \n",
    "        self.get_data_for_epoch()\n",
    "        self.num_samples = self.specs.shape[0]\n",
    "        logging.debug(\"Objs: {}\".format(self.objs))\n",
    "        # self.on_epoch_end()\n",
    "\n",
    "    def get_data_for_epoch(self):\n",
    "        spec_list = []\n",
    "        meta_list = []\n",
    "        obj_list = []\n",
    "        \n",
    "        for key_dif in self.key_difs:\n",
    "            key, dif = key_dif.split(\"-\")\n",
    "            \n",
    "            meta_arr, spec, time_indices, obj_arr = get_song_count_train_data(self.song_df, key, self.num_mels, self.win_len, dif, \n",
    "                                                                self.pitch_shift, self.object_type)\n",
    "            \n",
    "            \n",
    "            for index in np.nonzero(obj_arr)[0]:\n",
    "                spec_arr_new = chunk_spectrogram(spec, time_indices, index, 1, self.chunk_radius)\n",
    "                spec_list.append(spec_arr_new)\n",
    "                meta_list.append(meta_arr[index, 0]) # We only care about difficulty for this, so get only that column\n",
    "                obj_list.append(obj_arr[index])\n",
    "        \n",
    "        \n",
    "        self.specs = np.vstack(spec_list)\n",
    "        self.specs = reshape_spec_arr(self.specs)\n",
    "        self.meta = np.stack(meta_list)\n",
    "        self.objs = keras.utils.to_categorical(np.stack(obj_list), self.max_placements)\n",
    "        logging.debug(\"Shapes for epoch. Specs: {}, meta: {}, objs: {}\".format(self.specs.shape, \n",
    "                                                                              self.meta.shape, self.objs.shape))\n",
    "    def __len__(self):\n",
    "        'Denotes the number of batches per epoch'\n",
    "        return math.ceil(self.num_samples / self.batch_size)\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        'Update pitches after each epoc'\n",
    "        \n",
    "        self.pitch_index += 1\n",
    "        \n",
    "        if self.pitch_index >= len(self.pitch_shifts):\n",
    "            self.pitch_index = 0\n",
    "        \n",
    "        self.pitch_shift = self.pitch_shifts[self.pitch_index]\n",
    "        self.get_data_for_epoch()\n",
    "        \n",
    "        logging.debug(\"Value counts: {}\".format(self.val_counts))\n",
    "        self.val_counts = {}\n",
    "        \n",
    "        for i in range(self.max_placements):\n",
    "            self.val_counts[i] = 0\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        'Generate one batch of data'\n",
    "        #import pdb; pdb.set_trace()\n",
    "        # Get key and difficulty for current         \n",
    "        x = [self.specs[index:index+batch_size], self.meta[index:index+batch_size]]\n",
    "        y = self.objs[index:index+batch_size]\n",
    "        \n",
    "        sample_weights = np.ones(y.shape[0])\n",
    "        logging.debug(\"Y values?: {}\".format(y))\n",
    "        \n",
    "        for i in range(2, max_placements):\n",
    "            \n",
    "            non_zero = np.nonzero(y[:, i])[0]\n",
    "            \n",
    "            for sub_index in non_zero:\n",
    "                if i in self.count_weights:\n",
    "                    sample_weights[sub_index] = self.count_weights[i]\n",
    "                else:\n",
    "                    sample_weights[sub_index] = i * 10\n",
    "                \n",
    "                self.val_counts[i] += 1\n",
    "            \n",
    "        logging.debug(\"Sample weights: {}\".format(sample_weights))\n",
    "        return x, y, sample_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_keys = train_keys_s\n",
    "validation_keys = validation_keys_s\n",
    "max_placements = 13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Spectrogram input\n",
    "# PREDICTING NOTES COUNTS\n",
    "image_input = Input(shape=(num_mels, chunk_radius * 2 + 1, 1))\n",
    "\n",
    "# Timing and difficulty information\n",
    "other_data_input = Input(shape=(1, ))   \n",
    "\n",
    "# First convolution\n",
    "conv = Convolution2D(64, (3,3), activation=\"relu\")(image_input)\n",
    "conv = Convolution2D(64, (1,1), activation=\"relu\")(conv)\n",
    "conv = Convolution2D(64, (1,1), activation=\"relu\")(conv)\n",
    "conv = MaxPooling2D(pool_size=(2,2))(conv)\n",
    "\n",
    "# Second Convolution\n",
    "conv = Convolution2D(128, (3,3), activation=\"relu\")(image_input)\n",
    "conv = Convolution2D(128, (1,1), activation=\"relu\")(conv)\n",
    "conv = Convolution2D(128, (1,1), activation=\"relu\")(conv)\n",
    "conv = MaxPooling2D(pool_size=(2,2))(conv)\n",
    "\n",
    "# Flatten the output to enable the merge to happen with the other input\n",
    "first_part_output = Flatten()(conv)\n",
    "\n",
    "# Merge the output of the convNet with the added features by concatenation\n",
    "merged_model = keras.layers.concatenate([first_part_output, other_data_input])\n",
    "\n",
    "# Add an extra dense layer between the data and the output\n",
    "dense = Dense(chunk_radius * 2 + 1, activation ='relu')(merged_model)\n",
    "#dense = Dropout(.2)(dense)\n",
    "dense = Dense(chunk_radius * 2 + 1, activation ='relu')(dense)\n",
    "#dense = Dropout(.2)(dense)\n",
    "dense = Dense(chunk_radius * 2 + 1, activation ='relu')(dense)\n",
    "#dense = Dropout(.2)(dense)\n",
    "\n",
    "# Predict on the output \n",
    "predictions = Dense(max_placements, activation ='softmax')(dense)\n",
    "\n",
    "# Create the model\n",
    "note_count_class_model = Model(inputs=[image_input, other_data_input], outputs=predictions) \n",
    "#model1.summary()\n",
    "\n",
    "# Set optimizer and compile\n",
    "#optimizer = keras.optimizers.Adam(lr=0.00001)\n",
    "#optimizer = keras.optimizers.Adam(lr=0.0001)\n",
    "optimizer = keras.optimizers.Adam(lr=0.001)\n",
    "note_count_class_model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=[\"accuracy\", keras_metrics.precision(), keras_metrics.recall()])\n",
    "#model1.compile(loss='binary_crossentropy', optimizer=\"Adam\", metrics=['binary_accuracy'])\n",
    "\n",
    "# Create generators\n",
    "train_generator = CountDataGenerator(train_keys, song_df, num_mels, win_len, \n",
    "                                     chunk_radius, \"Note\", max_placements, batch_size)\n",
    "validation_generator = CountDataGenerator(validation_keys, song_df, num_mels, win_len, \n",
    "                                          chunk_radius, \"Note\", max_placements, batch_size)\n",
    "note_count_class_model.fit_generator(generator=train_generator, epochs=100, validation_data=validation_generator)\n",
    "note_count_class_model.save('Experiments/Experiment1/note_count_class.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TypeDataGenerator(keras.utils.Sequence):\n",
    "    'Generates data for Keras'\n",
    "    def __init__(self, key_difs, song_df, num_mels, win_len, chunk_radius, previous_steps, object_type=\"Note\", batch_size=200):\n",
    "        'Initialization'\n",
    "        self.key_difs = key_difs\n",
    "        self.song_df = song_df\n",
    "        self.num_mels = num_mels\n",
    "        self.win_len = win_len\n",
    "        self.chunk_radius = chunk_radius\n",
    "        self.previous_steps = previous_steps\n",
    "        self.object_type = object_type\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        self.pitch_shift = 0\n",
    "        self.pitch_index = 0\n",
    "        self.pitch_shifts = [0, 4, -4, 8, -8, 12, -12, 16, -16]\n",
    "        \n",
    "        self.get_data_for_epoch()\n",
    "        self.num_samples = self.specs.shape[0]\n",
    "\n",
    "    def get_data_for_epoch(self):\n",
    "        spec_list = []\n",
    "        meta_list = []\n",
    "        obj_list = []\n",
    "        \n",
    "        for key_dif in self.key_difs:\n",
    "            key, dif = key_dif.split(\"-\")\n",
    "            \n",
    "            meta_arr, spec, time_indices, obj_count_arr = get_song_count_train_data(self.song_df, key, self.num_mels, self.win_len, dif, \n",
    "                                                                self.pitch_shift, self.object_type)\n",
    "            \n",
    "            # Something else\n",
    "            obj_df = get_pandas_song_map(song_df[(song_df.key == \"570\") & (song_df.difficulty == \"Expert\")].iloc[0])\n",
    "            obj_df[\"input_num\"] = round(obj_df[\"time\"] * 1000 / win_len)\n",
    "            \n",
    "            obj_df = obj_df[obj_df.object_type == \"Note\"]\n",
    "            obj_df = obj_df.drop([\"time\", \"duration\", \"width\", \"object_type\", \"value\"], axis=1)\n",
    "            obj_df[\"type\"] = obj_df[\"type\"].astype(pd.api.types.CategoricalDtype(categories = [0, 1]))\n",
    "            obj_df[\"line_index\"] = obj_df[\"line_index\"].astype(pd.api.types.CategoricalDtype(categories =[0, 1, 2, 3]))\n",
    "            obj_df[\"line_layer\"] = obj_df[\"line_layer\"].astype(pd.api.types.CategoricalDtype(categories =[0, 1, 2]))\n",
    "            obj_df[\"cut_direction\"] = obj_df[\"cut_direction\"].astype(pd.api.types.CategoricalDtype(categories =[0, 1, 2, 3, 4, 5, 6, 7, 8]))\n",
    "            obj_df = pd.get_dummies(obj_df,prefix=[\"type\", \"index\", \"layer\", \"direction\"])\n",
    "            obj_df.sort_values(by=[\"input_num\", \"layer_0\", \"layer_1\", \"layer_2\", \"index_0\", \"index_1\", \"index_2\", \"index_3\"])\n",
    "            \n",
    "            obj_arr = obj_df.to_numpy()\n",
    "            obj_arr_index = 0\n",
    "            \n",
    "            max_index_dif = 500\n",
    "            \n",
    "            prev_indices = np.ones(self.previous_steps) * -max_index_dif\n",
    "            prev_counts = np.zeros(self.previous_steps)\n",
    "            \n",
    "            for time_index in obj_arr[:, 0]:\n",
    "                \n",
    "                if time_index < meta_arr.shape[0]: # Sanity check for notes that have been placed after song end\n",
    "                    time_difs = np.ones(self.previous_steps) * time_index - prev_indices\n",
    "                    time_difs = np.clip(time_difs, 0, max_index_dif) / max_index_dif\n",
    "\n",
    "                    # Stack the difference from the last time step, the counts at the last time steps, and the difficulty\n",
    "                    # as the \n",
    "                    meta_info = np.hstack([time_difs, prev_counts, meta_arr[int(time_index), 0]]) \n",
    "\n",
    "                    spec_arr_new = chunk_spectrogram(spec, time_indices, time_index, 1, self.chunk_radius)\n",
    "                    spec_list.append(spec_arr_new)\n",
    "                    meta_list.append(meta_info) \n",
    "                    obj_list.append(obj_arr[obj_arr_index, 1:])\n",
    "                    obj_arr_index += 1\n",
    "\n",
    "                    if time_index == prev_indices[0]:\n",
    "                        prev_counts[0] += 1\n",
    "                    else:\n",
    "                        for i in range(self.previous_steps - 1, 0, -1):\n",
    "                            prev_indices[i] = prev_indices[i - 1]\n",
    "                            prev_counts[i] = prev_counts[i - 1]\n",
    "\n",
    "                        prev_indices[0] = time_index\n",
    "                        prev_counts[0] = 1\n",
    "        \n",
    "        self.specs = reshape_spec_arr(np.vstack(spec_list))\n",
    "        self.meta = np.stack(meta_list)\n",
    "        self.objs = np.stack(obj_list)\n",
    "        logging.debug(\"Shapes for epoch. Specs: {}, meta: {}, objs: {}\".format(self.specs.shape, \n",
    "                                                                              self.meta.shape, self.objs.shape))\n",
    "        \n",
    "    def __len__(self):\n",
    "        'Denotes the number of batches per epoch'\n",
    "        return math.ceil(self.num_samples / self.batch_size)\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        'Update pitches after each epoc'\n",
    "        \n",
    "        self.pitch_index += 1\n",
    "        \n",
    "        if self.pitch_index >= len(self.pitch_shifts):\n",
    "            self.pitch_index = 0\n",
    "        \n",
    "        self.pitch_shift = self.pitch_shifts[self.pitch_index]\n",
    "        \n",
    "        self.get_data_for_epoch()\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        'Generate one batch of data'\n",
    "        #import pdb; pdb.set_trace()\n",
    "        # Get key and difficulty for current         \n",
    "        #x = [self.specs[index:index+batch_size], self.meta[index:index+batch_size]]\n",
    "        x = self.meta[index:index+batch_size]\n",
    "        y = self.objs[index:index+batch_size]\n",
    "        \n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "previous_steps = 3\n",
    "train_keys = train_keys_s\n",
    "validation_keys = train_keys_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Timing and difficulty information\n",
    "other_data_input = Input(shape=(1 + previous_steps * 2, ))   \n",
    "\n",
    "merged_model = Reshape((1, 1 + previous_steps * 2))(other_data_input)\n",
    "\n",
    "# Add an extra dense layer between the data and the output\n",
    "dense = LSTM(chunk_radius * 2 + 1, return_sequences=True, activation ='relu')(merged_model)\n",
    "#dense = Dropout(.2)(dense)\n",
    "dense = LSTM(chunk_radius * 2 + 1, return_sequences=True, activation ='relu')(dense)\n",
    "dense = LSTM(chunk_radius * 2 + 1, return_sequences=True, activation ='relu')(dense)\n",
    "dense = LSTM(chunk_radius * 2 + 1, return_sequences=True, activation ='relu')(dense)\n",
    "dense = LSTM(chunk_radius * 2 + 1, activation ='relu')(dense)\n",
    "\n",
    "# Predict on the output \n",
    "predictions = Dense(18, activation ='sigmoid')(dense)\n",
    "\n",
    "# Create the model\n",
    "note_type_class_model = Model(inputs=[other_data_input], outputs=predictions) \n",
    "note_type_class_model.summary()\n",
    "\n",
    "# Set optimizer and compile\n",
    "#optimizer = keras.optimizers.Adam(lr=0.00001)\n",
    "#optimizer = keras.optimizers.Adam(lr=0.0001)\n",
    "optimizer = keras.optimizers.Adam(lr=0.001)\n",
    "note_type_class_model.compile(loss='binary_crossentropy', optimizer=optimizer, \n",
    "                              metrics=[\"accuracy\", keras_metrics.precision(), keras_metrics.recall()])\n",
    "\n",
    "# Create generators\n",
    "train_generator = TypeDataGenerator(train_keys, song_df, num_mels, win_len, \n",
    "                                     chunk_radius, previous_steps, \"Note\", batch_size)\n",
    "validation_generator = TypeDataGenerator(validation_keys, song_df, num_mels, win_len, \n",
    "                                          chunk_radius, previous_steps, \"Note\", batch_size)\n",
    "note_type_class_model.fit_generator(generator=train_generator, epochs=50, validation_data=validation_generator)\n",
    "note_type_class_model.save('Experiments/Experiment1/note_type_class.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_info_file_for_song(directory, song_name, bpm, sub_name=\"\", author_name=\"\", \n",
    "                              level_author_name=\"TheBeatBot\", preview_start_time=10, preview_duration=10):\n",
    "\n",
    "    with open(\"MetadataFiles/info.dat\", 'r') as base_file:\n",
    "        filedata = base_file.read()\n",
    "\n",
    "    # Replace the target string\n",
    "    filedata = filedata.replace('-song_name-', song_name)\n",
    "    filedata = filedata.replace('-song_sub_name-', sub_name)\n",
    "    filedata = filedata.replace('-song_author-', author_name)\n",
    "    filedata = filedata.replace('-level_author-', level_author_name)\n",
    "    filedata = filedata.replace('-preview_start-', str(preview_start_time))\n",
    "    filedata = filedata.replace('-preview_duration-', str(preview_duration))\n",
    "    filedata = filedata.replace('-bpm-', str(bpm))\n",
    "    \n",
    "    # convert into JSON:\n",
    "    file_path = \"{}/info.dat\".format(directory)\n",
    "    \n",
    "    with open(file_path, 'w') as outfile:\n",
    "        outfile.write(filedata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_place_model(spec, time_indices, meta_arr, chunk_radius, place_model, min_threshold):\n",
    "    spec_arr = chunk_spectrogram(spec, time_indices, 0, spec.shape[1], chunk_radius)\n",
    "\n",
    "    # Reshape the spectrogram to fit the expected 4 dimensions\n",
    "    spec_arr = reshape_spec_arr(spec_arr)\n",
    "\n",
    "    place_pred = place_model.predict([spec_arr, meta_arr])\n",
    "    maximums = []\n",
    "    max_vals = []\n",
    "\n",
    "    #min_threshold = .5\n",
    "    #import pdb; pdb.set_trace()\n",
    "\n",
    "    index_len = len(place_pred)\n",
    "    index_range = range(index_len - 1)\n",
    "\n",
    "    for i in index_range:\n",
    "        if (i > 0 and place_pred[i] > place_pred[i - 1] and i < index_len - 1 and place_pred[i] > place_pred[i + 1] \n",
    "            and place_pred[i] > min_threshold):\n",
    "            maximums.append(i)\n",
    "            max_vals.append(place_pred[i])\n",
    "    \n",
    "    return maximums, max_vals\n",
    "\n",
    "def run_count_model(spec, time_indices, meta_arr, indices, chunk_radius, count_model):\n",
    "    spec_list = []\n",
    "    meta_list = []\n",
    "\n",
    "    for index in indices:\n",
    "        spec_arr_new = chunk_spectrogram(spec, time_indices, index, 1, chunk_radius)\n",
    "        spec_list.append(spec_arr_new)\n",
    "        meta_list.append(meta_arr[index, 0])\n",
    "\n",
    "    spec_arr = reshape_spec_arr(np.vstack(spec_list))\n",
    "    meta_arr = np.stack(meta_list)\n",
    "\n",
    "    #import pdb; pdb.set_trace()\n",
    "    return count_model.predict([spec_arr, meta_arr])\n",
    "\n",
    "def run_type_model(spec, time_indices, meta_arr, indices, chunk_radius, previous_steps, max_index_dif, type_model):\n",
    "    spec_list = []\n",
    "    meta_list = []\n",
    "    \n",
    "    prev_indices = np.ones(previous_steps) * -max_index_dif\n",
    "    prev_counts = np.zeros(previous_steps)\n",
    "\n",
    "    for time_index in indices:\n",
    "        if time_index < meta_arr.shape[0]: # Sanity check for notes that have been placed after song end\n",
    "            time_difs = np.ones(previous_steps) * time_index - prev_indices\n",
    "            time_difs = np.clip(time_difs, 0, max_index_dif) / max_index_dif\n",
    "\n",
    "            # Stack the difference from the last time step, the counts at the last time steps, and the difficulty\n",
    "            # as the \n",
    "            meta_info = np.hstack([time_difs, prev_counts, meta_arr[int(time_index), 0]]) \n",
    "\n",
    "            spec_arr_new = chunk_spectrogram(spec, time_indices, time_index, 1, chunk_radius)\n",
    "            spec_list.append(spec_arr_new)\n",
    "            meta_list.append(meta_info)\n",
    "\n",
    "            if time_index == prev_indices[0]:\n",
    "                prev_counts[0] += 1\n",
    "            else:\n",
    "                for i in range(previous_steps - 1, 0, -1):\n",
    "                    prev_indices[i] = prev_indices[i - 1]\n",
    "                    prev_counts[i] = prev_counts[i - 1]\n",
    "\n",
    "                prev_indices[0] = time_index\n",
    "                prev_counts[0] = 1\n",
    "    \n",
    "    spec_arr = reshape_spec_arr(np.vstack(spec_list))\n",
    "    meta_arr = np.stack(meta_list)\n",
    "    \n",
    "    #return type_model.predict([spec_arr, meta_arr])\n",
    "    return type_model.predict(meta_arr)\n",
    "\n",
    "def index_to_beat(win_len, index, bpm):\n",
    "    return (win_len / 1000) * index * bpm / 60\n",
    "\n",
    "def create_map_file_for_song(directory, difficulty_name, notes):\n",
    "    info_dict = {\"_version\": \"2.0.0\",\n",
    "                 \"_PBMChanges\": [],\n",
    "                 \"_events\": [],\n",
    "                 \"_notes\": notes,\n",
    "                 \"_obstacles\": [],\n",
    "                 \"_bookmarks\": []}\n",
    "    \n",
    "    file_path = \"{}/{}.dat\".format(directory, difficulty_name)\n",
    "    \n",
    "    with open(file_path, 'w') as outfile:\n",
    "        json.dump(info_dict, outfile)\n",
    "\n",
    "def create_map_for_song(filename, num_mels, win_len, output_directory, \n",
    "                        previous_steps, max_index_dif, \n",
    "                        place_model, count_model, type_model,\n",
    "                        song_name, sub_name=\"\", author_name=\"\", \n",
    "                        level_author_name=\"TheBeatBot\", preview_start_time=10, preview_duration=10):\n",
    "    \n",
    "    y, sr = librosa.load(filename)\n",
    "    bpm = librosa.beat.tempo(y, sr)[0]\n",
    "    \n",
    "    create_info_file_for_song(output_directory, song_name, bpm, sub_name=\"\", author_name=\"\", \n",
    "                              level_author_name=\"TheBeatBot\", preview_start_time=10, preview_duration=10)\n",
    "    \n",
    "    spec, time_indices = get_spectrogram_from_file(filename, num_mels, win_len)\n",
    "    input_len = len(time_indices)\n",
    "    \n",
    "    difficulties = [\"Easy\", \"Normal\", \"Hard\", \"Expert\", \"ExpertPlus\"]\n",
    "    #difficulties = [\"Expert\"]\n",
    "    \n",
    "    for dif in difficulties:\n",
    "        meta_arr = get_meta_arr(input_len, dif_val_from_string(dif), win_len)\n",
    "        \n",
    "        maximums, max_vals = run_place_model(spec, time_indices, meta_arr, chunk_radius, place_model, min_threshold=.5)\n",
    "        \n",
    "        count_pred = run_count_model(spec, time_indices, meta_arr, maximums, chunk_radius, count_model)\n",
    "        \n",
    "        type_indices = []\n",
    "\n",
    "        cur_max_index = 0\n",
    "\n",
    "        for num_indices in np.argmax(count_pred, axis=1):\n",
    "            for i in range(num_indices):\n",
    "                type_indices.append(maximums[cur_max_index])\n",
    "\n",
    "            cur_max_index += 1\n",
    "        \n",
    "        type_pred = run_type_model(spec, time_indices, meta_arr, type_indices, chunk_radius, previous_steps, max_index_dif, type_model)\n",
    "        \n",
    "        # 0 - 1 = Type = value of 0 - 1\n",
    "        # 2 - 5 = line index = value 0 - 3\n",
    "        # 6 - 8 = line layer = value 0 - 2\n",
    "        # 9 - 17 = cut direction = value 0 - 8\n",
    "        pred_type = np.argmax(type_pred[:, 0:2], axis=1)\n",
    "        pred_index = np.argmax(type_pred[:, 2:6], axis=1)\n",
    "        pred_layer = np.argmax(type_pred[:, 6:9], axis=1)\n",
    "        pred_direction = np.argmax(type_pred[:, 9:18], axis=1)\n",
    "        \n",
    "        notes = []\n",
    "        #import pdb; pdb.set_trace()\n",
    "        for i in range(len(type_indices)):\n",
    "            note = {}\n",
    "            note[\"_time\"] = index_to_beat(win_len, type_indices[i], bpm)\n",
    "            note[\"_lineIndex\"] = int(pred_index[i])\n",
    "            note[\"_lineLayer\"] = int(pred_layer[i])\n",
    "            note[\"_type\"] = int(pred_type[i])\n",
    "            note[\"_cutDirection\"] = int(pred_direction[i])\n",
    "            \n",
    "            notes.append(note)\n",
    "        \n",
    "        \n",
    "        create_map_file_for_song(output_directory, dif, notes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"Experiments/Experiment1/Fallout 76.egg\"\n",
    "output_directory = \"Experiments/Experiment1\"\n",
    "place_model = load_model('Experiments/Experiment1/note_place_class.h5', custom_objects={'binary_precision': keras_metrics.precision(), 'binary_recall': keras_metrics.recall()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_model = load_model('Experiments/Experiment1/note_count_class.h5', custom_objects={'binary_precision': keras_metrics.precision(), 'binary_recall': keras_metrics.recall()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type_model = load_model('Experiments/Experiment1/note_type_class.h5', custom_objects={'binary_precision': keras_metrics.precision(), 'binary_recall': keras_metrics.recall()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_map_for_song(filename, num_mels, win_len, output_directory, previous_steps, 500,\n",
    "                     note_place_class_model, note_count_class_model, note_type_class_model, \n",
    "                     song_name=\"Test Song\", sub_name=\"Test sub\", author_name=\"Test Author Name\", \n",
    "                     level_author_name=\"TestTheBeatBot\", preview_start_time=10, preview_duration=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
